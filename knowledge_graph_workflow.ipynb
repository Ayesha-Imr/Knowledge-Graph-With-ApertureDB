{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic models for structured output\n",
    "class EntityType(BaseModel):\n",
    "    \"\"\"Model for an entity type with its properties.\"\"\"\n",
    "    properties: List[str] = Field(description=\"List of properties for this entity type\")\n",
    "\n",
    "class EntityExtractionOutput(BaseModel):\n",
    "    \"\"\"Model for the entity extraction output.\"\"\"\n",
    "    entity_types: Dict[str, List[str]] = Field(\n",
    "        description=\"Dictionary mapping entity types to their properties\"\n",
    "    )\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict[str, List[str]]) -> \"EntityExtractionOutput\":\n",
    "        \"\"\"Create an EntityExtractionOutput from a dictionary.\"\"\"\n",
    "        return cls(entity_types=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM with Gemini 2.0 Flash\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",  # Using Gemini 2.0 Flash\n",
    "    temperature=0,\n",
    "    google_api_key=google_api_key\n",
    ")\n",
    "\n",
    "# Create the output parser\n",
    "parser = PydanticOutputParser(pydantic_object=EntityExtractionOutput)\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"You are part of an agentic workflow that processes data input by a user step by step. \n",
    "The end result of the workflow is a detailed knowledge graph generated from the input data.\n",
    "\n",
    "You are the Preprocessing & Entity Extraction Agent, which is the FIRST agent in this workflow. Your responsibilities are:\n",
    "- Identify entity types and associated properties (e.g., name, age, industry)\n",
    "- Return a global entity list for further processing\n",
    "\n",
    "Given the following text, extract broad, general categories of entities and list the general types of properties associated with each category.\n",
    "\n",
    "Focus on identifying general categories of entities and generalizable properties that could apply to those categories.\n",
    "\n",
    "Text to process: {input}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Response:\"\"\"\n",
    "\n",
    "# Create the prompt\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "# Create the chain\n",
    "chain = (\n",
    "    {\"input\": RunnablePassthrough(), \"format_instructions\": lambda _: parser.get_format_instructions()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_types(text: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"Extract entity types and their properties from text.\n",
    "    \n",
    "    Args:\n",
    "        text: The input text to process\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary mapping entity types to their properties\n",
    "    \"\"\"\n",
    "    result = chain.invoke(text)\n",
    "    return result.entity_types\n",
    "\n",
    "def workflow_step_1(text: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"First step in the knowledge graph workflow.\n",
    "    \n",
    "    Args:\n",
    "        text: The input text to process\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary mapping entity types to their properties\n",
    "    \"\"\"\n",
    "    return extract_entity_types(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_file(file_path: str) -> str:\n",
    "    if file_path.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_path.endswith('.docx'):\n",
    "        loader = DocxLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Only PDF and DOCX are supported.\")\n",
    "    \n",
    "    # Load the document and extract text\n",
    "    document = loader.load()\n",
    "    extracted_text = \"\\n\".join(page.page_content for page in document)\n",
    "    \n",
    "    # Return the extracted text\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investigating Explainability of Generative AI for Code through\n",
      "Scenario-based Design\n",
      "Jiao Sun‚àó\n",
      "University of Southern California\n",
      "Los Angeles, USA\n",
      "jiaosun@usc.edu\n",
      "Q. Vera Liao‚Ä†\n",
      "Microsoft Research\n",
      "Montr√©al, Canada\n",
      "veraliao@microsoft.com\n",
      "Michael Muller\n",
      "IBM Research AI\n",
      "Yorktown Heights, USA\n",
      "michael_muller@us.ibm.com\n",
      "Mayank Agarwal\n",
      "IBM Research AI\n",
      "Yorktown Heights, USA\n",
      "Mayank.Agarwal@ibm.com\n",
      "Stephanie Houde\n",
      "IBM Research AI\n",
      "Yorktown Heights, USA\n",
      "Stephanie.Houde@ibm.com\n",
      "Kartik Talamadupula\n",
      "IBM Research AI\n",
      "Yorktown Heights, USA\n",
      "krtalamad@us.ibm.com\n",
      "Justin D. Weisz\n",
      "IBM Research AI\n",
      "Yorktown Heights, USA\n",
      "jweisz@us.ibm.com\n",
      "ABSTRACT\n",
      "What does it mean for a generative AI model to be explainable?\n",
      "The emergent discipline of explainable AI (XAI) has made great\n",
      "strides in helping people understand discriminative models. Less\n",
      "attention has been paid to generative models that produce arti-\n",
      "facts, rather than decisions, as output. Meanwhile, generative AI\n",
      "(GenAI) technologies are maturing and being applied to application\n",
      "domains such as software engineering. Using scenario-based de-\n",
      "sign and question-driven XAI design approaches, we explore users‚Äô\n",
      "explainability needs for GenAI in three software engineering use\n",
      "cases: natural language to code, code translation, and code auto-\n",
      "completion. We conducted 9 workshops with 43 software engineers\n",
      "in which real examples from state-of-the-art generative AI mod-\n",
      "els were used to elicit users‚Äô explainability needs. Drawing from\n",
      "prior work, we also propose 4 types of XAI features for GenAI for\n",
      "code and gathered additional design ideas from participants. Our\n",
      "work explores explainability needs for GenAI for code and demon-\n",
      "strates how human-centered approaches can drive the technical\n",
      "development of XAI in novel domains.\n",
      "CCS CONCEPTS\n",
      "‚Ä¢ Computing methodologies ‚ÜíNatural language generation;\n",
      "‚Ä¢ Software and its engineering ; ‚Ä¢ Human-centered comput-\n",
      "ing ‚ÜíHuman computer interaction (HCI) ; User studies ;\n",
      "‚àóWork done during the first author‚Äôs internship at IBM Research AI.\n",
      "‚Ä†Work done while the second author was at IBM Research AI.\n",
      "Permission to make digital or hard copies of part or all of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for third-party components of this work must be honored.\n",
      "For all other uses, contact the owner/author(s).\n",
      "IUI ‚Äô22, March 22‚Äì25, 2022, Helsinki, Finland\n",
      "¬© 2022 Copyright held by the owner/author(s).\n",
      "ACM ISBN 978-1-4503-9144-3/22/03.\n",
      "https://doi.org/10.1145/3490099.3511119\n",
      "KEYWORDS\n",
      "generative AI, software engineering tooling, explainable AI, human-\n",
      "centered AI, scenario based design\n",
      "ACM Reference Format:\n",
      "Jiao Sun, Q. Vera Liao, Michael Muller, Mayank Agarwal, Stephanie Houde,\n",
      "Kartik Talamadupula, and Justin D. Weisz. 2022. Investigating Explain-\n",
      "ability of Generative AI for Code through Scenario-based Design. In 27th\n",
      "International Conference on Intelligent User Interfaces (IUI ‚Äô22), March 22‚Äì\n",
      "25, 2022, Helsinki, Finland. ACM, New York, NY, USA, 17 pages. https:\n",
      "//doi.org/10.1145/3490099.3511119\n",
      "1 INTRODUCTION\n",
      "Generative AI (GenAI) is a class of machine learning (ML) algo-\n",
      "rithms that can learn from content such as text, images, and audio\n",
      "in order to generate new content. In contrast to discriminative\n",
      "ML algorithms, which learn decision boundaries, GenAI models\n",
      "produce artifacts as output, which can have a wide range of va-\n",
      "riety and complexity. One major recent development of GenAI is\n",
      "the introduction of OpenAI‚Äôs GPT-3 [13] model, which can gener-\n",
      "ate human-like language output and has striking versatility. Other\n",
      "generative language models have emerged that focus on specific\n",
      "domains such as software engineering, implementing use cases of\n",
      "auto-completing code [15, 39], translating code from one program-\n",
      "ming language to another [88], and converting natural language to\n",
      "code [23]. The industry has begun to use these models to support\n",
      "software engineering practices, with the most prominent example\n",
      "being GitHub CoPilot [27], a GenAI-based co-programming tool.\n",
      "As a novel technology applied to novel domains, there are many\n",
      "open questions to be answered for how to make GenAI more ca-\n",
      "pable and user-friendly. One open question is how to enable ex-\n",
      "plainability‚Äîallowing users to understand and have a better mental\n",
      "model‚Äîof GenAI. Recent works by Goodfellow et al. [28] and Ross\n",
      "et al. [86] have explored developing more interpretable GenAI mod-\n",
      "els that follow more human-understandable processes. However, a\n",
      "more comprehensive view of explainability for GenAI is lacking:\n",
      "arXiv:2202.04903v1  [cs.HC]  10 Feb 2022\n",
      "IUI ‚Äô22, March 22‚Äì25, 2022, Helsinki, Finland Jiao Sun et al.\n",
      "what do users need to understand about a GenAI model in order to\n",
      "effectively achieve their goals when working with it? In this paper,\n",
      "we build a foundational understanding of explainability needs for\n",
      "GenAI in the context of generative code models.\n",
      "This question ofwhat do users need to understand about AI systems\n",
      "is core to the nascent field of Human-Centered Explainable AI\n",
      "(HCXAI) [21, 22, 52], which is a subset of the fields of human\n",
      "centered AI and human centered data science [6, 7, 25, 41, 65‚Äì67].\n",
      "Our work is informed by a few key lessons from recent work in\n",
      "HCXAI, mostly conducted in the context of discriminative ML (e.g.,\n",
      "for decision-support systems). First, explainability needs should be\n",
      "considered broadly as any means of helping users achieve a better\n",
      "understanding of the AI system . Liao et al. [48] proposed to define\n",
      "user‚Äôs explainablity needs by what questions they ask to understand\n",
      "the AI [48] and developed a framework of common questions. This\n",
      "framework demonstrates that users are interested in a broad range\n",
      "of explanatory information about an AI model, including its overall\n",
      "logic, how it reasons to produce a particular output, the training\n",
      "data, and its performance and range of output. However, user needs\n",
      "regarding generative models were not explored in that work.\n",
      "Second, XAI solutions that address explainability needs should\n",
      "not be limited to algorithmic explanations or showing model inter-\n",
      "nals. Depending on user needs, it may be more critical to provide\n",
      "transparent information about a model‚Äôs capabilities, limitations\n",
      "(e.g. uncertainty [10]) or provenance [8]. Moreover, users may need\n",
      "additional information beyond algorithmic explanations to fill in\n",
      "gaps of understanding. For example, Ehsan et al . [19] proposed\n",
      "that social transparency ‚Äì making visible the socio-organizational\n",
      "factors that govern the use of AI ‚Äì can help users form a socially-\n",
      "situated understanding of an AI system and take more effective\n",
      "actions with it.\n",
      "Finally, perhaps the most important lesson from HCXAI is that\n",
      "users‚Äô explainability needs emerge in a usage context , guided by\n",
      "their goals and shaped by their backgrounds, expectations, as well\n",
      "as socio, organizational and cultural contexts [20, 48, 52]. It is thus\n",
      "necessary to follow a user-centered approach to understand ex-\n",
      "plainability needs by involving target users and leveraging HCI\n",
      "methods that allow inquiry within the context of usage.\n",
      "Based on these lessons, we adopted a scenario-based design\n",
      "method by constructing realistic usage scenarios for three use cases\n",
      "of GenAI for code: code translation, code auto-completion, and natu-\n",
      "ral language to code. We invited 43 software engineers to participate\n",
      "in 9 workshops to elicit their explainability needs and design ideas\n",
      "around these scenarios. We adapted the question-driven method\n",
      "of Liao et al. [48, 50] to elicit and comprehensively explore partic-\n",
      "ipants‚Äô explainability needs by what kind questions they would\n",
      "ask in the scenarios. We also gathered feedback and design ideas\n",
      "from participants for four kinds of XAI features that we propose\n",
      "for the uses cases of GenAI for code. Our work makes three main\n",
      "contributions to the IUI community:\n",
      "(1) We identify 11 categories of explainability needs in the con-\n",
      "text of Generative AI (GenAI) for code, for which we provide\n",
      "definitions and examples. We further contrast these cate-\n",
      "gories with previous XAI techniques for discriminative ML\n",
      "and discuss explainability needs unique to GenAI and code\n",
      "generation use cases. We believe we are among the first to\n",
      "explore users‚Äô explainability needs in an application domain\n",
      "of GenAI.\n",
      "(2) We propose four kinds of XAI features to support users of\n",
      "GenAI for code, based on prior work and adapted to the\n",
      "domain of code generation. These features are: AI documen-\n",
      "tation, indications of model uncertainty, visualizations of\n",
      "model attention, and social transparency. Based on partici-\n",
      "pants‚Äô responses, we provide concrete design recommenda-\n",
      "tions to operationalize these features.\n",
      "(3) Our work makes methodological contributions by combining\n",
      "scenario-based design, participatory design workshops, and\n",
      "a question-driven approach to elicit explainability needs. We\n",
      "also reflect on the values and limitations of this method to\n",
      "inform future work that explores GenAI in new domains.\n",
      "2 RELATED WORK\n",
      "We review three areas of related work that shaped our study: GenAI\n",
      "for code, explainable AI, and human-centered approaches to AI.\n",
      "2.1 Generative AI for Code\n",
      "The application of modern NLP techniques to programming lan-\n",
      "guage can be traced back to the naturalness hypothesis [4, 17, 37],\n",
      "that software is a form of human communication. This hypothesis\n",
      "opened the door for applying NLP techniques previously used on\n",
      "human natural languages to source code, and recent work in this\n",
      "space is summarized by Talamadupula [92] and Allamanis et al. [4].\n",
      "One example is how existing work on automatic, machine transla-\n",
      "tion between human natural languages [71, 72] was applied to code.\n",
      "Specifically, the TransCoder [88] system applied neural machine\n",
      "translation techniques to translate source code across different lan-\n",
      "guages. Other GenAI models have been developed that implement\n",
      "other use cases, such as generating documentation for code [23],\n",
      "auto-completing code [15, 39], generating unit tests [93], and find-\n",
      "ing duplicate code [30]. Models trained on massive code data sets\n",
      "are even able to handle multiple use cases at the same time, such as\n",
      "PLBART [3], CodeBERT [23], GraphCodeBERT [30]. Most recently,\n",
      "OpenAI released Codex [15], which is a GPT-based model trained\n",
      "on code from GitHub and powers their CoPilot [27] product. This\n",
      "model is capable of auto-completing code for various programming\n",
      "languages (e.g., Python, TypeScript, Go, Ruby), as well as generat-\n",
      "ing code from natural language. The release of Copilot is seen as a\n",
      "revolution of AI-assisted software programming and has attracted\n",
      "much attention since its release [63].\n",
      "Despite the fact that GenAI for code models still have room for\n",
      "improvement in the quality of their outputs ‚Äì e.g., TransCoder\n",
      "only produces a correct translation 30%-70% of the time depending\n",
      "on the source and target language [ 88] ‚Äì recent work by Weisz\n",
      "et al. [100] suggests that software engineers may nonetheless be\n",
      "tolerant of using such models in their work. Given the emerging\n",
      "productization of GenAI for code models, we believe it is necessary\n",
      "to develop a comprehensive understanding of the kinds of questions\n",
      "software engineers will have when working with such models to\n",
      "guide technical XAI work and design solutions to answer them.\n",
      "Investigating Explainability of Generative AI for Code through Scenario-based Design IUI ‚Äô22, March 22‚Äì25, 2022, Helsinki, Finland\n",
      "2.2 Explainable AI\n",
      "Explainable AI (XAI) has spurred great academic, industry, and\n",
      "public interests in the past few years, thanks to the popularity of\n",
      "inscrutable ‚Äúopaque-box‚Äù ML models. Many XAI techniques have\n",
      "been developed for discriminative ML models, both through pro-\n",
      "ducing directly interpretable models [14, 44] and generating post-\n",
      "hoc explanations for a trained opaque-box model [47, 62, 80, 104].\n",
      "These explanations take a variety of forms. For example, global\n",
      "explanations provide an overview of the model logic, whilelocal ex-\n",
      "planations elucidate the rationale behind a particular output. A full\n",
      "review of XAI techniques is beyond the scope of this paper and can\n",
      "be found in many recent survey papers [1, 29, 55, 56]. Our work is\n",
      "most closely informed by, and intends to bridge, the emerging topic\n",
      "of explainability for generative models, and the inter-disciplinary\n",
      "field of Human-Centered Explainable AI (HCXAI) [21, 22, 52].\n",
      "Compared to discriminative models, much less attention has\n",
      "been paid to developing XAI techniques for generative AI models.\n",
      "Some explored ways to make generative models more directly in-\n",
      "terpretable, which is often framed as making the representation\n",
      "learning of a GenAI model in its latent dimensions semantically\n",
      "meaningful so people can directly examine the model internals\n",
      "[28]. For example, disentanglement is a technique that seeks map-\n",
      "pings between high-dimensional inputs and low-dimensional rep-\n",
      "resentations such that representation dimensions correspond to\n",
      "the ground-truth factors that generated the data [83]. Accordingly,\n",
      "some proposed disentanglement measures [ 16, 84] as a way to\n",
      "evaluate a GenAI model‚Äôs interpretability. A recent study by Ross\n",
      "et al. [86] proposed a user evaluation task to evaluate the human\n",
      "interpretability of generative models, by the ability for people to\n",
      "interactively modify representations to reconstruct target instances.\n",
      "Others explored visualization approaches to present the repre-\n",
      "sentations to help users (often model developers) make sense of\n",
      "what the GenAI model has learned. For example, Ross et al. [86] use\n",
      "sliders to let users dynamically modify representation dimensions\n",
      "and see how corresponding instances change. Recent HCI works\n",
      "also explored the approach of ‚Äúexplainability through interaction\"\n",
      "for generative models, by allowing users to interact with the input\n",
      "or guiding output generation process [ 59, 60, 86, 103]. Through\n",
      "observing immediate feedback from changes to the model output,\n",
      "people can make better sense of how a generative model works. For\n",
      "example, Zhang and Banovic developed a system that allows users\n",
      "to interactively explore the output space of an image generative\n",
      "model to assess the model quality [103].\n",
      "These varied approaches suggest that explainability is still a less\n",
      "than well-defined notion for GenAI. Here we adopt a HCXAI posi-\n",
      "tion that explainability, or the effectiveness of explanation, should\n",
      "be defined as enabling people‚Äôs understanding of the AI to achieve\n",
      "their goals [52]. With this human-centered definition, many have ar-\n",
      "gued that it is necessary to provide transparent information beyond\n",
      "the model internals, such as its performance, limitations, training\n",
      "data, and development procedure, to enable a holistic understanding\n",
      "of the AI and more actionable insights [11, 48, 73, 94]. As discussed,\n",
      "Liao et al. proposed to identify users‚Äô explainability needs by elic-\n",
      "iting the questions they ask to understand the AI [ 48, 50]. This\n",
      "method allows approaching XAI from the users‚Äô perspective and\n",
      "thoroughly identify transparent information they need to achieve\n",
      "an understanding necessary for their goals, which, in the context\n",
      "of GenAI for code, could be about optimizing for the usage of the\n",
      "AI system and overall productivity. Liao et al. ‚Äôs work was based on\n",
      "prior HCI work that defines ‚Äúintelligibility types\" of information for\n",
      "context-aware intelligent systems by prototypical questions such\n",
      "as Input, Output, Why, What-if, etc. [53, 54]. It also draws on social\n",
      "science literature showing that people‚Äôs explanatory goals can be\n",
      "expressed in different kinds of questions [ 34]. We build on this\n",
      "line of work and adapt the question-drive method to elicit users‚Äô\n",
      "explainability needs for GenAI for code.\n",
      "2.3 Human-centered approaches to AI\n",
      "The term Human-centered AI has emerged in many academic works\n",
      "and public discussions [22, 25, 45, 65, 85, 89]. While definitions vary,\n",
      "human-centered approaches to AI aim to develop AI systems that\n",
      "serve the needs, improve the conditions, and align with the val-\n",
      "ues of human stakeholders. Research begins to develop practical\n",
      "methods that can help achieve these goals. One set of methods\n",
      "gained much attention lately under the umbrella term of ‚Äúpartic-\n",
      "ipatory machine learning\" [ 32, 46, 97]. Built on the tradition of\n",
      "participatory design, participatory machine learning emphasizes\n",
      "involving stakeholders, especially affected marginalized groups,\n",
      "into the development process early on to shape the overall goals\n",
      "and design choices of ML systems. Some suggested that abstracting\n",
      "user values from their participatory input is especially effective\n",
      "to guide ML modeling choices such as defining its optimization\n",
      "functions, features, constraints, and so on [49, 68, 105]. This notion\n",
      "of driving technical development based on insights from empirical\n",
      "user studies is also at the core of broad IUI research [5].\n",
      "One challenge to involving users to shape the design and techni-\n",
      "cal development of AI systems is that these systems often do not\n",
      "exist yet for people to experience and provide realistic feedback.\n",
      "This challenge can be tackled by human-centered methods that\n",
      "allow ‚Äúenvisioning future use possibilities‚Äù. One effective method is\n",
      "scenario-based design (SBD) [87]. SBD suspends the needs to define\n",
      "system operations by using narrative descriptions of how a user\n",
      "uses a system to accomplish a task, allowing people to respond to\n",
      "concrete interactions. We chose to use SBD to explore GenAI for\n",
      "code use cases as most software engineers do not have experience\n",
      "with such technologies. SBD also allows us to explore XAI design\n",
      "without the constraint of current technical feasibility, as adopted\n",
      "by several prior XAI works [19, 102].\n",
      "3 METHODOLOGY: SCENARIO-BASED\n",
      "DESIGN WORKSHOPS\n",
      "We conducted 9 semi-structured workshops, with 3-6 participants in\n",
      "each, which lasted for 60-70 minutes. Due to the impact of the global\n",
      "COVID-19 pandemic, participants joined the workshop remotely\n",
      "via a video conferencing tool. We also used Mural1, which provides\n",
      "visual workspaces for virtual collaboration. Each workshop was\n",
      "based on one of three use cases of GenAI for code: code translation,\n",
      "code autocompletion, and natural language to code. In the following,\n",
      "we first introduce the three use cases then describe the workshop\n",
      "in detail, then participants and analysis.\n",
      "1https://www.mural.co/\n",
      "IUI ‚Äô22, March 22‚Äì25, 2022, Helsinki, Finland Jiao Sun et al.\n",
      "3.1 Use Cases and Scenarios\n",
      "We focus on three specific use cases of GenAI for code, based on\n",
      "the preferred choices that could deliver high value for software\n",
      "engineering tasks from a pre-study survey with 81 people who\n",
      "responded to our study recruitment message.\n",
      "‚Ä¢Code translation , in which a generative model translates\n",
      "source code from one language (e.g., Java) to another (e.g.,\n",
      "Python). This task has been an important benchmark for\n",
      "technical work in GenAI for code [ 61] and has gained ex-\n",
      "tensive attention from both industry and academia [ 3, 23,\n",
      "30, 88, 99]. Such technologies can significantly reduce the\n",
      "cost and expertise barriers for code modernization work, in\n",
      "which a legacy codebase is ported to a modern programming\n",
      "language.\n",
      "‚Ä¢Code autocompletion, in which a generative model takes com-\n",
      "ments and source code as input (e.g. a function specification\n",
      "and/or signature), and produces code as output (e.g. the\n",
      "implementation of the function). This use case can fulfill\n",
      "pervasive needs of software engineers to improve their pro-\n",
      "ductivity and efficiency. Notably, autocompletion is one of\n",
      "the primary functions of GitHub Copilot.\n",
      "‚Ä¢Natural language to code , in which a generative model takes\n",
      "natural language (e.g. ‚Äúchange the color of the button to\n",
      "blue‚Äù) and produces code as output (e.g. button.setColor\n",
      "(Color.blue)). This use case is another function offered\n",
      "by GitHub Copilot and represents a promising way to reduce\n",
      "entry barriers to programming.\n",
      "For each use case, we created a persona, Alex, who is a software\n",
      "engineer working in a team. In the scenario given to participants,\n",
      "Alex and his teammates are introduced to an AI system to support\n",
      "their work. They are told that they could ask for more information\n",
      "and new functions added to the AI system to help them understand\n",
      "and work better with the AI. Figure 1 (ùëè)shows the description\n",
      "of the persona for the code auto-completion use case. A concrete\n",
      "scenario of a programming task that Alex would perform is shown\n",
      "in Figure 1 (ùëê), and we show the programming tasks of the other\n",
      "two use cases (i.e., code translation and code autocompletion) in\n",
      "Appendix A.\n",
      "To give participants a realistic experience, all AI-produced code\n",
      "in our scenarios was generated using state-of-the-art generative\n",
      "code models. We used TransCoder [ 88] for the code translation\n",
      "use case and Copilot [27] for the other two use cases. For the code\n",
      "translation use case, we selected a programming solution to the\n",
      "problem of converting integers to their Roman numeral representa-\n",
      "tions.2 One reason we selected this code example is that there was\n",
      "a subtle bug in the translation generated by TransCoder [88]. This\n",
      "bug was pointed out by the workshop facilitator when introduc-\n",
      "ing the scenario, to allow us to probe on participants‚Äô reactions to\n",
      "the limitations of the AI. For the code autocompletion and natural\n",
      "language to code use cases, we selected a programming solution\n",
      "to the problem of counting the number of prime numbers from a\n",
      "given input. We sampled this problem from Project CodeNet [77],\n",
      "a large dataset of code samples. Based on CodeNet‚Äôs metadata, the\n",
      "acceptance rate of Python solutions for this problem is lower than\n",
      "2We selected a Java implementation of this problem from https://algorithms.\n",
      "tutorialhorizon.com.\n",
      "50%, indicating that it is a non-trivial coding problem for which\n",
      "GenAI models might provide assistance.\n",
      "3.2 Workshop Format and Procedure\n",
      "In each workshop, the facilitator first stated the purpose of the\n",
      "workshop, introduced concepts about generative AI for code, and\n",
      "asked for participants‚Äô verbal consent and permission to record.\n",
      "Then, the facilitator started the recording and introduced the Alex\n",
      "persona and one of the programming problem scenarios described\n",
      "above. The main part of the workshop was carried out in two stages:\n",
      "first, an open-ended question elicitation exercise to understand\n",
      "participants‚Äô explainability needs in the given scenario; and second,\n",
      "an ideation session in which 4 types of XAI features were used as\n",
      "design probes to elicit feedback and design ideas, to be described\n",
      "below. The content of the persona, scenario, and design probes were\n",
      "customized based on which use case was selected for the workshop.\n",
      "All discussions and screen activities were recorded, transcribed,\n",
      "and analyzed, together with the content on the Mural pages.\n",
      "3.2.1 Stage 1: Question elicitation exercise. The first stage of the\n",
      "workshop was designed to elicit what kinds of questions partici-\n",
      "pants would have for understanding the GenAI in the given sce-\n",
      "nario, based on the question-driven approach to XAI design pro-\n",
      "posed by Liao et al. [48, 50]. After describing Alex‚Äôs persona and the\n",
      "task scenario, the workshop facilitator asked participants:‚ÄúPut your-\n",
      "self in Alex‚Äôs shoes, what does Alex want to know? What questions\n",
      "does Alex want to ask about the natural language to code model/AI?‚Äù\n",
      "Participants were given several minutes to come up as many ques-\n",
      "tions they could, posted as sticky notes on the Mural page. Next, the\n",
      "facilitator worked with participants to collaboratively cluster simi-\n",
      "lar questions together. Participants were asked to speak out loud\n",
      "as they moved around the sticky notes to describe their thought\n",
      "process and rationale. This process encouraged participants to read\n",
      "each other‚Äôs questions, ask for clarifications, and have discussions.\n",
      "After the clustering, the facilitator chose one sticky note in each\n",
      "cluster to represent the cluster, then invited participants to cast up\n",
      "to 3 votes for questions they considered to be the most important to\n",
      "address. After the voting session, participants were asked to share\n",
      "their rationales behind the votes to exchange ideas and stimulate\n",
      "new ideas.\n",
      "3.2.2 Stage 2: Ideation on XAI features. In addition to exploring\n",
      "general explainability needs, our study also aims to explore a few\n",
      "areas to develop XAI features that can address these needs. Given\n",
      "the novelty in both the technical XAI approaches for GenAI and the\n",
      "application domain, we adapted existing ideas from XAI solutions\n",
      "for discriminative ML models as a starting point of our discussions.\n",
      "Liao et al. [50] provides a suggested mapping between prototypi-\n",
      "cal user questions (e.g., why, performance, data, output) and XAI\n",
      "methods or features that can answer the questions, primarily in\n",
      "the context of discriminative AI for decision-support systems. We\n",
      "built upon this work, as well as other work that explores XAI or\n",
      "transparency features [10, 21, 31], to select and adapt features that\n",
      "could apply to the context of GenAI for code. We also took into\n",
      "consideration their technical feasibility and potential values they\n",
      "can provide for the use cases. Through this process, we arrived at\n",
      "the following XAI features:\n",
      "Investigating Explainability of Generative AI for Code through Scenario-based Design IUI ‚Äô22, March 22‚Äì25, 2022, Helsinki, Finland\n",
      "AI Supported Software Engineering\n",
      "Our Goal We are trying to understand what kinds of features can help‚Ä¶\n",
      "What is a ‚Äúgenerative AI model‚Äù? Unlike other kinds of AI models ‚Ä¶..\n",
      "Group Rules\n",
      "Everyone‚Äôs voice matters, speak out loud whenever you have anything to say!\n",
      "No right or wrong answers\n",
      "Success is generating ideas of new features or what‚Äôs needed to improve‚Ä¶\n",
      "Meet Alex\n",
      "Introduce Alex\n",
      "- Alex is a Software Engineer who works with 5 people as a team ‚Ä¶  \n",
      "- He and his teammates are offered an AI that can write Python code ‚Ä¶  \n",
      "- AI will help them on the fly when they type, AI-suggested content will ‚Ä¶  \n",
      "- They are informed that AI is not perfect and will likely make mistakes,  \n",
      "    and they can ask for more information that can help them understand \n",
      "    the AI to work better with it.\n",
      "Natural language to Code ‚ÄúBase Case‚Äù\n",
      "The first attempt \n",
      "  \n",
      "- Alex input ‚Äúread an integer‚Ä¶‚Äù \n",
      "- But the AI did not \n",
      "understand‚Ä¶  \n",
      "- Alex changed his input to ‚Ä¶\n",
      "The second attempt \n",
      "             ‚Ä¶‚Ä¶\n",
      "What questions does Alex want to ask about the model/AI?\n",
      "Designs from the AI provider\n",
      " Alex is not alone\n",
      "a b\n",
      "c d\n",
      "e f\n",
      "Figure 1: Overview of the Mural used in workshop #6 (W6-NL2Code) for the code autocompletion use case. We introduce the\n",
      "workshop and set ground rules in (a), introduce the Alex persona in (b), and show the model output for the use case in (c).\n",
      "Then, we ask participants for what they want to know about the GenAI model in (d), elicit design ideas for AI documentation\n",
      "in (e), and ask participants to ideate on what information Alex would want to know about his team members‚Äô use of the model\n",
      "in (f).\n",
      "‚Ä¢AI documentation is embodied by the recently-proposed con-\n",
      "cepts of FactSheets [8, 81], Model Cards [98], and About ML\n",
      "pages [78]. AI documentation is developed by AI model or\n",
      "system providers to give users information about its pur-\n",
      "pose, performance, safety, and provenance. Liao et al. [48]\n",
      "suggest that this type of feature could be used to address\n",
      "several categories of explainability needs, including data,\n",
      "output, performance, and how (global logic) questions. We\n",
      "designed a table with 4 categories of facts about the model,\n",
      "without providing specific information, as shown in Figure 2\n",
      "(a). This table served as a probe to elicit other categories of\n",
      "information that should be included in AI documentation\n",
      "for generative code models. We intentionally left the content\n",
      "incomplete to inspire design ideation from participants.\n",
      "‚Ä¢Uncertainty indicator is a feature to communicate the model‚Äôs\n",
      "uncertainty level about its output. Uncertainty is considered\n",
      "a critical form of transparency that can alert people about\n",
      "the limitations of AI [10]. In discriminative ML, uncertainty\n",
      "can take the form of a confidence score for a classification\n",
      "model or a prediction interval for a regression model [26].\n",
      "In the context of generative code models, we envisioned\n",
      "that uncertainty could be assessed and represented at the\n",
      "line level. This observation was motivated by a recent work\n",
      "by Agarwal et al . [2], which demonstrates how to derive\n",
      "line-level confidences by aggregating token probabilities. In\n",
      "the design probe (Figure 2 (b)), we communicate the line-\n",
      "level lack-of-confidence with an wavy underline when it\n",
      "falls below some uncertainty threshold.\n",
      "IUI ‚Äô22, March 22‚Äì25, 2022, Helsinki, Finland Jiao Sun et al.\n",
      "(a) AI documentation\n",
      " (b) Uncertainty Indicator\n",
      "(c) Attention Visualizer\n",
      "(d) Social Transparency\n",
      "Figure 2: Examples of UI probes used in the code translation use case. Probes used in the other use cases were similar in design\n",
      "but differed in the specific code examples used. Each design is translated from existing XAI approaches for discriminative\n",
      "AI. The probes were designed to elicit questions and spark discussion around Alex‚Äôs information needs for working with a\n",
      "generative code model.\n",
      "‚Ä¢Attention visualization is a type of local explanation to an-\n",
      "swer the why question [51]: why did the model produce a\n",
      "particular output for a given input? For NLP tasks using deep\n",
      "neural networks, a common approach to explain a predic-\n",
      "tion is to utilize the attention mechanism, which provides\n",
      "a distribution over attended-to input units [101] to indicate\n",
      "the relative importance of input units for the output. Prior\n",
      "work has shown how to use attention weights to visually\n",
      "depict which words in an input sequence, plus which words\n",
      "in previously-generated output, were responsible for a given\n",
      "token‚Äôs output [95, 96]. In our design probe (Figure 2 (c)),\n",
      "we show the user querying a portion of output (in red) and\n",
      "seeing which tokens were responsible for that output (in\n",
      "yellow, where opacity is used to signal the strength of the\n",
      "attention weight). The opacities in the figure are based on\n",
      "the actual cross-attention weights from the model we used\n",
      "to generate the output.\n",
      "‚Ä¢Social transparency makes transparent the social contexts\n",
      "around the usage of an AI system to help people better under-\n",
      "stand the AI and how to utilize it. This feature was inspired\n",
      "by a recent work by Ehsan and Riedl [21], which examined\n",
      "the impact of making other users‚Äô past interactions with\n",
      "a decision-support AI system transparent. To explore so-\n",
      "cial transparency in our context, we created an open-ended\n",
      "probe to invite ideation by visually emphasizing the fact that\n",
      "Alex is not working alone, but works within a group of other\n",
      "software engineers (Figure 2 (d)).\n",
      "3.3 Participants\n",
      "Recruitment. We advertised our study within a large interna-\n",
      "tional information technology company, by distributing a survey\n",
      "targeted at software engineers working across many product lines\n",
      "and locations. 81 people responded. In the recruiting survey, we\n",
      "asked questions about their demographic information, self evalu-\n",
      "ation of programming skills and previous experience of working\n",
      "with AI. In addition, we asked them to rate their interest in the\n",
      "6 candidate user cases of generative AI models for code (in addi-\n",
      "tion to the 3 selected use cases we included test case generation,\n",
      "documentaion generation and code repair).\n",
      "Selection Criteria. For code autocompletion and natural language\n",
      "to code use cases, we required participants to have 1+ year of Python\n",
      "programming experience. For the code translation use case, we\n",
      "required participants to have 1+ year of both Python and Java\n",
      "programming experience. Among all sign-up responses, 56 (69%)\n",
      "had at least 1+ year of Python programming experience. From this\n",
      "set, we prioritized to select those who have had experience with AI\n",
      "Investigating Explainability of Generative AI for Code through Scenario-based Design IUI ‚Äô22, March 22‚Äì25, 2022, Helsinki, Finland\n",
      "ID Use Case Participants\n",
      "1 Code Translation (CT) Software Engineer (P16, P20), Researcher/Scientist (P5, P52)\n",
      "2 Code Translation (CT) Software Engineer (P3, P62), Hardware Verification Engineer (P54),\n",
      "Data Scientist (P54), Researcher/Scientist (P25)\n",
      "3 Code Autocompletion (CA) Software Engineer (P2, P53 , P63), Data Engineer (P64), Researcher/Scientist (P66)\n",
      "4 Code Autocompletion (CA) Software Engineer (P17, P39), Researcher/Scientist (P37, P55),\n",
      "Bioinformatician (P48), Data Scientist (P61)\n",
      "5 Code Translation (CT) Researcher/Scientist (P19), Software Engineer (P46),\n",
      "Software Architect (P14, P45, P75), Data Scientist (P78)\n",
      "6 Natural language to code\n",
      "(NL2Code) Software Engineer (P44), Researcher/Scientist (P71), Data Scientist (P80, P81)\n",
      "7 Natural language to code\n",
      "(NL2Code)\n",
      "Software Engineer (P59, P15), Software Architect (P41, P42),\n",
      "Researcher/Scientist (P13), Data Scientist (P30)\n",
      "8 Natural language to code\n",
      "(NL2Code) Software Engineer (P7, P22), Data Scientist (P8, P32)\n",
      "9 Code Autocompletion (CA) Software Engineer (P57, P72), Data Engineer (P33)\n",
      "Table 1: Participants and their roles in our 9 brainstorming workshops about co-designing generative AI models for Code.\n",
      "Cyan indicates the participant does not have experience of working with AI. We will use abbreviations to refer to workshops\n",
      "throughout the paper, for example, W1-CT for workshop 1 about the CT (Code Translation) use case.\n",
      "systems, and we further narrowed down to 43 participants based\n",
      "on their time availability.\n",
      "Demographics. For the geographical location, the majority of\n",
      "our participants were from the United States (N=22; 65%) and India\n",
      "(N=14; 41%), with smaller numbers from Canada (N=3; 7%), Ger-\n",
      "many (N=2; 5%), Ireland (N=1; 2%) and Brazil (N=1; 2%). For the gen-\n",
      "der identity, 33 (77%) participants identified as male, 8 (19%) female,\n",
      "and 2 (5%) preferred not to answer. Our participants also had diverse\n",
      "job roles: 17 (40%) software engineers, 9 (21%) researchers/scientists,\n",
      "9 (21%) data scientists, 5 (12%) software architects, 1 data engineer,\n",
      "1 bioinformatician and 1 hardware verification engineer. Among\n",
      "them, 40 (93%) had experience working with AI in their jobs, while\n",
      "3 of them did not.\n",
      "Assignment. We then assigned participants to 9 workshops (3\n",
      "for each use case) as shown in Table 1, where we mark participants\n",
      "who do not have experience working with AI in blue.\n",
      "3.4 Analysis\n",
      "We conducted content analysis on the written content in Mural, and\n",
      "around 600 minutes of interviews were recorded and transcribed.\n",
      "In total, we extracted 487 messages from Mural and 249 paragraphs\n",
      "from the video transcript relevant to explainability. We followed the\n",
      "workshop structure, and coded the data in five parts: question elici-\n",
      "tation exercise, discussions around AI documentation, uncertainty\n",
      "indicator, attention visualiser and social transparency.\n",
      "To analyze the questions collected from the question elicitation\n",
      "exercise, we referred to prior work using prototypical questions to\n",
      "represent explainability needs by Liao et al. [48] in the context of\n",
      "discriminative ML systems, and ‚Äúintelligibility types‚Äù [53, 54] for\n",
      "context-aware intelligent systems by Lim and Dey, and used these\n",
      "existing definitions to guide our coding. We also paid attention to\n",
      "new types of questions that were not covered in these prior works.\n",
      "Two researchers first independently coded 106 questions collected,\n",
      "and discussed their codes to reach a consensus. Then, they updated\n",
      "their coding independently with the agreed codes. As a result, they\n",
      "reached a high level of agreement on 66 out of 71 messages (93%).\n",
      "After another iteration of discussion to finalize the coding schema,\n",
      "one of the two researchers coded the rest of questions. We discuss\n",
      "findings from this analysis in Section 4.\n",
      "For the rest of the workshops, we coded participants‚Äô feedback\n",
      "and design ideas with regard to the four design probes. Our goal\n",
      "is to further understand users‚Äô explainability needs in the context\n",
      "of generative AI, as well as to inform concrete XAI designs that\n",
      "can address these needs. This part of coding was conducted by one\n",
      "researcher with frequent discussions with the other researchers.\n",
      "We discuss these findings in Section 5.\n",
      "4 EXPLANABILTY NEEDS FOR GENAI FOR\n",
      "CODE\n",
      "This section presents the results from the question elicitation exer-\n",
      "cise to understand what types of explainability needs particpants\n",
      "had for the GenAI for code use cases. As mentioned, we followed\n",
      "the definitions of prototypical user questions in XAI Question Bank\n",
      "by Liao et al. [48] and ‚Äúintelligibility types‚Äù in Lim and Dey [53] to\n",
      "code the following categories: Input, Output, Performance, Data,\n",
      "How (global), Why/Why NOT, How to, What if, Control. We fur-\n",
      "ther identified two new categories that were not covered by the\n",
      "prior works: System Requirements and Impact, and Limitations. In\n",
      "Table 2, we present these question categories, with their definitions,\n",
      "and example questions asked by participants. They are ordered by\n",
      "IUI ‚Äô22, March 22‚Äì25, 2022, Helsinki, Finland Jiao Sun et al.\n",
      "Category & Description Exemplar Questions\n",
      "Input‚àó. Questions about the kinds of input that the model can take\n",
      "or should be given.\n",
      "- What kind of input can the AI reasonably generate code from?\n",
      "- What types of [data types, data structures, algorithm...] does the AI\n",
      "understand?\n",
      "- How specific does the input need to be to get a good output?\n",
      "- Is there certain thing I need to specify to get a good output?\n",
      "Output. Questions about what the AI can produce, such as the\n",
      "output type, scope and system capabilities.\n",
      "- Will this produce idiomatic Python code, or a 1:1 translation?\n",
      "- Will the translated code have the same [variable name, complexity, etc.]\n",
      "as the original?\n",
      "- Will there be automated tests generated?\n",
      "- Can the model generate code that does error/exception\n",
      "handling correctly?\n",
      "How (global). Questions about how the model operates to have a\n",
      "global understanding.\n",
      "- How exactly does it generate code?\n",
      "- How does the translation enforce the dynamic typing rules of Python?\n",
      "- Can this AI use design patterns?\n",
      "- Does the AI optimize for big O?\n",
      "Performance. Questions about the quality of AI generated artifacts\n",
      "or the runtime performance (e.g. time taken) of the model.\n",
      "- How correct is the translation guaranteed to be?\n",
      "- How confident is the AI about the solution?\n",
      "- How will the performance be for code of larger inputs?\n",
      "- How much time will it take to translate the code?\n",
      "How to . Questions about how to change the input to affect the\n",
      "quality or characteristics of the output\n",
      "- Can I give hints or specify my problem better to improve the model‚Äôs\n",
      "output?\n",
      "- How can I get the AI to write more efficient code?\n",
      "- How can I optimize for one type of output over another?\n",
      "Control‚àó. Questions about options to customize or specify prefer-\n",
      "ences for how the model should work.\n",
      "- Can I select a preference for [datatypes, packages, etc.] used?\n",
      "- Can I tune the translation?\n",
      "- Can I set specific regional settings?\n",
      "Why/Why not. Questions about why the model produced a given\n",
      "output or why an input failed to produce the desired output.\n",
      "- Why didn‚Äôt my input work?\n",
      "- How did the AI recognize the function from comments in this task?\n",
      "- Why did the AI think that its code satisfied the requirement?\n",
      "- Why did that 4th attempt with the least programming thinking give a\n",
      "good result?\n",
      "Data. Questions about the characteristics and provenance of the\n",
      "the data on which the model was trained.\n",
      "- What data was this trained on?\n",
      "- What kind of training set was the AI built with?\n",
      "- Where does the code data the AI was trained on come from?\n",
      "System Requirements & Impact ‚Ä†. Questions about the require-\n",
      "ments to use the system or its impact, to gauge the appropriate\n",
      "conditions of usage.\n",
      "- What are the hardware requirements for using the system?\n",
      "- Can I use the system in [a closed source project, my development\n",
      "environment, etc]?\n",
      "- What is the energy consumption for using the model?\n",
      "Limitations‚Ä†. Questions about the limitations of the model‚Äôs capa-\n",
      "bilities.\n",
      "- What limits are there to the model‚Äôs function?\n",
      "- What scenarios does it cover or not cover?\n",
      "What if . Questions about what the output would be if the input\n",
      "changes or in hypothetical situations.\n",
      "- What to return if the input is invalid?\n",
      "- What if I am translating from a language that does not allow overflow.\n",
      "Table 2: Question categories that emerged in our workshops. We applied the definitions of prototypical user questions in\n",
      "XAI Question Bank by Liao et al. [48] (in the context of discriminative ML, mainly for decision-support) and ‚Äúintelligibility\n",
      "types‚Äù in Lim and Dey [53] (for context-aware intelligent systems). Categories marked with a asterisk ( ‚àó) are categories only\n",
      "appeared in Lim and Dey [53] not Liao et al. [48]. Categories with dagger ( ‚Ä†) represent new categories emerged in our GAI for\n",
      "code context. For each question category, we provide the definition we used for coding and a few example questions asked by\n",
      "participants.\n",
      "Investigating Explainability of Generative AI for Code through Scenario-based Design IUI ‚Äô22, March 22‚Äì25, 2022, Helsinki, Finland\n",
      "the frequency of being asked by participants. Categories unique\n",
      "to the GenAI for code are marked with a dagger ( ‚Ä†) sign. Cate-\n",
      "gories that appeared in Lim and Dey [53] but not Liao et al. [48]\n",
      "are marked with an asterisk ( ‚àó) sign. Below we enumerate each\n",
      "question category and what we can learn about users‚Äô explainability\n",
      "needs with GenAI for code.\n",
      "Input. 3 To understand what kind of inputs the model can take\n",
      "or should be given was the most prominent explainability needs of\n",
      "participants, making up about 16% of all questions. On one hand,\n",
      "participants wanted to have an overview of the types or scope of\n",
      "inputs the AI can work with. Some asked about the AI‚Äôs ability to\n",
      "process types of programming languages, data types, algorithms,\n",
      "language versions, and so on. On the other hand, participants were\n",
      "eager to know how tooptimize their inputs to produce better outputs.\n",
      "For example, participants in W4-CA raised the question ‚ÄúShould\n",
      "Alex write smaller functions that are easier to name or extensive\n",
      "documentation for complex functions?‚Äù and ‚ÄúCan I describe the entire\n",
      "problem in a single well-written sentence? or will I always need to\n",
      "break down the problem into small natural language chunks‚Äù asked\n",
      "by W6-NL2Code.4 Participants‚Äô questions reflected their current\n",
      "mental models on how the GenAI works to process inputs, which\n",
      "were not necessarily accurate, suggesting that explanations may\n",
      "need to start from a high-level overview of how GenAI works in\n",
      "a specific code generation use case. It is interesting to note that\n",
      "this category is not a prominent need for decision-support AI using\n",
      "descriminative ML, since the input is often fixed or implicit. In\n",
      "contrast, the great variability and close coupling between input and\n",
      "output for GenAI made it a primary explainability needs to address.\n",
      "Output. To understand what output the GenAI can produce is\n",
      "another frequent explainability need. Participants were mainly\n",
      "interested in the characteristics of the output code, and the scope of\n",
      "the output or system functions. Understanding the characteristics\n",
      "of the output can help users determine how to utilize the output,\n",
      "for example P5 from W1-CT commented: ‚Äúyou cannot ask Python\n",
      "to be as efficient as Java, but at the same time, Python may be more\n",
      "convenient than Java in some specific situations... ‚Äù . Understanding the\n",
      "characteristics can also guide users to assess the quality of output,\n",
      "identify potential shortcomings or errors for further actions, as\n",
      "commented by P19 from W4-CA: ‚Äúif it‚Äôs translating from a language\n",
      "like RUST that doesn‚Äôt allow overflow, it‚Äôs not possible into a language\n",
      "like Java allows it. ‚Äù . Meanwhile, some participants asked about the\n",
      "scope of the output, as to discover what the GenAI can do, such\n",
      "as whether it can generate test cases or multiple candidates as\n",
      "alternatives.\n",
      "How (global). Participants also recognized the importance of\n",
      "having a a global understanding of the GenAI. Many wanted to\n",
      "3We adopt a similar definition of Input in Lim and Dey [53], as in what kind of inputs\n",
      "the model can take to generate code. Input was mentioned in Liao et al. [48] as training\n",
      "data went into the model. We consider that kind of question under the category of\n",
      "‚ÄúData‚Äù.\n",
      "4Please see Table 1 for an explanation of the meaning of ‚ÄùCA‚Äù and ‚ÄùNL2Code‚Äù. The\n",
      "prepended ‚ÄùW‚Äù number is the number of the workshop in the table. Also, we some\n",
      "of our transcribed comments came from the audio channel of workshop recordings,\n",
      "and we could not reliably determine who had spoken. Other comments recorded on\n",
      "sticky-notes could be attributed. Therefore, we report participants‚Äô comments in terms\n",
      "of which workshop they occurred in, and we do not always know which participant\n",
      "actually said them. We report participant IDs where we could determine them.\n",
      "have a high-level understanding of how the codes are generated ,\n",
      "such as ‚Äúhow exactly does it generate code? is it pulling from sites or\n",
      "using GANs to generate new code‚Äù from W3-CA and ‚Äúhow the model\n",
      "comprehends the user input‚Äù from W6-NL2Code. Some raised these\n",
      "questions based on their expectations of the input and output of\n",
      "the model (e.g., ‚ÄúPython is dynamically typed, Java is static, how\n",
      "do you enforce these typing rules?‚Äù from W1-CT. Others inquired\n",
      "about how the model deals with specific types of input , such as ‚ÄúHow\n",
      "untagged text is used for analysis‚Äù from W9-CA. These questions\n",
      "reflected interests in both a high-level description of how code\n",
      "generation works, and examining detailed model logic. For descrip-\n",
      "tive ML, global explanations often take the forms of showing how\n",
      "the model weighs different features or describing the general rules\n",
      "the predictions follow. How to communicate such a global view of\n",
      "model logic for GenAI is an open technical challenge.\n",
      "Performance. Many participants were concerned about under-\n",
      "standing the performance, i.e. how well the GenAI works for the\n",
      "use case. These questions are critical for users to develop appropri-\n",
      "ate trust and adopt the system. We found the questions are mainly\n",
      "concerned about theoverall performance, the quality of a specific gen-\n",
      "erated code and the run-time efficiency of the model (e.g., inference\n",
      "time of the model, if the AI supports multi-threading). It is interest-\n",
      "ing to note that ‚Äúquality of code‚Äù may be multi-faceted as partici-\n",
      "pants mentioned correctness, runtime complexity, space complex-\n",
      "ity, understandability, and testability. Many of these performance-\n",
      "related questions were asked to understand performance differ-\n",
      "ences and limitations with regard to different types of input (e.g.\n",
      "larger code). The interest in run-time performance is a unique need\n",
      "emerged in the GenAI for code context.\n",
      "How to. These questions inquired about how to change or im-\n",
      "prove the input to get a better output. Answers to these questions\n",
      "can help participants choose strategies to make better use of the AI,\n",
      "such as ‚ÄúHow to [make] requirement better [to improve the output]‚Äù?\n",
      "from W9-CA and ‚ÄúHow can I get the AI to write efficient code?‚Äù from\n",
      "W6-NL2Code.\n",
      "Control. These questions were regarding options to customize or\n",
      "specify preferences for how the model should work. This category\n",
      "was not seen in Liao et al. ‚Äôs work defining explainability needs\n",
      "for decision-support descriminative ML. Emergence of this type\n",
      "of question in our context suggests that users of GenAI for code\n",
      "are interested in having more control on the working of the model,\n",
      "as illustrated by P61 from W1-CT saying that ‚ÄúOftentimes, you‚Äôre\n",
      "not using the tool for just basic examples. I guess more in depth, like,\n",
      "explanations of what you can do with the model would be appreciated. ‚Äù\n",
      "Why/Why Not (local). Different from asking How (global) ques-\n",
      "tions to understand the overall process or logic of the model, aWhy\n",
      "(local) question was asked to understand a specific output from the\n",
      "model. More often than not, the Why question was triggered by a\n",
      "surprising or suspicious output, such as ‚ÄúWhy did the AI think the\n",
      "code satisfied the requirement?‚Äù at W7-NL2Code or ‚ÄúHow did the\n",
      "AI recognize the function from comments in this task?‚Äù at W4-CA.\n",
      "Sometimes the Why question was asked in contrast to the expected\n",
      "output, and hence a Why Not (the expected output) question, such\n",
      "as ‚ÄúWhy didn‚Äôt my input work?‚Äù from W3-CA.\n",
      "IUI ‚Äô22, March 22‚Äì25, 2022, Helsinki, Finland Jiao Sun et al.\n",
      "Data. Some participants brought up questions regarding the\n",
      "training data, especially regarding the provenance of the data, such\n",
      "as ‚ÄúWhere does the code the AI was trained on come from?‚Äù from W3-\n",
      "CA. Echoing findings in Liao et al. [48], explanations on training\n",
      "data can help users gauge the capability of the model and its proper\n",
      "usage, by the validity of the training data and their alignment with\n",
      "ones‚Äô own programming tasks. How to define alignment of training\n",
      "data in the context of GenAI, however, remains an open question.\n",
      "System Requirement and impact. A new category emerged in our\n",
      "data that was not covered in prior works. This category is questions\n",
      "regarding requirements or impact of the system, which can help\n",
      "users gauge the appropriate conditions of usage. They were asked\n",
      "regarding the usage of the system instead of the underlying model.\n",
      "These questions were highly specific to the software engineer con-\n",
      "text, including compatible development environments, software\n",
      "and hardware requirements, dependencies, and so on.\n",
      "Limitations. Another category unique in our data is questions ex-\n",
      "plicitly asking about the limitations of the model or system, such as\n",
      "what kind of scenarios it cannot cover or limitations to the model‚Äôs\n",
      "functions. These questions emerged likely due to the complexity of\n",
      "the input and output spaces of GenAI, which is less straightforward\n",
      "to comprehend than a decision-support AI system.\n",
      "What if. Some asked about what the output would be given hypo-\n",
      "thetical changes to the input, which may further help participants\n",
      "understand how the AI makes decisions in a counterfactual manner,\n",
      "such as ‚Äúwhat to return if the input is invalid?‚Äù from W5-CT.\n",
      "To summarize, we identified 11 categories of explainability needs\n",
      "in GenAI for code use cases. Four of these categories did not appear\n",
      "in Liao et al. as prominent needs for descriminative ML used for\n",
      "decision-support: Input, Control, System Requirements & Impact\n",
      "and Limitations. The top five most frequent categories are Input,\n",
      "Output, How (global), Performance and How to. In Section 6, we\n",
      "will further discuss insights revealed about explainabilty needs\n",
      "that are unique to the GenAI technology and the novel use case of\n",
      "supporting code generation.\n",
      "5 XAI FEATURES FOR GENAI FOR CODE\n",
      "As discussed in Section 3.2, we proposed four types of XAI features\n",
      "for GenAI for code and created design probes to elicit ideation\n",
      "from participants: AI documentation, uncertainty indicator, atten-\n",
      "tion visualizer, and social transparency. In this section we discuss\n",
      "participants‚Äô response to derive concrete design recommendations.\n",
      "5.1 AI Documentation\n",
      "AI documentation is advocated to increase the transparency and\n",
      "facilitate understanding of AI [ 35, 36, 40, 75, 76, 82], and is em-\n",
      "bodied in recently proposed features of AI Factsheets [8, 81] and\n",
      "Model Cards [64]. However, what documentation should include\n",
      "for generative AI models is under-investigated, let alone GenAI for\n",
      "code use cases. We are interested in what categories of information\n",
      "about the model should be presented in AI documentation for GenAI\n",
      "for code , and how they might differ from that of discriminative ML.\n",
      "We used a low-fidelity UI design of a factsheet with intentionally\n",
      "open-ended content, as shown in Figure 2a, and asked participants\n",
      "to brainstorm AI documentation categories that could help the user\n",
      "in the design scenario.\n",
      "We summarize the categories identified from data analysis in\n",
      "Table 3, ranked by the frequencies of mentions by participants.\n",
      "In particular, we identified categories that surfaced in the context\n",
      "of GenAI for code that were not discussed in previous works on\n",
      "AI FactSheets or Model Cards: Examples and tutorials , Software\n",
      "engineering capabilities , Output code quality and utility , Supported\n",
      "languages and frameworks , Control and Deployment Requirement &\n",
      "Platform.\n",
      "Consistent with findings from the question elicitation exercise\n",
      "in Section 5, participants expressed strong interests in understand-\n",
      "ing the scopes of inputs and outputs of the system. Examples and\n",
      "tutorials, as the most frequently mentioned category, suggests that\n",
      "example-based explanations can be a fruitful area to explore for\n",
      "GenAI to help users understand the model inputs and outputs. Such\n",
      "interests are also expressed as requirements specific for software\n",
      "engineering tasks, including Software engineering capabilities and\n",
      "Supported languages and frameworks . Participants also wished to\n",
      "see information about performance, data, control, system deployment\n",
      "requirement, and global model explanations , suggesting that provid-\n",
      "ing documentation could help address corresponding explainability\n",
      "needs found in Section 5.\n",
      "Another interesting pattern is that participants requested cate-\n",
      "gories of information regarding the generated code artefacts rather\n",
      "than the model itself (see the second column in Table 3). Many\n",
      "demanded to see metrics about the code quality and utility , which\n",
      "should be differentiated from model performance metrics such as\n",
      "accuracy. While the latter are commonly calculated with some\n",
      "ground-truth in a held-out test dataset, the former are concerned\n",
      "with characterizing the generated code and their utility for improv-\n",
      "ing human productivity.\n",
      "In general, documentation for GenAI for code should be trans-\n",
      "parent about the quality of the produced code from the software\n",
      "engineering perspective, and align with the professional culture\n",
      "and standards, such as communicating the license and regulatory\n",
      "requirements in both the training data acquisition and the usage of\n",
      "the produced code. Table 3 is not meant to serve as an exhaustive\n",
      "template for AI documentation of generative AI models for code.\n",
      "Instead, we hope that these discovered categories can inspire what\n",
      "to document for generative AI models for code.\n",
      "5.2 Model Uncertainty\n",
      "Inspired by previous work on model uncertainty of generative AI\n",
      "models for code [100] and communicating uncertainty of output\n",
      "for discriminative ML models [10, 26], we created the design probe\n",
      "in Figure 2b to communicate uncertainty for a line of AI generated\n",
      "code, indicated by a wavy underline. We asked participants to react\n",
      "to the feature and discuss how to improve the feature, including\n",
      "follow-up interactions, to better support users‚Äô understanding and\n",
      "their usage of the AI system .\n",
      "Their suggested improvement and follow-up interactions can be\n",
      "categorized asAI initiated and human initiated [38, 74]. For the AI to\n",
      "take the initiative, participants suggested to see more information\n",
      "from the model to help them understand the output and edit if\n",
      "necessary, for example to include suggesting alternative outputs,\n",
      "Investigating Explainability of Generative AI for Code through Scenario-based Design IUI ‚Äô22, March 22‚Äì25, 2022, Helsinki, Finland\n",
      "Category Applied to Definition\n",
      "Examples &\n",
      "Tutorials‚Ä†\n",
      "Models Examples of input-output pairs generated by the model; Tutorials on how to use the model\n",
      "effectively, including what kinds of input can get high-quality outputs\n",
      "Software Engineering\n",
      "Capabilities‚àó\n",
      "Models Description of software engineering features or capabilities that the AI can support (e.g., version\n",
      "information, stress testing for large loads, dependency handling, data structure, kernel status,\n",
      "coding style)\n",
      "Model Performance Models Technical evaluation metrics of the generative model, including accuracy, performance, perfor-\n",
      "mance change by types of input, CPU consumption, and model inference time\n",
      "Output Code Quality\n",
      "and Utility‚àó‚Ä†\n",
      "Outputs Metrics characterizing the generated code, including correctness, lint errors, code efficiency, time\n",
      "complexity; Metrics reflecting the system‚Äôs impact on human productivity, including estimated time\n",
      "savings in conducting programming tasks, estimated improvements to code quality, comparisons\n",
      "with other kinds of GenAI for code tools\n",
      "Supported Languages\n",
      "& Frameworks‚àó\n",
      "Models List of programming and/or human languages which the model is capable of understanding (e.g.\n",
      "as input) or producing (e.g. as output); List of programming frameworks or APIs which the model\n",
      "supports as input or output (e.g. React, Flask)\n",
      "Data Models Information about what data the model was trained on, including its provenance and any applicable\n",
      "privacy policies, data usage guidelines, or code licences\n",
      "Control ‚Ä† Models Description of customization options or other mechanisms for users to control the output of the\n",
      "model; Description of how the model can be fine-tuned for additional use cases\n",
      "Deployment\n",
      "Requirements &\n",
      "Platform‚àó\n",
      "Models Technical requirements for hosting the model, including: software dependencies, hardware re-\n",
      "quirements, cloud-hosting requirements, and supported IDE integration\n",
      "Model Explanations Models Explanation of how the model operates (e.g. a basic description of how transformer models work\n",
      "or a visualization of model attention)\n",
      "Usage Rights Outputs Information about usage restrictions and/or licensing terms for code produced by the model\n",
      "Optimal & Poor\n",
      "Conditions\n",
      "Models The conditions under which the AI model performs well or performs more poorly than expected\n",
      "Intended Usage Models The use cases supported by the model; Other potential use cases that might be implemented via\n",
      "fine-tuning\n",
      "Table 3: Categories of AI documentation for GenAI for code models and their outputs. Categories are sorted in descending\n",
      "order, based on the frequency of mention by participants. Categories marked with a ‚Ä†or ‚àóhave not been identified by previous\n",
      "works in AI documentation. We postulate that categories marked with a dagger ( ‚Ä†) emerged due to the context of generative\n",
      "models, and categories marked with an asterisk ( ‚àó) emerged due to the context of software engineering support.\n",
      "providing uncertainty explanations, and giving more fine-grained\n",
      "uncertainty. We elaborate on each category below.\n",
      "Alternative outputs. Participants wanted transparency about what\n",
      "other alternatives the AI model had considered as output. Some\n",
      "participants had low expectations of generated code and hoped to\n",
      "gain insights from more options, as P63 from W3-CA stated that ‚ÄúI\n",
      "voted for the multiple [version] candidates, which I think is just gen-\n",
      "erally helpful. Even if the top generated candidate was not quite right\n",
      "maybe one of the top five will be. Or will be better for whatever the\n",
      "particular use cases. ‚Äù. Some participants hoped to see alternatives\n",
      "with specific characteristics (e.g., better readability or optimization),\n",
      "as P80 from W6-NL2Code stated: ‚ÄúObviously if human knows about\n",
      "the time complexity and the optimization of the code, he can fetch\n",
      "a better one. And at present, we do not know whether [the] AI [is]\n",
      "trying to be more on the readable side or more on the optimization\n",
      "side. ‚Äù\n",
      "Uncertainty explanation. Participants desired to see uncertainty\n",
      "explanation from the model, on why a particular part of the code\n",
      "was highlighted or why the AI was uncertain. This is an under-\n",
      "explored area in XAI. Participants suggested that the sources for\n",
      "uncertainty explanations could be what alternative options the\n",
      "model was struggling with (W8-NL2Code), or what goal or rationale\n",
      "the model follows (W6-NL2Code). We quote P71 from W6-NL2Code\n",
      "who proposed an interactive explanation to address this point:\n",
      "‚ÄúI‚Äôve voted twice for an indication of what the model is\n",
      "trying to achieve, because I think that‚Äôs very important.\n",
      "It could be very simple. It could be as simple as just\n",
      "reminding you exactly what natural language prompt\n",
      "this was derived from, so you can think to yourself:\n",
      "IUI ‚Äô22, March 22‚Äì25, 2022, Helsinki, Finland Jiao Sun et al.\n",
      "‚Äòwhat was it trying to do?‚Äô Uh, based on what I wrote,\n",
      "cause I know what I wrote, and I know what that means\n",
      "and then I can rewrite it from there or it could be more\n",
      "complex. It could, you know, if there‚Äôs a serious natural\n",
      "language model behind this, maybe it‚Äôs capable of gen-\n",
      "erating speech back to me about what this code exactly\n",
      "does and then you can compare that yourself. ‚Äù\n",
      "Another path to explain uncertainty suggested by participants\n",
      "was through highlighting corresponding parts that contributed to\n",
      "the uncertainty. The corresponding parts could be from input, pre-\n",
      "viously generated lines, or training data.\n",
      "Fine-grained uncertainty. Some participants wanted to see more\n",
      "than just a wavy line under outputs that are below an uncertainty\n",
      "threshold. For example, P71 from W6-NL2Code said: ‚Äú...if there\n",
      "is some confidence percentage that could be helpful to, you know,\n",
      "like Green Zone, Red zone and Yellow zone etc., to give an idea of\n",
      "the highest priority I need to fix this. ‚Äù Related to the observation\n",
      "that code quality is multi-faceted, some participants wanted to\n",
      "understand the specific definition or aspect of uncertainty, such as\n",
      "whether it is regarding the correctness, time complexity, or runtime.\n",
      "Another area of suggested interactions requires the human to\n",
      "take the initiative, including allowing human input to resolve uncer-\n",
      "tainty and supporting interactive testing for the uncertain regions,\n",
      "as discussed below.\n",
      "Human input to resolve uncertainty. A frequently requested inter-\n",
      "action was that the AI waits for human input to resolve uncertainty.\n",
      "This could be realized by the AI prompting the human with a vari-\n",
      "ety of questions, including confirmation (W3-CA), clarification (e.g.,\n",
      "‚Äúshow did you mean ... options‚Äù from W1-CT), inspection (e.g., ‚ÄúTell\n",
      "user to examine the line carefully and verify‚Äù from W4-CA), prefer-\n",
      "ences or how they would code (e.g., ‚ÄúAsk user for decision making ‚Äì\n",
      "‚Äòdo you want to keep this line or not‚Äô and highlight it with reasoning‚Äù\n",
      "from W5-CT), and immediate modification (e.g., ‚ÄúDialog box to type\n",
      "preferred translation (if Alex knows python‚Äù) from W2-CT)).\n",
      "Interactive testing. Another interaction that participants expected\n",
      "to perform is interactive testing for situations in which the model\n",
      "was uncertain, which can also facilitate understanding of the AI\n",
      "model. P81 from W6-NL2Code said that ‚Äú... If we can probably give\n",
      "a similar interactive component [as Jupyter Notebook] where it gives\n",
      "you the option to fetch a context from the previous clients, and then\n",
      "run only that piece of code to see what the output is. Probably the\n",
      "person can make a better decision on whether to keep it or change. ‚Äù\n",
      "The overall feedback from participants was that showing a line-\n",
      "level uncertainty indicator would be useful to guide users‚Äô atten-\n",
      "tion to potentially low-quality code. However, showing uncertainty\n",
      "alone was not enough to satisfy their explainability needs. Many of\n",
      "the suggested interactions also serve the purpose of helping users\n",
      "better understand the model behaviors through interactive testing,\n",
      "alternative output it considered, and explaining the uncertainty.\n",
      "Through these interactions, users can develop a more appropri-\n",
      "ate mental model of the system that can help them interact more\n",
      "effectively in the long run.\n",
      "5.3 Attention Distribution\n",
      "Attention distribution is a popular approach to explain an individual\n",
      "prediction made by a neural networks model, by the relative impor-\n",
      "tance of parts of input to determine the output [95, 96]. Mapping\n",
      "to the context of GenAI for code, we designed the UI in Figure 2c,\n",
      "where the user can select a span of generated content. The AI can\n",
      "then highlight some spans in the previous content, to illustrate the\n",
      "idea of attention when making local decisions, indicating different\n",
      "attention weights via different opacity. We showed it to partici-\n",
      "pants to gather their response on the utility of attention distribution\n",
      "visualizer for users of GenAI for code .\n",
      "In general, participants said that the attention features would\n",
      "be useful to help them understand how the model worked, and to\n",
      "guide them in modifying the input to produce better results. P42\n",
      "from W7-NL2Code said that ‚Äú It‚Äôs what trains me as the user to use\n",
      "the AI better‚Äù and P63 from W3-CA said that ‚Äúif you see an issue,\n",
      "you might be able to say, oh, I can go back and try tweaking this part\n",
      "of the input and see whether I can get a better result. So that could be\n",
      "useful. ‚Äù\n",
      "Participants also proposed potential ways to improve this local\n",
      "explanation feature. For example: 1) line-level interpretation might\n",
      "be more appropriate than the span level, as P53 in W3-CA said:\n",
      "‚Äúselecting a span may not be the best option for a user interface;\n",
      "potentially a line is better‚Äù ; 2) being able to make immediate changes\n",
      "‚Äùright after identifying the suspicious spots‚Äù via such a tool (P55 in\n",
      "W4-CA); 3) syntax tree driven selection so that ‚Äúif you click on the\n",
      "range, you should get the whole function not only the span‚Äù (P63 in\n",
      "W3-CA); 4) adding natural language explanation in addition to the\n",
      "color coding (P71 in W6-NL2Code and P22 in W8-NL2Code).\n",
      "Moreover, participants saw value in utilizing user edits or inter-\n",
      "action with attention distribution as signals to improve the model\n",
      "or future interactions, for example, collecting the user edits, as P63\n",
      "in W3-CA said that ‚Äúit might be more useful for how do we improve\n",
      "the model? Cause if somebody... starts typing in, here‚Äôs the code that\n",
      "I really wanted here. And you can see how does that align and say,\n",
      "wait, why isn‚Äôt this aligning? It might be more useful is user feedback\n",
      "back to the model builders. ‚Äù\n",
      "5.4 Social Transparency\n",
      "Social transparency (ST) in AI system is recently proposed by Ehsan\n",
      "et al. [19], built on the concept of social transparency in social\n",
      "computing[90]. ST makes visible socio-organizational factors that\n",
      "influence the use of AI by presenting other users‚Äô interactions with\n",
      "the AI, aiming to facilitate a better understanding of and effective\n",
      "actions with AI output. To explore ST in the context of GenAI\n",
      "for code, we introduced an open-ended probe in Figure 2d: an\n",
      "image highlighting that Alex was not working alone but together\n",
      "with a team of software engineers, all using the GenAI. We asked\n",
      "participants to ideate on what users might want to know about\n",
      "other team members‚Äô interactions with the AI to help them better\n",
      "understand and use the AI . We asked participants to write down\n",
      "their ideas in Mural, and then did the clustering, voting and sharing\n",
      "again. We collected 111 responses about ST from Mural, grouped\n",
      "them to categories, and sorted them based on the frequencies of\n",
      "their occurrences. With the open-ended probe, we were able to\n",
      "elicit user needs for ST in the broad context of software engineer\n",
      "Investigating Explainability of Generative AI for Code through Scenario-based Design IUI ‚Äô22, March 22‚Äì25, 2022, Helsinki, Finland\n",
      "work with the participation of GenAI for code. We therefore map\n",
      "the findings to a stage model of the software development life\n",
      "cycle based on Rani [79]: 1) Requirement Analysis; 2) Design; 3)\n",
      "Implementation; 4) Testing; 5) Deployment and Maintenance [79].\n",
      "At the stage of requirement analysis, participants wanted to\n",
      "know about the team information of who, and for what, will be\n",
      "using the AI, such as business requirements and goals of the team,\n",
      "information about each team member (e.g., programming skills\n",
      "and experience), their interaction patterns with the AI (e.g., who is\n",
      "frequently using AI and who is better at using AI) and the reason\n",
      "why they are using the AI. Knowing the team information early on\n",
      "could help users better coordinate and interpret ST information in\n",
      "later stages.\n",
      "Information about other people‚Äôs experience with the AI could\n",
      "help at the design stage so that users could better design their pro-\n",
      "gramming tasks with the GenAI, including taking precautions on\n",
      "when and how they need to take more control on the code gener-\n",
      "ation. It is especially helpful to learn about the AI‚Äôs performance,\n",
      "capability and limitations from others‚Äô experience at this stage. For\n",
      "example, participants wanted to know how other people had evalu-\n",
      "ated the performance of the model and their judgement of the code\n",
      "quality, including how much time they invested in getting started\n",
      "with AI, how the AI-generated code compared to human-written\n",
      "code, together with the strengths and weaknesses of the AI. More-\n",
      "over, they were interested in seeing limitations, such as common\n",
      "pitfalls and errors that other members had experienced.\n",
      "Going into the development stage, participants wanted to have\n",
      "information about similar tasks or requests of other people to better\n",
      "understand what the AI model was capable of doing, even re-use the\n",
      "generated content, to help them better manage the programming\n",
      "tasks. Some participants expressed a desire to always have clarity on\n",
      "the authoring entity of code (whether from AI, a colleague, or other\n",
      "resources) throughout the development stage to better manage their\n",
      "expectations, actions as well as accountability.\n",
      "At the testing stage, participants wished to know how other\n",
      "people had tested the AI model as a reference, including what\n",
      "measures they had used in their evaluation of code quality. Many\n",
      "requested to understand AI‚Äôs impact on humans at this stage, i.e.\n",
      "how the introduction of the AI influenced the productivity of the\n",
      "team members, such as team speed and productivity. They also\n",
      "wanted to know whether and how the AI-generated code might be\n",
      "added to the codebase.\n",
      "Finally, during the deployment and maintenance stage, partic-\n",
      "ipants asked for information about how to customize the model\n",
      "parameters and monitored metrics based on the preferences of the\n",
      "team, and how to collect the team‚Äôs edits and feedback to improve\n",
      "the model.\n",
      "These results move beyond Ehsan et al. [19]‚Äôs work that defined\n",
      "ST features at the point of AI decision-support. Our results open\n",
      "up the design space to embed rich ST features at different stages of\n",
      "software development. ST features in the context of GenAI for code\n",
      "can not only facilitate users to better understand and use the AI,\n",
      "but also potentially foster a more effective ‚Äúhuman-AI assemblage\"\n",
      "where GenAI is introduced as a member or assistant of a software\n",
      "engineer team. We encourage future work to further explore the\n",
      "design space.\n",
      "6 DISCUSSION\n",
      "6.1 Informing XAI approaches for GenAI for\n",
      "code\n",
      "The rich and complex inputs and outputs of GenAI, and the tight\n",
      "coupling of the two, make it a prominent need for users to form a\n",
      "clear mental model on the scope, characteristics, and limitations\n",
      "of the input and output spaces, as well as a global view of how\n",
      "the model generates outputs from inputs. The observation that\n",
      "participants‚Äô explainability needs focused overwhelmingly on in-\n",
      "put, output and global explanations, suggests a mismatch between\n",
      "what users of GenAI applications need and the technical commu-\n",
      "nity‚Äôs current focus on explanations for representation learning\n",
      "and visualizing representations.\n",
      "The explainability categories we identified have varied technical\n",
      "feasibility with current techniques, and point to topics that are\n",
      "under-explored for generative AI. For example, for the Performance\n",
      "category, existing works have used the Computational Accuracy\n",
      "metric to evaluate generative code models [9, 15, 33, 88], but not\n",
      "other metrics we uncovered regarding the characteristics of the gen-\n",
      "erated artifacts and run-time efficiency. To understand performance\n",
      "differences and limitations with regard to different types of input,\n",
      "solutions have been explored for natural language generation under\n",
      "Prompt Engineering [57, 58]. Similar studies in the code space are\n",
      "limited to the number of few-shot examples in the prompt, and the\n",
      "effect of length of prompt on performance [9, 15]. A more detailed\n",
      "analysis of these phenomenon, better grounded in the semantics\n",
      "of programming languages, could be a promising area for future\n",
      "research. For the Control category, it is feasible to directly interact\n",
      "with model parameters. However, questions of how to elicit human\n",
      "feedback or edits and incorporate them to train generative models\n",
      "are currently under explored.\n",
      "XAI for GenAI needs to be contextualized by the characteristics\n",
      "of the generated artifacts, and the practical and cultural contexts\n",
      "to use these artifacts, i.e. the software engineering domain in our\n",
      "case. For example, participants were interested in understanding\n",
      "the input and output spaces with regard to supported language,\n",
      "frameworks, data structures, and so on. They are also interested in\n",
      "metrics reflecting various aspects of code characteristics and impact\n",
      "on human productivity rather than technical accuracy alone.\n",
      "Participants‚Äô questions reflected their desire for an actionable\n",
      "or utility-oriented understanding to support their end goal of opti-\n",
      "mizing code generation and programming productivity [18, 50, 52],\n",
      "such as asking the Input, Output, How and How-to XAI questions\n",
      "(described in Section 4) to facilitate strategies to get better outputs\n",
      "from the AI. This actionable understanding can also be supported\n",
      "by enabling follow-up actions towards their goals after seeing trans-\n",
      "parent information. For example, participants suggested many in-\n",
      "teractions to allow them to act on uncertainty information and\n",
      "improve the generated code.\n",
      "In short, explainability needs should be understood and ad-\n",
      "dressed with the ecological factors in mind, situated in an un-\n",
      "derstanding of the behavioral and social contexts of the system\n",
      "[41, 70]. We paid attention to the nature of software engineer tasks\n",
      "and workflows as part of the pragmatic human environment of the\n",
      "system. For example, participants demanded information regarding\n",
      "system requirements and impacts in relation to their broader work\n",
      "IUI ‚Äô22, March 22‚Äì25, 2022, Helsinki, Finland Jiao Sun et al.\n",
      "environment. The discussions around ST further suggested that\n",
      "users‚Äô explainability needs may vary at different stages of software\n",
      "engineering lifecycle, adding temporal context to the work contexts.\n",
      "We encourage future research to consider these contextual aspects\n",
      "while designing and evaluating GenAI explanations.\n",
      "6.2 Design implications for GenAI for code\n",
      "In our study, we observed a positive user reception towards natural\n",
      "language explanations and interactions with the GenAI as user\n",
      "inputs are programming- or natural-language based. We envision\n",
      "that a conversational agent interface could be a natural fit for AI\n",
      "assistant for code or co-programming tools, as explored in recent\n",
      "work [42, 43]\n",
      "We also noticed that the explainability needs and user needs in\n",
      "general may differ for users with varied levels of programming skills.\n",
      "Less experienced participants generally asked fewer questions in the\n",
      "workshop. It is possible that they face more challenges articulating\n",
      "or realizing they had certain explainability needs, and may benefit\n",
      "from more proactive explanations or an entire interaction session\n",
      "focusing on training and explaining rather than in-situ explanations.\n",
      "Future research should further explore user needs in GenAI for code\n",
      "use cases that target reducing programming barriers and enabling\n",
      "novices to code.\n",
      "Lastly, the bulk of explainability needs around GenAI for code\n",
      "leads to the question of utility of GenAI for code itself. One concern\n",
      "stems from the assumption that users are burdened with under-\n",
      "standing how the AI works and adjusting their inputs for optimal\n",
      "outcomes, as expressed by P7 in W8-NL2Code: ‚Äúas you were talking\n",
      "about this in this session, I keep thinking, if you are learning about\n",
      "all the use cases and how you‚Äôre going to structure the natural lan-\n",
      "guage and everything, how is it kind of different than just learning an\n",
      "actual language?‚Äù . We also note that at the current time, generative\n",
      "code models are prone to errors and will require post-generation\n",
      "improvements from humans [69, 100]. There is a fundamental ques-\n",
      "tion on the readiness of the technology, which may not be able to\n",
      "addressed by providing explainability alone. We urge the AI and\n",
      "HCI community to evaluate GenAI for code technologies, define in-\n",
      "tended use, and refrain from use cases that have not been validated\n",
      "or have risks of harmful consequences for stakeholders.\n",
      "6.3 Human-centered, participatory, and\n",
      "question-driven approaches to XAI design\n",
      "Our study used a scenario-based design [ 87] combined with a\n",
      "question-driven method to elicit explaiinability needs, based on\n",
      "work by Liao et al. [ 48, 50]. We reflect on a few strategies that\n",
      "worked well for our study, and encourage researchers and practi-\n",
      "tioners to adopt similar approaches to understand users‚Äô explain-\n",
      "ability needs early-on to drive technical and design choices.\n",
      "First, we found the use of a scenario, persona and an illustrated\n",
      "prototype corresponding to the scenario to effectively aid the elici-\n",
      "tation of rich user questions, even for a technology that was com-\n",
      "pletely new for participants. As Liao et al. [50] suggested ‚Äúfor highly\n",
      "novel systems, scenarios or low-fi prototypes can be used to elicit\n",
      "questions‚Äù. Moreover, we incorporated real outputs from state-of-\n",
      "the-art generative code models in the low-fi prototype to illustrate\n",
      "the model capabilities, which was repeatedly inquired by partici-\n",
      "pants to confirm the scenario has a realistic reflection of the GenAI\n",
      "capabilities. This approach also echos a recent trend in AI design to\n",
      "utilize real data points as ‚Äúdata probes‚Äù to aid design ideation [91].\n",
      "The choice of data point can sway the discussions in the user study.\n",
      "We suggest to choose ones that reflect the AI‚Äôs true capabilities,\n",
      "including limitations and errors to explore the design space more\n",
      "thoroughly.\n",
      "Second, we found the procedure of ‚Äúpooling-clustering-voting‚Äù\n",
      "questions to be productive. While Liao et al. [50]‚Äôs original method\n",
      "only deals with question elicitation in one-on-one interviews, we\n",
      "adopted a participatory workshop format to balance between indi-\n",
      "vidual brainstorming and group discussions. Giving participants\n",
      "seven minutes to post their questions allowed a quantity of ques-\n",
      "tions to be added. Working collaboratively to cluster them and vote\n",
      "on the clusters encouraged discussions and building upon each\n",
      "other‚Äôs ideas. This procedure also allowed participants to naturally\n",
      "articulate reasons behind their questions for our data collection.\n",
      "Lastly, in addition to the question elicitation exercise, we ex-\n",
      "plored four types of XAI features with low-fi prototypes. We de-\n",
      "fined these features based on prior works in different technical and\n",
      "application domains. A critical study design decision we had to\n",
      "make is the open-endedness of these prototypes. For some features\n",
      "(uncertainty indicators and attention visualizers), there already\n",
      "exist relevant techniques, so we chose a more concrete design to\n",
      "elicit feedback and ideas for extension. For other features (docu-\n",
      "mentation and ST), the design space is less defined so we used\n",
      "more open-ended probes to allow participants to freely and criti-\n",
      "cally brainstorm. In short, the design of a ‚Äúprobe‚Äù for participatory\n",
      "ideation and feedback is a non-trivial design decision, requiring\n",
      "a balance between concreteness and openness, considerations of\n",
      "both technical feasibility and user value, and diligent pilot test-\n",
      "ing [12, 24].\n",
      "7 CONCLUSION\n",
      "Despite growing efforts to apply state-of-the-art GenAI models to\n",
      "support software engineering tasks, investigations on user needs for\n",
      "such technologies have been scarce. Our work is among the first to\n",
      "study users‚Äô explainability needs of GenAI for code. By combining\n",
      "scenario-based design [87] and a recently proposed question-driven\n",
      "design method for XAI [48, 50], we conducted 9 participatory work-\n",
      "shops with 43 software engineers to understand their explainability\n",
      "needs with three use cases of GenAI for code: natural language to\n",
      "code, code translation, and code auto-completion. As a result, we\n",
      "identified 11 categories of explainability needs in the context of\n",
      "GenAI for code. We provided detailed definitions and examples for\n",
      "these categories, and contrasted them with common explainability\n",
      "needs for discriminative ML discussed in prior work [ 48, 53]. In\n",
      "addition, we proposed four areas of XAI features for these use cases,\n",
      "collected feedback from participants and provided concrete design\n",
      "recommendations. We hope that our results can inspire future AI\n",
      "and HCI work that can enable better human-AI collaboration for\n",
      "software engineering, and encourage more human-centered ap-\n",
      "proaches to drive AI technical development.\n",
      "Investigating Explainability of Generative AI for Code through Scenario-based Design IUI ‚Äô22, March 22‚Äì25, 2022, Helsinki, Finland\n",
      "REFERENCES\n",
      "[1] Amina Adadi and Mohammed Berrada. 2018. Peeking inside the black-box:\n",
      "A survey on Explainable Artificial Intelligence (XAI). IEEE Access 6 (2018),\n",
      "52138‚Äì52160.\n",
      "[2] Mayank Agarwal, Kartik Talamadupula, Stephanie Houde, Fernando Martinez,\n",
      "Michael J. Muller, John T. Richards, Steven Ross, and Justin D. Weisz. 2020. Qual-\n",
      "ity Estimation & Interpretability for Code Translation. ArXiv abs/2012.07581\n",
      "(2020).\n",
      "[3] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Uni-\n",
      "fied Pre-training for Program Understanding and Generation. In Proceedings of\n",
      "the 2021 Conference of the North American Chapter of the Association for Computa-\n",
      "tional Linguistics: Human Language Technologies . Association for Computational\n",
      "Linguistics, Online, 2655‚Äì2668. https://doi.org/10.18653/v1/2021.naacl-main.211\n",
      "[4] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018.\n",
      "A survey of machine learning for big code and naturalness. ACM Computing\n",
      "Surveys (CSUR) 51, 4 (2018), 1‚Äì37.\n",
      "[5] Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza.\n",
      "2014. Power to the people: The role of humans in interactive machine learning.\n",
      "Ai Magazine 35, 4 (2014), 105‚Äì120.\n",
      "[6] Cecilia Aragon, Shion Guha, Marina Kogan, Michael Muller, and Gina Neff. 2022.\n",
      "Human-Centered Data Science: An Introduction . MIT Press, Cambridge, MA.\n",
      "[7] Cecilia Aragon, Clayton Hutto, Andy Echenique, Brittany Fiore-Gartland, Yun\n",
      "Huang, Jinyoung Kim, Gina Neff, Wanli Xing, and Joseph Bayer. 2016. Devel-\n",
      "oping a research agenda for human-centered data science. In Proceedings of\n",
      "the 19th ACM Conference on Computer Supported Cooperative Work and Social\n",
      "Computing Companion . 529‚Äì535.\n",
      "[8] Matthew Arnold, Rachel KE Bellamy, Michael Hind, Stephanie Houde, Sameep\n",
      "Mehta, Aleksandra Mojsiloviƒá, Ravi Nair, K Natesan Ramamurthy, Alexandra\n",
      "Olteanu, David Piorkowski, et al . 2019. FactSheets: Increasing trust in AI\n",
      "services through supplier‚Äôs declarations of conformity. IBM Journal of Research\n",
      "and Development 63, 4/5 (2019), 6‚Äì1.\n",
      "[9] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk\n",
      "Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le,\n",
      "et al. 2021. Program Synthesis with Large Language Models. arXiv preprint\n",
      "arXiv:2108.07732 (2021).\n",
      "[10] Umang Bhatt, Javier Antor√°n, Yunfeng Zhang, Q Vera Liao, Prasanna Sattigeri,\n",
      "Riccardo Fogliato, Gabrielle Melan√ßon, Ranganath Krishnan, Jason Stanley,\n",
      "Omesh Tickoo, et al. 2021. Uncertainty as a form of transparency: Measuring,\n",
      "communicating, and using uncertainty. In Proceedings of the 2021 AAAI/ACM\n",
      "Conference on AI, Ethics, and Society . 401‚Äì413.\n",
      "[11] Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yun-\n",
      "han Jia, Joydeep Ghosh, Ruchir Puri, Jos√© MF Moura, and Peter Eckersley. 2020.\n",
      "Explainable machine learning in deployment. In Proceedings of the 2020 Confer-\n",
      "ence on Fairness, Accountability, and Transparency . 648‚Äì657.\n",
      "[12] Kirsten Boehner, Janet Vertesi, Phoebe Sengers, and Paul Dourish. 2007. How\n",
      "HCI interprets the probes. In Proceedings of the SIGCHI conference on Human\n",
      "factors in computing systems . 1077‚Äì1086.\n",
      "[13] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\n",
      "Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
      "Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan,\n",
      "Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter,\n",
      "Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\n",
      "Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\n",
      "Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.\n",
      "ArXiv abs/2005.14165 (2020).\n",
      "[14] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie\n",
      "Elhadad. 2015. Intelligible models for healthcare: Predicting pneumonia risk\n",
      "and hospital 30-day readmission. In Proceedings of KDD .\n",
      "[15] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de\n",
      "Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg\n",
      "Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,\n",
      "Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail\n",
      "Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter,\n",
      "Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\n",
      "Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex\n",
      "Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,\n",
      "Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan\n",
      "Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew\n",
      "Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew,\n",
      "Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.\n",
      "Evaluating Large Language Models Trained on Code. arXiv:2107.03374 [cs.LG]\n",
      "[16] Tian Qi Chen, Xuechen Li, Roger B. Grosse, and David Kristjanson Duvenaud.\n",
      "2018. Isolating Sources of Disentanglement in Variational Autoencoders. In\n",
      "NeurIPS.\n",
      "[17] Premkumar Devanbu. 2015. New initiative: The naturalness of software. In2015\n",
      "IEEE/ACM 37th IEEE International Conference on Software Engineering , Vol. 2.\n",
      "IEEE, 543‚Äì546.\n",
      "[18] Shipi Dhanorkar, Christine T Wolf, Kun Qian, Anbang Xu, Lucian Popa, and\n",
      "Yunyao Li. 2021. Who needs to know what, when?: Broadening the Explainable\n",
      "AI (XAI) Design Space by Looking at Explanations Across the AI Lifecycle. In\n",
      "Designing Interactive Systems Conference 2021 . 1591‚Äì1602.\n",
      "[19] Upol Ehsan, Q Vera Liao, Michael Muller, Mark O Riedl, and Justin D Weisz.\n",
      "2021. Expanding explainability: Towards social transparency in ai systems. In\n",
      "Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems .\n",
      "1‚Äì19.\n",
      "[20] Upol Ehsan and Mark Riedl. 2021. Explainability Pitfalls: Beyond Dark Patterns\n",
      "in Explainable AI - paper at HCAI@NeurIPS2021 workshop on human centered AI .\n",
      "https://sites.google.com/view/hcai-human-centered-ai-neurips/home Accessed\n",
      "January 19, 2022.\n",
      "[21] Upol Ehsan and Mark O Riedl. 2020. Human-centered explainable ai: Towards\n",
      "a reflective sociotechnical approach. In International Conference on Human-\n",
      "Computer Interaction . Springer, 449‚Äì466.\n",
      "[22] Upol Ehsan, Philipp Wintersberger, Q Vera Liao, Martina Mara, Marc Streit,\n",
      "Sandra Wachter, Andreas Riener, and Mark O Riedl. 2021. Operationalizing\n",
      "Human-Centered Perspectives in Explainable AI. In Extended Abstracts of the\n",
      "2021 CHI Conference on Human Factors in Computing Systems . 1‚Äì6.\n",
      "[23] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,\n",
      "Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. Codebert: A pre-trained\n",
      "model for programming and natural languages. arXiv preprint arXiv:2002.08155\n",
      "(2020).\n",
      "[24] Bill Gaver, Tony Dunne, and Elena Pacenti. 1999. Design: Cultural Probes.\n",
      "Interactions 6, 1 (jan 1999), 21‚Äì29. https://doi.org/10.1145/291224.291235\n",
      "[25] Werner Geyer, Lydia B Chilton, Justin D Weisz, and Mary Lou Maher. 2021.\n",
      "HAI-GEN 2021: 2nd Workshop on Human-AI Co-Creation with Generative\n",
      "Models. In 26th International Conference on Intelligent User Interfaces . 15‚Äì17.\n",
      "[26] Soumya Ghosh, Q Vera Liao, Karthikeyan Natesan Ramamurthy, Jiri Navratil,\n",
      "Prasanna Sattigeri, Kush R Varshney, and Yunfeng Zhang. 2021. Uncertainty\n",
      "Quantification 360: A Holistic Toolkit for Quantifying and Communicating the\n",
      "Uncertainty of AI. arXiv preprint arXiv:2106.01410 (2021).\n",
      "[27] Github. 2021. Copilot. Retrieved 03-August-2021 from https://copilot.github.\n",
      "com\n",
      "[28] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\n",
      "Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial\n",
      "networks. Commun. ACM 63, 11 (2020), 139‚Äì144.\n",
      "[29] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca\n",
      "Giannotti, and Dino Pedreschi. 2018. A survey of methods for explaining black\n",
      "box models. ACM computing surveys (CSUR) 51, 5 (2018), 1‚Äì42.\n",
      "[30] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long\n",
      "Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. 2020. Graphcodebert:\n",
      "Pre-training code representations with data flow.arXiv preprint arXiv:2009.08366\n",
      "(2020).\n",
      "[31] Shunan Guo, Fan Du, Sana Malik, Eunyee Koh, Sungchul Kim, Zhicheng Liu,\n",
      "Donghyun Kim, Hongyuan Zha, and Nan Cao. 2019. Visualizing uncertainty\n",
      "and alternatives in event sequence predictions. In Proceedings of the 2019 CHI\n",
      "Conference on Human Factors in Computing Systems . 1‚Äì12.\n",
      "[32] Aaron Halfaker and R Stuart Geiger. 2020. Ores: Lowering barriers with participa-\n",
      "tory machine learning in wikipedia.Proceedings of the ACM on Human-Computer\n",
      "Interaction 4, CSCW2 (2020), 1‚Äì37.\n",
      "[33] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul\n",
      "Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al.\n",
      "2021. Measuring Coding Challenge Competence With APPS. arXiv preprint\n",
      "arXiv:2105.09938 (2021).\n",
      "[34] Denis J Hilton. 1990. Conversational processes and causal explanation. Psycho-\n",
      "logical Bulletin 107, 1 (1990), 65.\n",
      "[35] M. Hind, Stephanie Houde, Jacquelyn Martino, A. Mojsilovic, David Piorkowski,\n",
      "John T. Richards, and K. Varshney. 2020. Experiences with Improving the\n",
      "Transparency of AI Models and Services. Extended Abstracts of the 2020 CHI\n",
      "Conference on Human Factors in Computing Systems (2020).\n",
      "[36] M. Hind, S. Mehta, A. Mojsilovic, R. Nair, K. Ramamurthy, Alexandra Olteanu,\n",
      "and K. Varshney. 2019. Increasing Trust in AI Services through Supplier‚Äôs\n",
      "Declarations of Conformity. IBM J. Res. Dev. 63 (2019), 6:1‚Äì6:13.\n",
      "[37] Abram Hindle, Earl T Barr, Mark Gabel, Zhendong Su, and Premkumar Devanbu.\n",
      "2016. On the naturalness of software. Commun. ACM 59, 5 (2016), 122‚Äì131.\n",
      "[38] Eric Horvitz. 1999. Principles of Mixed-Initiative User Interfaces. In Proceedings\n",
      "of the SIGCHI Conference on Human Factors in Computing Systems (Pittsburgh,\n",
      "Pennsylvania, USA) (CHI ‚Äô99). Association for Computing Machinery, New York,\n",
      "NY, USA, 159‚Äì166. https://doi.org/10.1145/302979.303030\n",
      "[39] Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. 2021. Code pre-\n",
      "diction by feeding trees to transformers. In 2021 IEEE/ACM 43rd International\n",
      "Conference on Software Engineering (ICSE) . IEEE, 150‚Äì162.\n",
      "[40] Bran Knowles and John T. Richards. 2021. The Sanction of Authority: Promot-\n",
      "ing Public Trust in AI. Proceedings of the 2021 ACM Conference on Fairness,\n",
      "Accountability, and Transparency (2021).\n",
      "[41] Marina Kogan, Aaron Halfaker, Shion Guha, Cecilia Aragon, Michael Muller,\n",
      "and Stuart Geiger. 2020. Mapping Out Human-Centered Data Science: Methods,\n",
      "IUI ‚Äô22, March 22‚Äì25, 2022, Helsinki, Finland Jiao Sun et al.\n",
      "Approaches, and Best Practices. In Companion of the 2020 ACM International\n",
      "Conference on Supporting Group Work . 151‚Äì156.\n",
      "[42] Sandeep Kaur Kuttal, Jarow Myers, Sam Gurka, David Magar, David Piorkowski,\n",
      "and Rachel Bellamy. 2020. Towards designing conversational agents for pair\n",
      "programming: Accounting for creativity strategies and conversational styles.\n",
      "In 2020 IEEE Symposium on Visual Languages and Human-Centric Computing\n",
      "(VL/HCC). IEEE, 1‚Äì11.\n",
      "[43] Sandeep Kaur Kuttal, Bali Ong, Kate Kwasny, and Peter Robe. 2021. Trade-offs\n",
      "for Substituting a Human with an Agent in a Pair Programming Context: The\n",
      "Good, the Bad, and the Ugly. InProceedings of the 2021 CHI Conference on Human\n",
      "Factors in Computing Systems . 1‚Äì20.\n",
      "[44] Himabindu Lakkaraju, Stephen H. Bach, and Jure Leskovec. 2016. Interpretable\n",
      "Decision Sets: A Joint Framework for Description and Prediction. InProceedings\n",
      "of the 22nd ACM SIGKDD International Conference on Knowledge Discovery\n",
      "and Data Mining (San Francisco, California, USA) (KDD ‚Äô16) . Association for\n",
      "Computing Machinery, New York, NY, USA, 1675‚Äì1684. https://doi.org/10.\n",
      "1145/2939672.2939874\n",
      "[45] Min Kyung Lee, Nina Grgiƒá-Hlaƒça, Michael Carl Tschantz, Reuben Binns, Adrian\n",
      "Weller, Michelle Carney, and Kori Inkpen. 2020. Human-centered approaches\n",
      "to fair and responsible AI. In Extended Abstracts of the 2020 CHI Conference on\n",
      "Human Factors in Computing Systems . 1‚Äì8.\n",
      "[46] Min Kyung Lee, Daniel Kusbit, Anson Kahng, Ji Tae Kim, Xinran Yuan, Allissa\n",
      "Chan, Daniel See, Ritesh Noothigattu, Siheon Lee, Alexandros Psomas, et al. 2019.\n",
      "WeBuildAI: Participatory framework for algorithmic governance. Proceedings\n",
      "of the ACM on Human-Computer Interaction 3, CSCW (2019), 1‚Äì35.\n",
      "[47] Tao Lei, Regina Barzilay, and T. Jaakkola. 2016. Rationalizing Neural Predictions.\n",
      "In EMNLP.\n",
      "[48] Q Vera Liao, Daniel Gruen, and Sarah Miller. 2020. Questioning the AI: informing\n",
      "design practices for explainable AI user experiences. In Proceedings of the 2020\n",
      "CHI Conference on Human Factors in Computing Systems . 1‚Äì15.\n",
      "[49] Q Vera Liao and Michael Muller. 2019. Enabling Value Sensitive AI Systems\n",
      "through Participatory Design Fictions. arXiv preprint arXiv:1912.07381 (2019).\n",
      "[50] Q Vera Liao, Milena Pribiƒá, Jaesik Han, Sarah Miller, and Daby Sow. 2021.\n",
      "Question-Driven Design Process for Explainable AI User Experiences. arXiv\n",
      "preprint arXiv:2104.03483 (2021).\n",
      "[51] Q Vera Liao, Moninder Singh, Yunfeng Zhang, and Rachel Bellamy. 2021. Intro-\n",
      "duction to explainable ai. In Extended Abstracts of the 2021 CHI Conference on\n",
      "Human Factors in Computing Systems . 1‚Äì3.\n",
      "[52] Q Vera Liao and Kush R Varshney. 2021. Human-Centered Explainable AI (XAI):\n",
      "From Algorithms to User Experiences. arXiv preprint arXiv:2110.10790 (2021).\n",
      "[53] Brian Y Lim and Anind K Dey. 2010. Toolkit to support intelligibility in context-\n",
      "aware applications. In Proceedings of the 12th ACM international conference on\n",
      "Ubiquitous computing . 13‚Äì22.\n",
      "[54] Brian Y Lim, Anind K Dey, and Daniel Avrahami. 2009. Why and why not\n",
      "explanations improve the intelligibility of context-aware intelligent systems.\n",
      "In Proceedings of the SIGCHI conference on human factors in computing systems .\n",
      "2119‚Äì2128.\n",
      "[55] Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris B. Kotsiantis. 2021.\n",
      "Explainable AI: A Review of Machine Learning Interpretability Methods.Entropy\n",
      "23 (2021).\n",
      "[56] Zachary C Lipton. 2018. The mythos of model interpretability. Queue 16, 3\n",
      "(2018), 31‚Äì57.\n",
      "[57] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and\n",
      "Weizhu Chen. 2021. What Makes Good In-Context Examples for GPT-3? arXiv\n",
      "preprint arXiv:2101.06804 (2021).\n",
      "[58] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Gra-\n",
      "ham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of prompt-\n",
      "ing methods in natural language processing. arXiv preprint arXiv:2107.13586\n",
      "(2021).\n",
      "[59] Ryan Louie, Andy Coenen, Cheng Zhi Huang, Michael Terry, and Carrie J Cai.\n",
      "2020. Novice-AI music co-creation via AI-steering tools for deep generative\n",
      "models. InProceedings of the 2020 CHI Conference on Human Factors in Computing\n",
      "Systems. 1‚Äì13.\n",
      "[60] Ryan Louie, Any Cohen, Cheng-Zhi Anna Huang, Michael Terry, and Carrie J\n",
      "Cai. 2020. Cococo: AI-Steering Tools for Music Novices Co-Creating with\n",
      "Generative Models.. In HAI-GEN+ user2agent@ IUI .\n",
      "[61] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio\n",
      "Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong\n",
      "Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan\n",
      "Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. 2021.\n",
      "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding\n",
      "and Generation. ArXiv abs/2102.04664 (2021).\n",
      "[62] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting\n",
      "model predictions. In Proceedings of the 31st international conference on neural\n",
      "information processing systems . 4768‚Äì4777.\n",
      "[63] Cade Metz. 2021. A.I. Can Now Write Its Own Computer Code. That‚Äôs Good\n",
      "News for Humans. The New York Times (9 September 2021). https://www.\n",
      "nytimes.com/2021/09/09/technology/codex-artificial-intelligence-coding.html\n",
      "[64] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasser-\n",
      "man, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru.\n",
      "2019. Model Cards for Model Reporting. Proceedings of the Conference on\n",
      "Fairness, Accountability, and Transparency (2019).\n",
      "[65] Michael Muller, Plamen Angelov, Shion Guha, Marina Kogan, Gina Neff, Nuria\n",
      "Oliver, Manuel Gomez Rodriquez, and Adrian Weller. 2021.HCAI@NeurIPS2021:\n",
      "Human Centered AI workshop at NeurIPS 2021 . https://sites.google.com/view/\n",
      "hcai-human-centered-ai-neurips/home Accessed January 17, 2022.\n",
      "[66] Michael Muller, Cecilia Aragon, Shion Guha, Marina Kogan, Gina Neff, Cathrine\n",
      "Seidelin, Katie Shilton, and Anissa Tanweer. 2020. Interrogating Data Science. In\n",
      "Conference Companion Publication of the 2020 on Computer Supported Cooperative\n",
      "Work and Social Computing . 467‚Äì473.\n",
      "[67] Michael Muller, Melanie Feinberg, Timothy George, Steven J Jackson, Bonnie E\n",
      "John, Mary Beth Kery, and Samir Passi. 2019. Human-centered study of data\n",
      "science work practices. In Extended Abstracts of the 2019 CHI Conference on\n",
      "Human Factors in Computing Systems . 1‚Äì8.\n",
      "[68] Michael Muller and Q Vera Liao. [n.d.]. Exploring AI Ethics and Values through\n",
      "Participatory Design Fictions. ([n. d.]).\n",
      "[69] Michael Muller, April Y. Wang, Steven I. Ross, Justin D. Weisz, Mayank Agarwal,\n",
      "Kartik Talamadupula, Stephanie Houde, Fernando Martinez, John Richards,\n",
      "Jaimie Drozdal, Xie Lui, David Piorkowski, and Dakuo Wang. 2021. How data\n",
      "scientists improve generated code documentation in Jupyter notebooks . Retrieved\n",
      "October 5, 2021 from https://hai-gen2021.github.io/program/\n",
      "[70] Michael Muller, Christine T Wolf, Josh Andres, Michael Desmond, Naren-\n",
      "dra Nath Joshi, Zahra Ashktorab, Aabhas Sharma, Kristina Brimijoin, Qian\n",
      "Pan, Evelyn Duesterwald, et al. 2021. Designing Ground Truth and the Social\n",
      "Life of Labels. In Proceedings of the 2021 CHI Conference on Human Factors in\n",
      "Computing Systems . 1‚Äì16.\n",
      "[71] Anh Tuan Nguyen, Tung Thanh Nguyen, and Tien N Nguyen. 2014. Migrating\n",
      "code with statistical machine translation. In Companion Proceedings of the 36th\n",
      "International Conference on Software Engineering . 544‚Äì547.\n",
      "[72] Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata, Sakriani Sakti,\n",
      "Tomoki Toda, and Satoshi Nakamura. 2015. Learning to generate pseudo-code\n",
      "from source code using statistical machine translation (t). In2015 30th IEEE/ACM\n",
      "International Conference on Automated Software Engineering (ASE) . IEEE, 574‚Äì\n",
      "584.\n",
      "[73] Andr√©s P√°ez. 2019. The pragmatic turn in explainable artificial intelligence\n",
      "(XAI). Minds and Machines 29, 3 (2019), 441‚Äì459.\n",
      "[74] Raja Parasuraman, Thomas B Sheridan, and Christopher D Wickens. 2000.\n",
      "A model for types and levels of human interaction with automation. IEEE\n",
      "Transactions on systems, man, and cybernetics-Part A: Systems and Humans 30, 3\n",
      "(2000), 286‚Äì297.\n",
      "[75] David Piorkowski, D. Gonz‚Äôalez, John T. Richards, and Stephanie Houde. 2020.\n",
      "Towards evaluating and eliciting high-quality documentation for intelligent\n",
      "systems. ArXiv abs/2011.08774 (2020).\n",
      "[76] David Piorkowski, Soya Park, A. Wang, Dakuo Wang, Michael J. Muller, and\n",
      "Felix Portnoy. 2021. How AI Developers Overcome Communication Challenges\n",
      "in a Multidisciplinary Team. Proceedings of the ACM on Human-Computer\n",
      "Interaction 5 (2021), 1 ‚Äì 25.\n",
      "[77] Ruchi Puri, D. Kung, G. Janssen, Wei Zhang, Giacomo Domeniconi, Vladmir\n",
      "Zolotov, Julian Dolby, Jie Chen, M. Choudhury, Lindsey Decker, Veronika Thost,\n",
      "Luca Buratti, Saurabh Pujar, and Ulrich Finkler. 2021. Project CodeNet: A Large-\n",
      "Scale AI for Code Dataset for Learning a Diversity of Coding Tasks. ArXiv\n",
      "abs/2105.12655 (2021).\n",
      "[78] Inioluwa Deborah Raji and Jingying Yang. 2019. About ml: Annotation and\n",
      "benchmarking on understanding and transparency of machine learning lifecy-\n",
      "cles. arXiv preprint arXiv:1912.06166 (2019).\n",
      "[79] Sahil Barjtya Ankur Sharma Usha Rani. 2017. A detailed study of Software\n",
      "Development Life Cycle (SDLC) Models. International Journal of Engineering\n",
      "and Computer Science 6 (2017).\n",
      "[80] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Why should i\n",
      "trust you?: Explaining the predictions of any classifier. In Proceedings of KDD .\n",
      "[81] John Richards, David Piorkowski, Michael Hind, Stephanie Houde, and Aleksan-\n",
      "dra Mojsiloviƒá. 2020. A Methodology for Creating AI FactSheets. arXiv preprint\n",
      "arXiv:2006.13796 (2020).\n",
      "[82] John T. Richards, David Piorkowski, M. Hind, Stephanie Houde, and Alek-\n",
      "sandra Mojsilovi‚Äôc. 2020. A Methodology for Creating AI FactSheets. ArXiv\n",
      "abs/2006.13796 (2020).\n",
      "[83] Karl Ridgeway. 2016. A Survey of Inductive Biases for Factorial Representation-\n",
      "Learning. ArXiv abs/1612.05299 (2016).\n",
      "[84] Karl Ridgeway and Michael C. Mozer. 2018. Learning Deep Disentangled Em-\n",
      "beddings with the F-Statistic Loss. In NeurIPS.\n",
      "[85] Mark O Riedl. 2019. Human-centered artificial intelligence and machine learning.\n",
      "Human Behavior and Emerging Technologies 1, 1 (2019), 33‚Äì36.\n",
      "[86] Andrew Ross, Nina Chen, Elisa Zhao Hang, Elena L Glassman, and Finale Doshi-\n",
      "Velez. 2021. Evaluating the Interpretability of Generative Models by Interactive\n",
      "Reconstruction. In Proceedings of the 2021 CHI Conference on Human Factors in\n",
      "Computing Systems . 1‚Äì15.\n",
      "Investigating Explainability of Generative AI for Code through Scenario-based Design IUI ‚Äô22, March 22‚Äì25, 2022, Helsinki, Finland\n",
      "[87] Mary Beth Rosson and John M Carroll. 2009. Scenario-based design. In Human-\n",
      "computer interaction . CRC Press, 161‚Äì180.\n",
      "[88] Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample.\n",
      "2020. Unsupervised Translation of Programming Languages.. In NeurIPS.\n",
      "[89] Ben Shneiderman. 2020. Bridging the gap between ethics and practice: Guide-\n",
      "lines for reliable, safe, and trustworthy Human-Centered AI systems. ACM\n",
      "Transactions on Interactive Intelligent Systems (TiiS) 10, 4 (2020), 1‚Äì31.\n",
      "[90] H Colleen Stuart, Laura Dabbish, Sara Kiesler, Peter Kinnaird, and Ruogu Kang.\n",
      "2012. Social transparency in networked information exchange: a theoretical\n",
      "framework. In Proceedings of the ACM 2012 conference on Computer Supported\n",
      "Cooperative Work. 451‚Äì460.\n",
      "[91] Hariharan Subramonyam, Colleen Seifert, and Eytan Adar. 2021. Towards A\n",
      "Process Model for Co-Creating AI Experiences. arXiv preprint arXiv:2104.07595\n",
      "(2021).\n",
      "[92] Kartik Talamadupula. 2021. Applied AI Matters - AI4Code: Applying Artificial\n",
      "Intelligence to Source Code. Association for Computing Machinery (ACM) Special\n",
      "Interest Group on AI (SIGAI) AI Matters 7 (2021). Issue 1.\n",
      "[93] Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and Neel\n",
      "Sundaresan. 2020. Unit Test Case Generation with Transformers. arXiv preprint\n",
      "arXiv:2009.05617 (2020).\n",
      "[94] Jennifer Wortman Vaughan and Hanna Wallach. 2020. A human-centered\n",
      "agenda for intelligible machine learning. Machines We Trust: Getting Along with\n",
      "Artificial Intelligence (2020).\n",
      "[95] Jesse Vig. 2019. A Multiscale Visualization of Attention in the Transformer\n",
      "Model. In ACL.\n",
      "[96] Jesse Vig and Yonatan Belinkov. 2019. Analyzing the Structure of Attention in\n",
      "a Transformer Language Model. In BlackboxNLP@ACL.\n",
      "[97] Donald Martin Vinodkumar Prabhakaran Jr. 2020. Participatory Machine Learn-\n",
      "ing Using Community-Based System Dynamics. Health and Human Rights 22, 2\n",
      "(2020), 71.\n",
      "[98] Abhishek Wadhwani and Priyank Jain. 2020. Machine Learning Model Cards\n",
      "Transparency Review: Using model card toolkit. In 2020 IEEE Pune Section\n",
      "International Conference (PuneCon) . IEEE, 133‚Äì137.\n",
      "[99] Yue Wang, Weishi Wang, Shafiq R. Joty, and S. Hoi. 2021. CodeT5: Identifier-\n",
      "aware Unified Pre-trained Encoder-Decoder Models for Code Understanding\n",
      "and Generation.\n",
      "[100] Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross,\n",
      "Fernando Martinez, Mayank Agarwal, and Kartik Talamadupula. 2021. Per-\n",
      "fection Not Required? Human-AI Partnerships in Code Translation. In 26th\n",
      "International Conference on Intelligent User Interfaces . 402‚Äì412.\n",
      "[101] Sarah Wiegreffe and Yuval Pinter. 2019. Attention is not not explanation. arXiv\n",
      "preprint arXiv:1908.04626 (2019).\n",
      "[102] Christine T Wolf. 2019. Explainability scenarios: towards scenario-based XAI\n",
      "design. In Proceedings of the 24th International Conference on Intelligent User\n",
      "Interfaces. 252‚Äì257.\n",
      "[103] Enhao Zhang and Nikola Banovic. 2021. Method for Exploring Generative\n",
      "Adversarial Networks (GANs) via Automatically Generated Image Galleries. In\n",
      "Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems .\n",
      "1‚Äì15.\n",
      "[104] Bolei Zhou, Aditya Khosla, √Ägata Lapedriza, Aude Oliva, and Antonio Torralba.\n",
      "2016. Learning Deep Features for Discriminative Localization. 2016 IEEE Con-\n",
      "ference on Computer Vision and Pattern Recognition (CVPR) (2016), 2921‚Äì2929.\n",
      "[105] Haiyi Zhu, Bowen Yu, Aaron Halfaker, and Loren Terveen. 2018. Value-sensitive\n",
      "algorithm design: Method, case study, and lessons. Proceedings of the ACM on\n",
      "Human-Computer Interaction 2, CSCW (2018), 1‚Äì23.\n",
      "Code Translation ‚ÄúBase Case‚Äù\n",
      "‚Ä¶\n",
      "b\n",
      "C1\n",
      "C2\n",
      "- Alex tests the code translation model with a function \n",
      "that converts integers to Roman Numerals \n",
      "- The parameter for the function is the integer to be \n",
      "converted (e.g. 22) \n",
      "- The function returns a string representing the Roman \n",
      "numeral (e.g. \"XXII\") \n",
      "First attempt: Alex wants AI to complete the code from \n",
      "function name, so he tried to come up with a function name \n",
      "that is almost describing everything \n",
      "- He realized that he should not have put arguments \n",
      "- Other than that, the program seems to be doing its job\n",
      "Code Autocomplete ‚ÄúBase Case‚Äù - Alex finds a problem in an online coding platform  and wants to test if AI can do it! \n",
      "- The problem description is: write a program which reads a list of N integers and \n",
      "prints the number of prime numbers in the list \n",
      "- Input: the number of elements in the list, and N numbers are given in the same line \n",
      "- Output: the number of prime numbers in the input \n",
      "‚Ä¶\n",
      "Fourth attempt: Alex wonder what if he does not specify the \n",
      "requirement in the function name, instead, he wrote it down \n",
      "as comments \n",
      "- AI seems to be working well\n",
      "‚Ä¶ ‚Ä¶\n",
      "Figure 3: The two other code \"base cases\" we used in the\n",
      "workshop on Mural. C1 is the example we used as the base\n",
      "case for the workshops about code translation (W1-CT, W2-\n",
      "CT and W5-CT), and C2 is for the code autocompletion use\n",
      "case (W3-CA, W4-CA and W9-CA). The C in Figure 1 is the\n",
      "base code base for the natural language to code use case (W6-\n",
      "NL2Code, W7-NL2Code, W8-NL2Code).\n",
      "A THE OTHER TWO USE CASES\n",
      "We show the example code as the base case for workshops about nat-\n",
      "ural language to code (W6-NL2Code, W7-NL2Code, W8-NL2Code)\n",
      "in Figure 1. As a supplement, we show how we replace Figure 1 (c)\n",
      "with other code examples in Figure 3 for the code translation and\n",
      "code autocompletion use cases. The other sub-figures in Figure 1\n",
      "stay almost the same for all workshops across all use cases with\n",
      "minimum edits.\n"
     ]
    }
   ],
   "source": [
    "extracted_text = extract_text_from_file(\"2202.04903v1.pdf\")\n",
    "\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Person\": [\n",
      "    \"name\",\n",
      "    \"affiliation\",\n",
      "    \"email\",\n",
      "    \"location\"\n",
      "  ],\n",
      "  \"Organization\": [\n",
      "    \"name\",\n",
      "    \"location\"\n",
      "  ],\n",
      "  \"Publication\": [\n",
      "    \"title\",\n",
      "    \"authors\",\n",
      "    \"date\",\n",
      "    \"venue\",\n",
      "    \"keywords\",\n",
      "    \"abstract\",\n",
      "    \"doi\",\n",
      "    \"isbn\"\n",
      "  ],\n",
      "  \"AI_Model\": [\n",
      "    \"name\",\n",
      "    \"type\",\n",
      "    \"application_domain\",\n",
      "    \"training_data\",\n",
      "    \"capabilities\",\n",
      "    \"limitations\",\n",
      "    \"performance_metrics\"\n",
      "  ],\n",
      "  \"Software_Engineering_Task\": [\n",
      "    \"name\",\n",
      "    \"description\",\n",
      "    \"programming_languages\",\n",
      "    \"input\",\n",
      "    \"output\"\n",
      "  ],\n",
      "  \"Conference\": [\n",
      "    \"name\",\n",
      "    \"date\",\n",
      "    \"location\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now pass the extracted text to the existing workflow\n",
    "result = workflow_step_1(extracted_text)\n",
    "\n",
    "# Display the result\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Person\": [\n",
      "    \"name\",\n",
      "    \"occupation\",\n",
      "    \"experience\",\n",
      "    \"location\"\n",
      "  ],\n",
      "  \"Organization\": [\n",
      "    \"name\",\n",
      "    \"industry\",\n",
      "    \"founding_year\",\n",
      "    \"location\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    " # Example text\n",
    "sample_text = \"\"\"\n",
    "John Smith works at Acme Corporation in New York. \n",
    "He is a senior software engineer with 10 years of experience.\n",
    "Acme Corporation is a technology company founded in 2010.\n",
    "\"\"\"\n",
    "\n",
    "# Process the text\n",
    "result = workflow_step_1(sample_text)\n",
    "\n",
    "# Display the result\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Second example:\n",
      "{\n",
      "  \"ArchitecturalStructure\": [\n",
      "    \"name\",\n",
      "    \"location\",\n",
      "    \"construction_materials\",\n",
      "    \"length\",\n",
      "    \"construction_period\",\n",
      "    \"historical_significance\",\n",
      "    \"status\"\n",
      "  ],\n",
      "  \"Dynasty\": [\n",
      "    \"name\",\n",
      "    \"start_year\",\n",
      "    \"end_year\",\n",
      "    \"rulers\",\n",
      "    \"significant_events\"\n",
      "  ],\n",
      "  \"GeographicLocation\": [\n",
      "    \"name\",\n",
      "    \"type\"\n",
      "  ],\n",
      "  \"Organization\": [\n",
      "    \"name\",\n",
      "    \"type\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Another example\n",
    "another_text = \"\"\"\n",
    "The Great Wall of China is one of the most impressive architectural achievements in history.\n",
    "It was built during the Ming Dynasty (1368-1644) to protect China from invasions.\n",
    "The wall stretches over 13,000 miles and was constructed using various materials including stone, brick, and wood.\n",
    "Today, it is a UNESCO World Heritage site and attracts millions of tourists annually.\n",
    "\"\"\"\n",
    "\n",
    "# Process the text\n",
    "result = workflow_step_1(another_text)\n",
    "\n",
    "# Display the result\n",
    "print(\"\\nSecond example:\")\n",
    "print(json.dumps(result, indent=2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text: str, chunk_size: int = 1000) -> List[str]:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    return text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityInstance(BaseModel):\n",
    "    entity: str = Field(description=\"The entity type\")\n",
    "    instances: List[Dict[str, str]] = Field(description=\"List of instances with available properties\")\n",
    "\n",
    "class EntityInstancesExtractionOutput(BaseModel):\n",
    "    entity_instances: List[EntityInstance] = Field(description=\"List of entity instances with properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_2_template = \"\"\"You are part of an agentic workflow that processes data input by a user step by step. \n",
    "The end result of the workflow is a detailed knowledge graph generated from the input data.\n",
    "\n",
    "You are the Entity Instances Extraction Agent, which is the SECOND agent in this workflow. Your responsibilities are:\n",
    "- Extract instances of entities and their available properties from the given text chunk\n",
    "- Use the global entity list from the previous step to identify relevant entities and properties\n",
    "- Only include instances of entities that are present in the text chunk, don't make up instances for entities that are not present in the text chunk\n",
    "- Only include properties of instances that are present in the text chunk, don't make up properties for instances that are not present in the text chunk\n",
    "- If any property of any instance is not present in the text chunk, don't include it in the response at all (even the key)\n",
    "\n",
    "Given the following text chunk and entity list, extract instances of entities and their available properties.\n",
    "\n",
    "Text chunk: {chunk}\n",
    "Entity list: {entity_list}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_2_parser = PydanticOutputParser(pydantic_object=EntityInstancesExtractionOutput)\n",
    "\n",
    "\n",
    "workflow_2_prompt = ChatPromptTemplate.from_template(template=workflow_2_template)\n",
    "\n",
    "# Create the chain for workflow 2\n",
    "workflow_2_chain = (\n",
    "    {\"chunk\": RunnablePassthrough(), \"entity_list\": lambda _: result, \"format_instructions\": lambda _: workflow_2_parser.get_format_instructions()}\n",
    "    | workflow_2_prompt\n",
    "    | llm\n",
    "    | workflow_2_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_workflow_2_in_parallel(text: str, workflow_1_result: Dict[str, List[str]], parallelism: int = 3) -> List[EntityInstancesExtractionOutput]:\n",
    "    \"\"\"Run workflow 2 with rate limiting to avoid API quota issues.\"\"\"\n",
    "    chunks = split_text_into_chunks(text)\n",
    "\n",
    "    print(len(chunks))\n",
    "    \n",
    "    # Create inputs for each chunk\n",
    "    inputs = [{\"chunk\": chunk, \"entity_list\": workflow_1_result} for chunk in chunks]\n",
    "    \n",
    "    # Process chunks in smaller batches with delays\n",
    "    results = []\n",
    "    batch_size = min(parallelism, 3)  # Limit batch size to 3 for free tier\n",
    "    \n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        batch = inputs[i:i + batch_size]\n",
    "        # Process current batch\n",
    "        batch_results = workflow_2_chain.batch(batch, max_concurrent=batch_size)\n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        # Add delay between batches if not the last batch\n",
    "        if i + batch_size < len(inputs):\n",
    "            time.sleep(2)  # 2 second delay between batches\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 33\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 33\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 31\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhausted\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result_2 = \u001b[43mrun_workflow_2_in_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextracted_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mrun_workflow_2_in_parallel\u001b[39m\u001b[34m(text, workflow_1_result, parallelism)\u001b[39m\n\u001b[32m     13\u001b[39m batch = inputs[i:i + batch_size]\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Process current batch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m batch_results = \u001b[43mworkflow_2_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_concurrent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m results.extend(batch_results)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Add delay between batches if not the last batch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3198\u001b[39m, in \u001b[36mRunnableSequence.batch\u001b[39m\u001b[34m(self, inputs, config, return_exceptions, **kwargs)\u001b[39m\n\u001b[32m   3196\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3197\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps):\n\u001b[32m-> \u001b[39m\u001b[32m3198\u001b[39m             inputs = \u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3199\u001b[39m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3200\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m   3201\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# each step a child run of the corresponding root run\u001b[39;49;00m\n\u001b[32m   3202\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3203\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43m+\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3204\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3205\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3206\u001b[39m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3207\u001b[39m \u001b[43m                \u001b[49m\u001b[43mreturn_exceptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3208\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3209\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3211\u001b[39m \u001b[38;5;66;03m# finish the root runs\u001b[39;00m\n\u001b[32m   3212\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:788\u001b[39m, in \u001b[36mRunnable.batch\u001b[39m\u001b[34m(self, inputs, config, return_exceptions, **kwargs)\u001b[39m\n\u001b[32m    785\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mlist[Output]\u001b[39m\u001b[33m\"\u001b[39m, [invoke(inputs[\u001b[32m0\u001b[39m], configs[\u001b[32m0\u001b[39m])])\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(configs[\u001b[32m0\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mlist[Output]\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:555\u001b[39m, in \u001b[36mContextThreadPoolExecutor.map.<locals>._wrapped_fn\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped_fn\u001b[39m(*args: Any) -> T:\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontexts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:781\u001b[39m, in \u001b[36mRunnable.batch.<locals>.invoke\u001b[39m\u001b[34m(input, config)\u001b[39m\n\u001b[32m    779\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m e\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:331\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    326\u001b[39m     **kwargs: Any,\n\u001b[32m    327\u001b[39m ) -> BaseMessage:\n\u001b[32m    328\u001b[39m     config = ensure_config(config)\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    330\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    341\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:894\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    885\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    886\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    887\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    891\u001b[39m     **kwargs: Any,\n\u001b[32m    892\u001b[39m ) -> LLMResult:\n\u001b[32m    893\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:719\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    718\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    725\u001b[39m         )\n\u001b[32m    726\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    727\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:960\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    959\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    963\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    964\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:961\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m    935\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m    936\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    937\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    948\u001b[39m     **kwargs: Any,\n\u001b[32m    949\u001b[39m ) -> ChatResult:\n\u001b[32m    950\u001b[39m     request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m    951\u001b[39m         messages,\n\u001b[32m    952\u001b[39m         stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m    959\u001b[39m         tool_choice=tool_choice,\n\u001b[32m    960\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m     response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:196\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    194\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:194\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    191\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    192\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:178\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    289\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    290\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    291\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    292\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    164\u001b[39m     time.sleep(sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:212\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    207\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    208\u001b[39m         error_list,\n\u001b[32m    209\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    210\u001b[39m         original_timeout,\n\u001b[32m    211\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    214\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    146\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(*args, **kwargs)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mResourceExhausted\u001b[39m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 31\n}\n]"
     ]
    }
   ],
   "source": [
    "result_2 = run_workflow_2_in_parallel(extracted_text, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
