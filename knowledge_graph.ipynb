{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4dad227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\aperturedb-project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Dict, List\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7aada5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4884710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-preview-04-17\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d43fffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_content(pdf_path, return_single_string=True, extract_metadata=False):\n",
    "    \"\"\"\n",
    "    Load and parse a PDF document, returning its text content.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        return_single_string (bool): If True, returns the entire PDF content as a single string.\n",
    "                                    If False, returns a list of strings (one per page).\n",
    "        extract_metadata (bool): If True, returns metadata along with content\n",
    "    \n",
    "    Returns:\n",
    "        If return_single_string is True and extract_metadata is False:\n",
    "            str: The entire text content of the PDF\n",
    "        If return_single_string is False and extract_metadata is False:\n",
    "            list: List of strings, one for each page\n",
    "        If extract_metadata is True:\n",
    "            tuple: (content, metadata) where content is either a string or list based on return_single_string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"PDF file not found at: {pdf_path}\")\n",
    "    \n",
    "    # Initialize the loader with the appropriate mode\n",
    "    mode = \"single\" if return_single_string else \"elements\"\n",
    "    loader = PyPDFLoader(pdf_path, mode=mode)\n",
    "    \n",
    "    # Load the documents\n",
    "    docs = loader.load()\n",
    "    \n",
    "    if return_single_string:\n",
    "        # With mode=\"single\", there should only be one document containing all pages\n",
    "        content = docs[0].page_content if docs else \"\"\n",
    "        metadata = docs[0].metadata if docs else {}\n",
    "    else:\n",
    "        # With default mode, each document is a page\n",
    "        content = [doc.page_content for doc in docs]\n",
    "        metadata = [doc.metadata for doc in docs]\n",
    "    \n",
    "    if extract_metadata:\n",
    "        return content, metadata\n",
    "    else:\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f714bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = load_pdf_content(\"Cloud Computing Copy Lecture Notes.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28f5c303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloud Computing Lecture Notes \n",
      "Distributed Computing/Systems \n",
      "Definition: \n",
      "Distributed computing refers to a system where computing resources are distributed \n",
      "across multiple locations rather than being centralized in a single system. This enables \n",
      "task distribution and efficient resource utilization. \n",
      "Why Use Distributed Systems? \n",
      "• Scalability Issues: Traditional computing faces bottlenecks due to hardware \n",
      "limitations, whereas distributed systems allow for hardware scaling. \n",
      "• Connected Devices: In a networked system, connected devices communicate, but \n",
      "this does not necessarily make them distributed. \n",
      "• IoT (Internet of Things): IoT is one of the largest examples of distributed computing. \n",
      "• Multi-layered System Design: Distributed computing enables systems to function \n",
      "in multiple layers, with each layer acting as a distributed entity. \n",
      "• User Perspective: Although the system consists of multiple machines, distributed \n",
      "computing presents a unified system to users. \n",
      " \n",
      "Parallel Comp\n"
     ]
    }
   ],
   "source": [
    "print(doc[:1000])  # Print the first 1000 characters of the loaded document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37f14e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic model for entity schema parser\n",
    "class EntitySchema(BaseModel):\n",
    "    \"\"\"Entity types and their properties.\"\"\"\n",
    "    entities: Dict[str, List[str]] = Field(\n",
    "        description=\"Dictionary mapping entity types to their possible properties\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8025570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entity extraction chain\n",
    "def create_entity_extraction_chain():\n",
    "    parser = JsonOutputParser(pydantic_object=EntitySchema)\n",
    "    \n",
    "    # Prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "    You are the first agent in a multi-step workflow to build a Knowledge Graph from raw text.\n",
    "\n",
    "    Workflow Steps Overview:\n",
    "    1. Extract high-level entity types and their properties from the text. [CURRENT STEP]\n",
    "    2. Extract specific instances of entities and their properties based on the identified types.\n",
    "    3. Deduplicate extracted instances and assign them unique identifiers.\n",
    "    4. Identify and define relationships between the instances of entities.\n",
    "    5. Create a structured knowledge graph using the extracted entities and relationships.\n",
    "\n",
    "    You are the FIRST agent in this workflow.\n",
    "\n",
    "\n",
    "    YOUR TASK:\n",
    "    - Identify high-level, general entity types (e.g., Person, Company, Location, Event).\n",
    "    - For each entity type, list all the possible (available) properties it might have.\n",
    "    - Focus on information that would be useful for structuring a knowledge graph.\n",
    "    - Stay general — do not extract specific names, examples, or relationships.\n",
    "    - Avoid unnecessary details or context-specific examples.\n",
    "\n",
    "    FORMAT:\n",
    "    - Return a valid JSON object.\n",
    "    - Keys = entity types (strings).\n",
    "    - Values = lists of property names (strings).\n",
    "    - Use double quotes for all keys and string values.\n",
    "    - No extra explanation, text, or markdown formatting.\n",
    "\n",
    "    EXAMPLES:\n",
    "    {{\n",
    "        \"Person\": [\"name\", \"age\", \"email\", \"address\"],\n",
    "        \"Company\": [\"name\", \"industry\", \"founded_date\"],\n",
    "        \"Location\": [\"name\", \"coordinates\", \"population\"]\n",
    "    }}\n",
    "\n",
    "    Text to process: {input}\n",
    "\n",
    "    {format_instructions}\n",
    "\n",
    "    Response:\n",
    "    \"\"\",\n",
    "        input_variables=[\"input\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Build the chain\n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    return chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88c9b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract entities from text with retry logic\n",
    "def extract_entity_schema(text, max_retries=3):\n",
    "    \"\"\"\n",
    "    Extract entity types and their properties from input text with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to analyze\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping entity types to lists of properties\n",
    "    \"\"\"\n",
    "    chain = create_entity_extraction_chain()\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = chain.invoke({\"input\": text})\n",
    "            # The result is the entities dictionary from the Pydantic model\n",
    "            return result.get(\"entities\", {})\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Attempt {attempt + 1} failed. Retrying... Error: {str(e)[:100]}...\")\n",
    "            else:\n",
    "                print(f\"All {max_retries} attempts failed. Last error: {str(e)[:100]}...\")\n",
    "                # Return empty dict as fallback\n",
    "                return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bfa82a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Person': ['name', 'age', 'occupation', 'email', 'residence', 'education'], 'Company': ['name', 'industry', 'founded_date', 'location'], 'Location': ['name'], 'Organization': ['name', 'type'], 'Degree': ['name', 'field_of_study', 'institution']}\n",
      "Extracted Entity Schema:\n",
      "\n",
      "Person:\n",
      "- name\n",
      "- age\n",
      "- occupation\n",
      "- email\n",
      "- residence\n",
      "- education\n",
      "\n",
      "Company:\n",
      "- name\n",
      "- industry\n",
      "- founded_date\n",
      "- location\n",
      "\n",
      "Location:\n",
      "- name\n",
      "\n",
      "Organization:\n",
      "- name\n",
      "- type\n",
      "\n",
      "Degree:\n",
      "- name\n",
      "- field_of_study\n",
      "- institution\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "John Doe, a 35-year-old software engineer, works at Google in Mountain View.\n",
    "He graduated from MIT with a degree in Computer Science and has been with the company for 5 years.\n",
    "Google, founded in 1998, is a technology company specializing in internet services and products.\n",
    "John lives in San Francisco and commutes to work daily. His email is john.doe@example.com.\n",
    "\"\"\"\n",
    "\n",
    "entities = extract_entity_schema(sample_text)\n",
    "print(entities)\n",
    "\n",
    "print(\"Extracted Entity Schema:\")\n",
    "for entity_type, properties in entities.items():\n",
    "    print(f\"\\n{entity_type}:\")\n",
    "    for prop in properties:\n",
    "        print(f\"- {prop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98d51aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Entity Schema:\n",
      "\n",
      "Computing Concept:\n",
      "- definition\n",
      "- characteristics\n",
      "- use_cases\n",
      "- limitations\n",
      "- aspects\n",
      "- related_concepts\n",
      "\n",
      "System Architecture:\n",
      "- description\n",
      "- characteristics\n",
      "- components\n",
      "- use_cases\n",
      "- comparison_aspects\n",
      "\n",
      "Platform:\n",
      "- overview\n",
      "- purpose\n",
      "- architecture\n",
      "- components\n",
      "- service_offerings\n",
      "- deployment_aspects\n",
      "- management_aspects\n",
      "- security_aspects\n",
      "- scalability_aspects\n",
      "- reliability_aspects\n",
      "- cost_aspects\n",
      "- features\n",
      "\n",
      "Resource:\n",
      "- description\n",
      "- characteristics\n",
      "- management_aspects\n",
      "- lifecycle_aspects\n",
      "- allocation_aspects\n",
      "- pricing_aspects\n",
      "- type\n",
      "\n",
      "Storage Type:\n",
      "- description\n",
      "- characteristics\n",
      "- use_cases\n",
      "- pricing_models\n",
      "- management_aspects\n",
      "\n",
      "Database Type:\n",
      "- description\n",
      "- characteristics\n",
      "- use_cases\n",
      "- management_aspects\n",
      "- migration_aspects\n",
      "\n",
      "Network Entity:\n",
      "- definition\n",
      "- purpose\n",
      "- characteristics\n",
      "- components\n",
      "- management_aspects\n",
      "- security_aspects\n",
      "- type\n",
      "\n",
      "Service Model:\n",
      "- definition\n",
      "- characteristics\n",
      "- responsibility_division\n",
      "\n",
      "Deployment Model:\n",
      "- definition\n",
      "- characteristics\n",
      "- access_scope\n",
      "- security_aspects\n",
      "- control_aspects\n",
      "- flexibility_aspects\n",
      "\n",
      "Role:\n",
      "- description\n",
      "- function\n",
      "- responsibility\n",
      "\n",
      "Organization:\n",
      "- description\n",
      "- role\n",
      "- service_offerings\n",
      "- market_position\n",
      "- infrastructure_aspects\n",
      "\n",
      "Software Component:\n",
      "- description\n",
      "- function\n",
      "- type\n",
      "- characteristics\n",
      "- related_components\n",
      "- management_aspects\n",
      "- security_aspects\n"
     ]
    }
   ],
   "source": [
    "# Extract entities from the loaded PDF document\n",
    "entities = extract_entity_schema(doc)\n",
    "\n",
    "print(\"\\nExtracted Entity Schema:\")\n",
    "for entity_type, properties in entities.items():\n",
    "    print(f\"\\n{entity_type}:\")\n",
    "    for prop in properties:\n",
    "        print(f\"- {prop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dfdf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Save entity schema for next step\n",
    "with open(\"entity_schema.json\", \"w\") as f:\n",
    "    json.dump({\"entities\": entities}, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41fee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
