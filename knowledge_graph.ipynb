{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4dad227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Dict, List, Any, Optional\n",
    "from collections import Counter\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import uuid\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f36d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7aada5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4884710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-preview-04-17\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d43fffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_content(pdf_path, return_single_string=True, extract_metadata=False):\n",
    "    \"\"\"\n",
    "    Load and parse a PDF document, returning its text content.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        return_single_string (bool): If True, returns the entire PDF content as a single string.\n",
    "                                    If False, returns a list of strings (one per page).\n",
    "        extract_metadata (bool): If True, returns metadata along with content\n",
    "    \n",
    "    Returns:\n",
    "        If return_single_string is True and extract_metadata is False:\n",
    "            str: The entire text content of the PDF\n",
    "        If return_single_string is False and extract_metadata is False:\n",
    "            list: List of strings, one for each page\n",
    "        If extract_metadata is True:\n",
    "            tuple: (content, metadata) where content is either a string or list based on return_single_string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"PDF file not found at: {pdf_path}\")\n",
    "    \n",
    "    # Initialize the loader with the appropriate mode\n",
    "    mode = \"single\" if return_single_string else \"elements\"\n",
    "    loader = PyPDFLoader(pdf_path, mode=mode)\n",
    "    \n",
    "    # Load the documents\n",
    "    docs = loader.load()\n",
    "    \n",
    "    if return_single_string:\n",
    "        # With mode=\"single\", there should only be one document containing all pages\n",
    "        content = docs[0].page_content if docs else \"\"\n",
    "        metadata = docs[0].metadata if docs else {}\n",
    "    else:\n",
    "        # With default mode, each document is a page\n",
    "        content = [doc.page_content for doc in docs]\n",
    "        metadata = [doc.metadata for doc in docs]\n",
    "    \n",
    "    if extract_metadata:\n",
    "        return content, metadata\n",
    "    else:\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f714bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = load_pdf_content(\"Cloud Computing Copy Lecture Notes.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28f5c303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloud Computing Lecture Notes \n",
      "Distributed Computing/Systems \n",
      "Definition: \n",
      "Distributed computing refers to a system where computing resources are distributed \n",
      "across multiple locations rather than being centralized in a single system. This enables \n",
      "task distribution and efficient resource utilization. \n",
      "Why Use Distributed Systems? \n",
      "• Scalability Issues: Traditional computing faces bottlenecks due to hardware \n",
      "limitations, whereas distributed systems allow for hardware scaling. \n",
      "• Connected Devices: In a networked system, connected devices communicate, but \n",
      "this does not necessarily make them distributed. \n",
      "• IoT (Internet of Things): IoT is one of the largest examples of distributed computing. \n",
      "• Multi-layered System Design: Distributed computing enables systems to function \n",
      "in multiple layers, with each layer acting as a distributed entity. \n",
      "• User Perspective: Although the system consists of multiple machines, distributed \n",
      "computing presents a unified system to users. \n",
      " \n",
      "Parallel Comp\n"
     ]
    }
   ],
   "source": [
    "print(doc[:1000])  # Print the first 1000 characters of the loaded document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63716dec",
   "metadata": {},
   "source": [
    "# Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37f14e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic model for entity schema parser\n",
    "class EntitySchema(BaseModel):\n",
    "    \"\"\"Entity types and their properties.\"\"\"\n",
    "    entities: Dict[str, List[str]] = Field(\n",
    "        description=\"Dictionary mapping entity types to their possible properties\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8025570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entity extraction chain\n",
    "def create_entity_extraction_chain():\n",
    "    parser = JsonOutputParser(pydantic_object=EntitySchema)\n",
    "    \n",
    "    # Prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "    You are the first agent in a multi-step workflow to build a Knowledge Graph from raw text.\n",
    "\n",
    "    Workflow Steps Overview:\n",
    "    1. Extract high-level entity types and their properties from the text. [CURRENT STEP]\n",
    "    2. Extract specific instances of entities and their properties based on the identified types.\n",
    "    3. Deduplicate extracted instances and assign them unique identifiers.\n",
    "    4. Identify and define relationships between the instances of entities.\n",
    "    5. Create a structured knowledge graph using the extracted entities and relationships.\n",
    "\n",
    "    You are the FIRST agent in this workflow.\n",
    "\n",
    "\n",
    "    YOUR TASK:\n",
    "    - Identify high-level, general entity types (e.g., Person, Company, Location, Event).\n",
    "    - For each entity type, list all the possible (available) properties it might have.\n",
    "    - Focus on information that would be useful for structuring a knowledge graph.\n",
    "    - Stay general — do not extract specific names, examples, or relationships.\n",
    "    - Avoid unnecessary details or context-specific examples.\n",
    "\n",
    "    FORMAT:\n",
    "    - Return a valid JSON object.\n",
    "    - Keys = entity types (strings).\n",
    "    - Values = lists of property names (strings).\n",
    "    - Use double quotes for all keys and string values.\n",
    "    - No extra explanation, text, or markdown formatting.\n",
    "\n",
    "    EXAMPLES:\n",
    "    {{\n",
    "        \"Person\": [\"name\", \"age\", \"email\", \"address\"],\n",
    "        \"Company\": [\"name\", \"industry\", \"founded_date\"],\n",
    "        \"Location\": [\"name\", \"coordinates\", \"population\"]\n",
    "    }}\n",
    "\n",
    "    Text to process: {input}\n",
    "\n",
    "    {format_instructions}\n",
    "\n",
    "    Response:\n",
    "    \"\"\",\n",
    "        input_variables=[\"input\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Build the chain\n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    return chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88c9b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract entities from text with retry logic\n",
    "def extract_entity_schema(text, max_retries=3):\n",
    "    \"\"\"\n",
    "    Extract entity types and their properties from input text with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to analyze\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping entity types to lists of properties\n",
    "    \"\"\"\n",
    "    chain = create_entity_extraction_chain()\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = chain.invoke({\"input\": text})\n",
    "            # The result is the entities dictionary from the Pydantic model\n",
    "            return result.get(\"entities\", {})\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Attempt {attempt + 1} failed. Retrying... Error: {str(e)[:100]}...\")\n",
    "            else:\n",
    "                print(f\"All {max_retries} attempts failed. Last error: {str(e)[:100]}...\")\n",
    "                # Return empty dict as fallback\n",
    "                return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bfa82a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Person': ['name', 'age', 'occupation', 'employer', 'education', 'residence', 'email', 'tenure'], 'Company': ['name', 'location', 'founded_date', 'industry', 'specialization'], 'Location': ['name'], 'Educational Institution': ['name'], 'Field of Study': ['name']}\n",
      "Extracted Entity Schema:\n",
      "\n",
      "Person:\n",
      "- name\n",
      "- age\n",
      "- occupation\n",
      "- employer\n",
      "- education\n",
      "- residence\n",
      "- email\n",
      "- tenure\n",
      "\n",
      "Company:\n",
      "- name\n",
      "- location\n",
      "- founded_date\n",
      "- industry\n",
      "- specialization\n",
      "\n",
      "Location:\n",
      "- name\n",
      "\n",
      "Educational Institution:\n",
      "- name\n",
      "\n",
      "Field of Study:\n",
      "- name\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "John Doe, a 35-year-old software engineer, works at Google in Mountain View.\n",
    "He graduated from MIT with a degree in Computer Science and has been with the company for 5 years.\n",
    "Google, founded in 1998, is a technology company specializing in internet services and products.\n",
    "John lives in San Francisco and commutes to work daily. His email is john.doe@example.com.\n",
    "\"\"\"\n",
    "\n",
    "entities = extract_entity_schema(sample_text)\n",
    "print(entities)\n",
    "\n",
    "print(\"Extracted Entity Schema:\")\n",
    "for entity_type, properties in entities.items():\n",
    "    print(f\"\\n{entity_type}:\")\n",
    "    for prop in properties:\n",
    "        print(f\"- {prop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98d51aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Entity Schema:\n",
      "\n",
      "Computing Concept:\n",
      "- definition\n",
      "- characteristics\n",
      "- use_cases\n",
      "- limitations\n",
      "- aspects\n",
      "- related_concepts\n",
      "\n",
      "System Architecture:\n",
      "- description\n",
      "- characteristics\n",
      "- components\n",
      "- use_cases\n",
      "- comparison_aspects\n",
      "\n",
      "Platform:\n",
      "- overview\n",
      "- purpose\n",
      "- architecture\n",
      "- components\n",
      "- service_offerings\n",
      "- deployment_aspects\n",
      "- management_aspects\n",
      "- security_aspects\n",
      "- scalability_aspects\n",
      "- reliability_aspects\n",
      "- cost_aspects\n",
      "- features\n",
      "\n",
      "Resource:\n",
      "- description\n",
      "- characteristics\n",
      "- management_aspects\n",
      "- lifecycle_aspects\n",
      "- allocation_aspects\n",
      "- pricing_aspects\n",
      "- type\n",
      "\n",
      "Storage Type:\n",
      "- description\n",
      "- characteristics\n",
      "- use_cases\n",
      "- pricing_models\n",
      "- management_aspects\n",
      "\n",
      "Database Type:\n",
      "- description\n",
      "- characteristics\n",
      "- use_cases\n",
      "- management_aspects\n",
      "- migration_aspects\n",
      "\n",
      "Network Entity:\n",
      "- definition\n",
      "- purpose\n",
      "- characteristics\n",
      "- components\n",
      "- management_aspects\n",
      "- security_aspects\n",
      "- type\n",
      "\n",
      "Service Model:\n",
      "- definition\n",
      "- characteristics\n",
      "- responsibility_division\n",
      "\n",
      "Deployment Model:\n",
      "- definition\n",
      "- characteristics\n",
      "- access_scope\n",
      "- security_aspects\n",
      "- control_aspects\n",
      "- flexibility_aspects\n",
      "\n",
      "Role:\n",
      "- description\n",
      "- function\n",
      "- responsibility\n",
      "\n",
      "Organization:\n",
      "- description\n",
      "- role\n",
      "- service_offerings\n",
      "- market_position\n",
      "- infrastructure_aspects\n",
      "\n",
      "Software Component:\n",
      "- description\n",
      "- function\n",
      "- type\n",
      "- characteristics\n",
      "- related_components\n",
      "- management_aspects\n",
      "- security_aspects\n"
     ]
    }
   ],
   "source": [
    "# Extract entities from the loaded PDF document\n",
    "entities = extract_entity_schema(doc)\n",
    "\n",
    "print(\"\\nExtracted Entity Schema:\")\n",
    "for entity_type, properties in entities.items():\n",
    "    print(f\"\\n{entity_type}:\")\n",
    "    for prop in properties:\n",
    "        print(f\"- {prop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56dfdf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Save entity schema for next step\n",
    "with open(\"entity_schema.json\", \"w\") as f:\n",
    "    json.dump({\"entities\": entities}, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ddede4",
   "metadata": {},
   "source": [
    "# Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f41fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into chunks\n",
    "def split_text_into_chunks(text, chunk_size=5000, chunk_overlap=500):\n",
    "    \"\"\"\n",
    "    Split the input text into manageable chunks using RecursiveCharacterTextSplitter.\n",
    "    \n",
    "    Args:\n",
    "        text: The input text to be split\n",
    "        chunk_size: Maximum size of each chunk in characters\n",
    "        chunk_overlap: Overlap between consecutive chunks\n",
    "        \n",
    "    Returns:\n",
    "        list: List of Document objects\n",
    "    \"\"\"\n",
    "    logger.info(f\"Splitting text into chunks (size={chunk_size}, overlap={chunk_overlap})\")\n",
    "    \n",
    "    # Initialize the splitter with paragraph-focused splitting\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Try to split at paragraph boundaries first\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        keep_separator=True,\n",
    "        add_start_index=True  # Add start position metadata\n",
    "    )\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = splitter.create_documents([text])\n",
    "    \n",
    "    # Add chunk index as metadata\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.metadata[\"chunk_id\"] = i\n",
    "        chunk.metadata[\"total_chunks\"] = len(chunks)\n",
    "    \n",
    "    logger.info(f\"Text split into {len(chunks)} chunks\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30ab1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic models for entity instance extraction\n",
    "class EntityInstances(BaseModel):\n",
    "    \"\"\"Instances of a specific entity type.\"\"\"\n",
    "    Entity: str = Field(description=\"The entity type name\")\n",
    "    Instances: Dict[str, Dict[str, Any]] = Field(\n",
    "        description=\"Dictionary mapping instance names to their properties\"\n",
    "    )\n",
    "\n",
    "class ChunkExtractionResult(BaseModel):\n",
    "    \"\"\"Result of entity extraction from a single chunk.\"\"\"\n",
    "    entities: List[EntityInstances] = Field(\n",
    "        description=\"List of entity types and their instances found in this chunk\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b20783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create entity instance extraction chain\n",
    "def create_entity_instance_extraction_chain():\n",
    "    \"\"\"\n",
    "    Create a chain for extracting entity instances from text chunks.\n",
    "    \n",
    "    Returns:\n",
    "        Chain: A chain that extracts entity instances from text chunks\n",
    "    \"\"\"\n",
    "    # Entity instance extraction result parser\n",
    "    parser = JsonOutputParser(pydantic_object=ChunkExtractionResult)\n",
    "    \n",
    "    # Create prompt template for entity instance extraction\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are part of a multi-step workflow to build a Knowledge Graph from raw text.\n",
    "\n",
    "        Workflow Steps Overview:\n",
    "        1. Extract high-level entity types and their properties from the text. [COMPLETED]\n",
    "        2. Extract specific instances of entities and their properties from text chunks. [CURRENT STEP]\n",
    "        3. Deduplicate extracted instances and assign them unique identifiers.\n",
    "        4. Identify and define relationships between the instances of entities.\n",
    "        5. Create a structured knowledge graph using the extracted entities and relationships.\n",
    "\n",
    "        YOUR TASK:\n",
    "        You are processing a CHUNK of the full text. Focus ONLY on extracting CONCRETE INSTANCES of entities found in this chunk.\n",
    "\n",
    "        GIVEN:\n",
    "        1. A chunk of text\n",
    "        2. A schema of entity types and their possible properties\n",
    "\n",
    "        INSTRUCTIONS:\n",
    "        - Extract ALL instances of the predefined entity types found in this chunk\n",
    "        - For each instance, extract values for as many properties as are mentioned in the text\n",
    "        - Be precise - only extract information explicitly stated in this chunk\n",
    "        - Do NOT make up or infer missing properties\n",
    "        - If a property is not mentioned, omit it from the output (don't include it with null/empty values)\n",
    "\n",
    "        INPUT TEXT CHUNK:\n",
    "        {chunk}\n",
    "\n",
    "        ENTITY TYPES AND THEIR PROPERTIES:\n",
    "        {entity_schema}\n",
    "\n",
    "        FORMAT YOUR RESPONSE AS FOLLOWS:\n",
    "        - Return a valid JSON object\n",
    "        - For each entity type found, include its name and an \"Instances\" object\n",
    "        - \"Instances\" should be a dictionary where:\n",
    "          - Keys are the instance names \n",
    "          - Values are objects containing the (available) instance properties \n",
    "        - Properties not mentioned should be omitted entirely\n",
    "        - If no instances of a particular entity type are found, do not include that entity type\n",
    "\n",
    "        {format_instructions}\n",
    "\n",
    "        EXAMPLE RESPONSE FOR A CHUNK ABOUT PEOPLE AND COMPANIES:\n",
    "        {{\n",
    "        \"entities\": [\n",
    "            {{\n",
    "            \"Entity\": \"Person\",\n",
    "            \"Instances\": {{\n",
    "                \"John Doe\": {{\n",
    "                    \"name\": \"John Doe\",\n",
    "                    \"age\": 35,\n",
    "                    \"email\": \"john@example.com\"\n",
    "                }},\n",
    "                \"Jane Smith\": {{\n",
    "                    \"name\": \"Jane Smith\",\n",
    "                    \"email\": \"jane@example.com\"\n",
    "                }}\n",
    "            }}\n",
    "            }},\n",
    "            {{\n",
    "            \"Entity\": \"Company\",\n",
    "            \"Instances\": {{\n",
    "                \"Google\": {{\n",
    "                    \"industry\": \"Technology\",\n",
    "                    \"founded\": 1998\n",
    "                }}\n",
    "            }}\n",
    "            }}\n",
    "        ]\n",
    "        }}\n",
    "        Begin your extraction now: \"\"\", \n",
    "    input_variables=[\"chunk\", \"entity_schema\", \"chunk_id\"], \n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}, \n",
    "    )\n",
    "\n",
    "    # Build the chain\n",
    "    chain = prompt | llm | parser\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a08d8f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single chunk with retry logic\n",
    "def process_chunk(inputs, max_retries=3):\n",
    "    \"\"\"\n",
    "    Process a single text chunk to extract entity instances with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Dictionary containing chunk and entity_schema\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        \n",
    "    Returns:\n",
    "        dict: Extraction results\n",
    "    \"\"\"\n",
    "    chunk = inputs[\"chunk\"]\n",
    "    entity_schema = inputs[\"entity_schema\"]\n",
    "    chunk_id = chunk.metadata.get(\"chunk_id\", 0)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chain = create_entity_instance_extraction_chain()\n",
    "            result = chain.invoke({\n",
    "                \"chunk\": chunk.page_content,\n",
    "                \"entity_schema\": json.dumps(entity_schema, indent=2),\n",
    "                \"chunk_id\": chunk_id\n",
    "            })\n",
    "            logger.info(f\"Successfully processed chunk {chunk_id}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                logger.warning(f\"Attempt {attempt + 1} failed for chunk {chunk_id}. Retrying... Error: {str(e)[:100]}...\")\n",
    "            else:\n",
    "                logger.error(f\"All {max_retries} attempts failed for chunk {chunk_id}. Error: {str(e)[:100]}...\")\n",
    "                return {\n",
    "                    \"entities\": [],\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"error\": str(e)[:200]\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16454cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the extract_entity_instances_parallel function to stop saving intermediate files\n",
    "def extract_entity_instances_parallel(document, entity_schema, max_concurrency=6):\n",
    "    \"\"\"\n",
    "    Extract entity instances from document chunks in parallel using RunnableParallel.\n",
    "    \n",
    "    Args:\n",
    "        document: The full text document\n",
    "        entity_schema: Dictionary of entity types and their properties\n",
    "        max_concurrency: Maximum number of chunks to process in parallel\n",
    "        \n",
    "    Returns:\n",
    "        list: List of entity instances extracted from all chunks\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(\"Beginning entity instance extraction (parallel)\")\n",
    "    \n",
    "    # Split the document into chunks\n",
    "    chunks = split_text_into_chunks(document, chunk_size=5000, chunk_overlap=500)\n",
    "    logger.info(f\"Document split into {len(chunks)} chunks\")\n",
    "    \n",
    "    # Prepare inputs for each chunk\n",
    "    inputs = [{\"chunk\": chunk, \"entity_schema\": entity_schema} for chunk in chunks]\n",
    "    \n",
    "    # Create a RunnableLambda for chunk processing\n",
    "    chunk_processor = RunnableLambda(process_chunk)\n",
    "    \n",
    "    # Process chunks in batches with progress tracking\n",
    "    all_results = []\n",
    "    batch_size = min(max_concurrency, len(chunks))\n",
    "    \n",
    "    # Use tqdm for progress tracking in batches\n",
    "    with tqdm(total=len(chunks), desc=\"Processing chunks\") as progress_bar:\n",
    "        for i in range(0, len(inputs), batch_size):\n",
    "            batch_inputs = inputs[i:i+batch_size]\n",
    "            \n",
    "            # Process the batch in parallel\n",
    "            batch_results = chunk_processor.batch(batch_inputs, config={\"max_concurrency\": max_concurrency})\n",
    "            all_results.extend(batch_results)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.update(len(batch_inputs))\n",
    "            \n",
    "            # Log progress without saving intermediate files\n",
    "            logger.info(f\"Processed {min(i+batch_size, len(inputs))}/{len(inputs)} chunks ({min((i+batch_size)/len(inputs), 1.0)*100:.1f}%)\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    logger.info(f\"Entity instance extraction completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73fbd50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_chunk_results(chunk_results):\n",
    "    \"\"\"\n",
    "    Merge the results from all chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunk_results: List of extraction results from chunks\n",
    "        \n",
    "    Returns:\n",
    "        list: Combined list of entity instances\n",
    "    \"\"\"\n",
    "    merged_results = []\n",
    "    \n",
    "    # Check for errors\n",
    "    errors = [r for r in chunk_results if r.get(\"error\")]\n",
    "    if errors:\n",
    "        logger.warning(f\"{len(errors)} chunks had errors during processing\")\n",
    "    \n",
    "    # Group by entity type\n",
    "    entity_instances = {}\n",
    "    for result in chunk_results:\n",
    "        if \"entities\" not in result:\n",
    "            continue\n",
    "            \n",
    "        for entity_data in result[\"entities\"]:\n",
    "            entity_type = entity_data.get(\"Entity\")\n",
    "            instances = entity_data.get(\"Instances\", {})\n",
    "            \n",
    "            if entity_type not in entity_instances:\n",
    "                entity_instances[entity_type] = {}\n",
    "                \n",
    "            # Merge instances from this chunk into the collected instances\n",
    "            # If instance already exists, update with any new properties\n",
    "            for instance_name, instance_props in instances.items():\n",
    "                if instance_name in entity_instances[entity_type]:\n",
    "                    # Add any new properties from this instance\n",
    "                    entity_instances[entity_type][instance_name].update(instance_props)\n",
    "                else:\n",
    "                    # Add the new instance\n",
    "                    entity_instances[entity_type][instance_name] = instance_props\n",
    "    \n",
    "    # Convert to the expected format\n",
    "    for entity_type, instances in entity_instances.items():\n",
    "        merged_results.append({\n",
    "            \"Entity\": entity_type,\n",
    "            \"Instances\": instances\n",
    "        })\n",
    "    \n",
    "    logger.info(f\"Merged results for {len(entity_instances)} entity types\")\n",
    "    return merged_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99bbe428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 16:15:49,153 - INFO - Beginning entity instance extraction (parallel)\n",
      "2025-04-27 16:15:49,155 - INFO - Splitting text into chunks (size=5000, overlap=500)\n",
      "2025-04-27 16:15:49,164 - INFO - Text split into 11 chunks\n",
      "2025-04-27 16:15:49,165 - INFO - Document split into 11 chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99b8256c4cd47568b9701f7afb0088f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chunks:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 16:16:23,535 - INFO - Successfully processed chunk 0\n",
      "2025-04-27 16:16:24,912 - INFO - Successfully processed chunk 3\n",
      "2025-04-27 16:16:33,934 - INFO - Successfully processed chunk 1\n",
      "2025-04-27 16:16:35,341 - INFO - Successfully processed chunk 2\n",
      "2025-04-27 16:16:42,185 - INFO - Successfully processed chunk 4\n",
      "2025-04-27 16:16:43,083 - INFO - Successfully processed chunk 5\n",
      "2025-04-27 16:16:43,097 - INFO - Processed 6/11 chunks (54.5%)\n",
      "2025-04-27 16:17:11,445 - INFO - Successfully processed chunk 6\n",
      "2025-04-27 16:17:33,340 - INFO - Successfully processed chunk 10\n",
      "2025-04-27 16:17:37,597 - INFO - Successfully processed chunk 8\n",
      "2025-04-27 16:17:53,346 - INFO - Successfully processed chunk 9\n",
      "2025-04-27 16:18:10,034 - INFO - Successfully processed chunk 7\n",
      "2025-04-27 16:18:10,042 - INFO - Processed 11/11 chunks (100.0%)\n",
      "2025-04-27 16:18:10,045 - INFO - Entity instance extraction completed in 140.89 seconds\n",
      "2025-04-27 16:18:10,051 - INFO - Merged results for 12 entity types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted 538 instances across 12 entity types\n",
      "Results saved to entity_instances.json\n",
      "\n",
      "Sample instances:\n",
      "\n",
      "Computing Concept (157 instances):\n",
      "  Instance 1: Distributed Computing - {'definition': 'a system where computing resources are distributed across multiple locations rather ...\n",
      "  Instance 2: Traditional computing - {'limitations': ['faces bottlenecks due to hardware limitations']}...\n",
      "  Instance 3: Parallel Computing - {'definition': 'involves executing multiple processes simultaneously to enhance speed and efficiency...\n",
      "  ... and 154 more\n",
      "\n",
      "System Architecture (31 instances):\n",
      "  Instance 1: Distributed Systems - {'description': 'a system where computing resources are distributed across multiple locations rather...\n",
      "  Instance 2: Clusters - {'description': 'consist of multiple machines with similar hardware and operating systems, working t...\n",
      "  Instance 3: Grids - {'description': 'consist of heterogeneous systems that may have different hardware, OS, and configur...\n",
      "  ... and 28 more\n",
      "\n",
      "Resource (92 instances):\n",
      "  Instance 1: Dedicated machines - {'characteristics': ['needed for parallel computing']}...\n",
      "  Instance 2: Cluster on Demand - {'type': 'cluster-based resources', 'allocation_aspects': ['Users can request cluster-based resource...\n",
      "  Instance 3: internet connection - {'type': 'internet connection'}...\n",
      "  ... and 89 more\n",
      "\n",
      "Storage Type (16 instances):\n",
      "  Instance 1: Shared Folders - {'description': 'Simply sharing files', 'characteristics': ['does not make a system truly distribute...\n",
      "  Instance 2: RAID (Redundant Array of Independent Disks) - {'description': 'Combines multiple storage disks into one logical storage unit', 'characteristics': ...\n",
      "  Instance 3: VM data storage - {}...\n",
      "  ... and 13 more\n",
      "\n",
      "Network Entity (60 instances):\n",
      "  Instance 1: Connected Devices - {'characteristics': ['communicate']}...\n",
      "  Instance 2: LAN (Local Area Networks) - {'type': 'Local Area Networks', 'characteristics': ['all machines are geographically close and in th...\n",
      "  Instance 3: networked devices - {'characteristics': ['communicate'], 'related_components': ['applications']}...\n",
      "  ... and 57 more\n",
      "\n",
      "Role (28 instances):\n",
      "  Instance 1: Individual users - {'function': ['can participate in grid computing', 'can sell extra computational resources within a ...\n",
      "  Instance 2: Users - {'description': 'Part of Stakeholders in the Cloud Ecosystem'}...\n",
      "  Instance 3: Cloud providers - {'role': 'Cloud provider', 'service_offerings': ['offer HPC as a service', 'combine resources into a...\n",
      "  ... and 25 more\n",
      "\n",
      "Organization (23 instances):\n",
      "  Instance 1: Organizations - {'role': ['do not allow their internal systems to be part of a grid due to security concerns']}...\n",
      "  Instance 2: AWS - {'service_offerings': ['bare metal as EC2 instances', 'other services similar to OpenStack'], 'role'...\n",
      "  Instance 3: AWS (Amazon Web Services) - {'role': 'Cloud provider', 'service_offerings': 'offer HPC as a service', 'market_position': 'one of...\n",
      "  ... and 20 more\n",
      "\n",
      "Software Component (66 instances):\n",
      "  Instance 1: Middleware - {'description': 'The software layer that facilitates communication between different system componen...\n",
      "  Instance 2: Kubernetes - {'function': 'Cloud computing allows users to build their own Kubernetes clusters or use Kubernetes ...\n",
      "  Instance 3: Apache Spark - {'type': 'Example'}...\n",
      "  ... and 63 more\n",
      "\n",
      "Platform (24 instances):\n",
      "  Instance 1: AWS EC2 (Elastic Compute Cloud) - {'features': 'provides rapid elasticity'}...\n",
      "  Instance 2: AWS EC2 - {}...\n",
      "  Instance 3: Kubernetes - {'purpose': 'container orchestration platform', 'architecture': ['Control Plane', 'Worker Nodes (Com...\n",
      "  ... and 21 more\n",
      "\n",
      "Service Model (17 instances):\n",
      "  Instance 1: Utility Computing Model - {'definition': 'Users pay only for what they consume.'}...\n",
      "  Instance 2: pay-as-you-go billing models - {'characteristics': 'Cloud providers implement pay-as-you-go billing models.'}...\n",
      "  Instance 3: Cloud Service Model - {'definition': 'define How users access services, How operators manage resources, Responsibility div...\n",
      "  ... and 14 more\n",
      "\n",
      "Deployment Model (17 instances):\n",
      "  Instance 1: Private clouds - {'characteristics': 'not massive', 'access_scope': 'for organizations'}...\n",
      "  Instance 2: Public cloud - {'market_position': '95-96% of cloud infrastructure belongs to a few major companies'}...\n",
      "  Instance 3: Cloud Deployment Models - {'definition': 'define how cloud infrastructure is set up, who can access it, and its intended use'}...\n",
      "  ... and 14 more\n",
      "\n",
      "Database Type (7 instances):\n",
      "  Instance 1: Relational Databases (RDS) - {'description': 'Structured storage with SQL support', 'characteristics': ['Structured storage', 'SQ...\n",
      "  Instance 2: Non-Relational Databases (NoSQL) - {'description': 'Schema-less, scalable databases', 'characteristics': ['Schema-less', 'scalable']}...\n",
      "  Instance 3: Database Migration Services - {'use_cases': ['migrating data between databases']}...\n",
      "  ... and 4 more\n"
     ]
    }
   ],
   "source": [
    "# Load the entity schema\n",
    "with open(\"entity_schema.json\", \"r\") as f:\n",
    "    entity_schema_data = json.load(f)\n",
    "    entity_schema = entity_schema_data.get(\"entities\", {})\n",
    "\n",
    "# Extract entity instances using parallel processing\n",
    "chunk_results = extract_entity_instances_parallel(\n",
    "    document=doc,\n",
    "    entity_schema=entity_schema,\n",
    "    max_concurrency=6  # Process 6 chunks in parallel\n",
    ")\n",
    "\n",
    "# Merge results from all chunks\n",
    "merged_results = merge_chunk_results(chunk_results)\n",
    "\n",
    "# Save only one final result file\n",
    "output_filepath = \"entity_instances.json\"\n",
    "with open(output_filepath, \"w\") as f:\n",
    "    json.dump(merged_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nExtracted {sum(len(entity['Instances']) for entity in merged_results)} instances across {len(merged_results)} entity types\")\n",
    "print(f\"Results saved to {output_filepath}\")\n",
    "\n",
    "# Print sample results\n",
    "print(\"\\nSample instances:\")\n",
    "for entity in merged_results:\n",
    "    entity_type = entity[\"Entity\"]\n",
    "    instances = entity[\"Instances\"]\n",
    "    print(f\"\\n{entity_type} ({len(instances)} instances):\")\n",
    "    \n",
    "    # Get list of instance keys (names) and take first 3\n",
    "    instance_keys = list(instances.keys())[:3]\n",
    "    \n",
    "    # Display up to 3 instances\n",
    "    for i, instance_name in enumerate(instance_keys):\n",
    "        instance_data = instances[instance_name]\n",
    "        print(f\"  Instance {i+1}: {instance_name} - {str(instance_data)[:100]}...\")\n",
    "    \n",
    "    if len(instances) > 3:\n",
    "        print(f\"  ... and {len(instances) - 3} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd1f739",
   "metadata": {},
   "source": [
    "# Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07a8743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_and_assign_ids(entity_data):\n",
    "    \"\"\"\n",
    "    Remove duplicate entity instances and assign unique IDs to each remaining instance.\n",
    "    \n",
    "    Args:\n",
    "        entity_data: List of dictionaries containing entity types and their instances\n",
    "        \n",
    "    Returns:\n",
    "        list: Deduplicated entity instances with unique IDs assigned\n",
    "    \"\"\"\n",
    "    logger.info(\"Beginning deduplication and ID assignment\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create a deep copy of the input data to avoid modifying it\n",
    "    deduplicated_data = []\n",
    "    \n",
    "    # Track statistics\n",
    "    total_instances_before = 0\n",
    "    total_instances_after = 0\n",
    "    duplicates_found = 0\n",
    "    \n",
    "    # Process each entity type\n",
    "    for entity in entity_data:\n",
    "        entity_type = entity[\"Entity\"]\n",
    "        instances = entity[\"Instances\"]\n",
    "        total_instances_before += len(instances)\n",
    "        \n",
    "        # Create a new dictionary for the deduplicated instances\n",
    "        deduplicated_instances = {}\n",
    "        \n",
    "        # For each instance, check if it already exists and merge if needed\n",
    "        for instance_name, instance_props in instances.items():\n",
    "            # If this instance name already exists, merge properties\n",
    "            if instance_name in deduplicated_instances:\n",
    "                duplicates_found += 1\n",
    "                existing_props = deduplicated_instances[instance_name]\n",
    "                \n",
    "                # Merge properties, keeping all unique properties\n",
    "                for key, value in instance_props.items():\n",
    "                    if key not in existing_props:\n",
    "                        existing_props[key] = value\n",
    "                        \n",
    "                # Log information about the merge\n",
    "                logger.debug(f\"Merged duplicate instance '{instance_name}' in entity type '{entity_type}'\")\n",
    "            else:\n",
    "                # Add this instance to the deduplicated set\n",
    "                deduplicated_instances[instance_name] = instance_props.copy()\n",
    "        \n",
    "        # Now assign unique IDs to each instance\n",
    "        for instance_name, props in deduplicated_instances.items():\n",
    "            # Generate a UUID for this instance\n",
    "            instance_id = str(uuid.uuid4())\n",
    "            \n",
    "            # Add the ID to the properties\n",
    "            props[\"id\"] = instance_id\n",
    "            \n",
    "            # Ensure 'name' property matches the instance name\n",
    "            if \"name\" not in props or props[\"name\"] != instance_name:\n",
    "                props[\"name\"] = instance_name\n",
    "        \n",
    "        # Add the deduplicated entity to the result\n",
    "        deduplicated_data.append({\n",
    "            \"Entity\": entity_type,\n",
    "            \"Instances\": deduplicated_instances\n",
    "        })\n",
    "        \n",
    "        total_instances_after += len(deduplicated_instances)\n",
    "        \n",
    "        logger.info(f\"Processed entity type '{entity_type}': {len(instances)} instances → {len(deduplicated_instances)} unique instances\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    logger.info(f\"Deduplication completed in {end_time - start_time:.2f} seconds\")\n",
    "    logger.info(f\"Total instances before: {total_instances_before}, after: {total_instances_after}\")\n",
    "    logger.info(f\"Removed {duplicates_found} duplicate instances\")\n",
    "    \n",
    "    return deduplicated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14014df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_step3():\n",
    "    \"\"\"\n",
    "    Execute Step 3: Deduplicate entity instances and assign unique IDs.\n",
    "    \"\"\"\n",
    "    # Load data from Step 2\n",
    "    logger.info(\"Loading entity instances from Step 2\")\n",
    "    try:\n",
    "        with open(\"entity_instances.json\", \"r\") as f:\n",
    "            entity_instances = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(\"entity_instances.json not found. Please complete Step 2 first.\")\n",
    "        return None\n",
    "    \n",
    "    # Deduplicate and assign IDs\n",
    "    deduplicated_data = deduplicate_and_assign_ids(entity_instances)\n",
    "    \n",
    "    # Save the deduplicated data\n",
    "    output_filepath = \"deduplicated_entities.json\"\n",
    "    with open(output_filepath, \"w\") as f:\n",
    "        json.dump(deduplicated_data, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"Deduplicated entities saved to {output_filepath}\")\n",
    "    \n",
    "    # Print summary\n",
    "    total_instances = sum(len(entity[\"Instances\"]) for entity in deduplicated_data)\n",
    "    print(f\"\\nDeduplication complete: {total_instances} unique instances across {len(deduplicated_data)} entity types\")\n",
    "    print(f\"Results saved to {output_filepath}\")\n",
    "    \n",
    "    # Print sample results\n",
    "    print(\"\\nSample deduplicated instances:\")\n",
    "    for entity in deduplicated_data:\n",
    "        entity_type = entity[\"Entity\"]\n",
    "        instances = entity[\"Instances\"]\n",
    "        print(f\"\\n{entity_type} ({len(instances)} instances):\")\n",
    "        \n",
    "        # Get list of instance keys (names) and take first 3\n",
    "        instance_keys = list(instances.keys())[:3]\n",
    "        \n",
    "        # Display up to 3 instances\n",
    "        for i, instance_name in enumerate(instance_keys):\n",
    "            instance_data = instances[instance_name]\n",
    "            # Show ID and a few other properties \n",
    "            props_preview = {k: v for k, v in list(instance_data.items())[:4]}\n",
    "            print(f\"  Instance {i+1}: {instance_name} - {props_preview}\")\n",
    "        \n",
    "        if len(instances) > 3:\n",
    "            print(f\"  ... and {len(instances) - 3} more\")\n",
    "    \n",
    "    return deduplicated_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80795399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 16:58:31,603 - INFO - Loading entity instances from Step 2\n",
      "2025-04-27 16:58:31,634 - INFO - Beginning deduplication and ID assignment\n",
      "2025-04-27 16:58:31,640 - INFO - Processed entity type 'Computing Concept': 157 instances → 157 unique instances\n",
      "2025-04-27 16:58:31,642 - INFO - Processed entity type 'System Architecture': 31 instances → 31 unique instances\n",
      "2025-04-27 16:58:31,645 - INFO - Processed entity type 'Resource': 92 instances → 92 unique instances\n",
      "2025-04-27 16:58:31,647 - INFO - Processed entity type 'Storage Type': 16 instances → 16 unique instances\n",
      "2025-04-27 16:58:31,651 - INFO - Processed entity type 'Network Entity': 60 instances → 60 unique instances\n",
      "2025-04-27 16:58:31,653 - INFO - Processed entity type 'Role': 28 instances → 28 unique instances\n",
      "2025-04-27 16:58:31,655 - INFO - Processed entity type 'Organization': 23 instances → 23 unique instances\n",
      "2025-04-27 16:58:31,659 - INFO - Processed entity type 'Software Component': 66 instances → 66 unique instances\n",
      "2025-04-27 16:58:31,663 - INFO - Processed entity type 'Platform': 24 instances → 24 unique instances\n",
      "2025-04-27 16:58:31,665 - INFO - Processed entity type 'Service Model': 17 instances → 17 unique instances\n",
      "2025-04-27 16:58:31,667 - INFO - Processed entity type 'Deployment Model': 17 instances → 17 unique instances\n",
      "2025-04-27 16:58:31,669 - INFO - Processed entity type 'Database Type': 7 instances → 7 unique instances\n",
      "2025-04-27 16:58:31,671 - INFO - Deduplication completed in 0.03 seconds\n",
      "2025-04-27 16:58:31,675 - INFO - Total instances before: 538, after: 538\n",
      "2025-04-27 16:58:31,680 - INFO - Removed 0 duplicate instances\n",
      "2025-04-27 16:58:31,788 - INFO - Deduplicated entities saved to deduplicated_entities.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deduplication complete: 538 unique instances across 12 entity types\n",
      "Results saved to deduplicated_entities.json\n",
      "\n",
      "Sample deduplicated instances:\n",
      "\n",
      "Computing Concept (157 instances):\n",
      "  Instance 1: Distributed Computing - {'definition': 'a system where computing resources are distributed across multiple locations rather than being centralized in a single system.', 'aspects': ['enables task distribution and efficient resource utilization', 'enables systems to function in multiple layers, with each layer acting as a distributed entity', 'presents a unified system to users'], 'use_cases': ['IoT (Internet of Things)'], 'related_concepts': ['Parallel Computing', 'Cloud Computing']}\n",
      "  Instance 2: Traditional computing - {'limitations': ['faces bottlenecks due to hardware limitations'], 'id': 'aeb3fe06-4a88-4285-ba29-90cc292e4b99', 'name': 'Traditional computing'}\n",
      "  Instance 3: Parallel Computing - {'definition': 'involves executing multiple processes simultaneously to enhance speed and efficiency.', 'aspects': ['Distribution & Speed', 'Core Objective'], 'use_cases': ['Vector processing', 'Image processing', 'Matrix multiplication'], 'limitations': ['Not all applications can be parallelized', 'Some components of code can be executed in parallel, while others may not', 'Specific programming languages are required for parallel computing']}\n",
      "  ... and 154 more\n",
      "\n",
      "System Architecture (31 instances):\n",
      "  Instance 1: Distributed Systems - {'description': 'a system where computing resources are distributed across multiple locations rather than being centralized in a single system.', 'characteristics': ['allow for hardware scaling', 'function in multiple layers, with each layer acting as a distributed entity', 'presents a unified system to users'], 'components': ['multiple machines', 'Shared Folders', 'Middleware'], 'use_cases': ['IoT (Internet of Things)']}\n",
      "  Instance 2: Clusters - {'description': 'consist of multiple machines with similar hardware and operating systems, working together in a symmetric manner.', 'characteristics': ['Middleware abstracts hardware, OS, and software details from users', 'All machines have the same type of hardware and OS', 'Parallel computing is implemented efficiently in clusters', 'Middleware in clusters has fewer complexities', 'typically exist in LAN (Local Area Networks)', 'all machines are geographically close and in the same environment'], 'components': ['multiple machines', 'Middleware'], 'comparison_aspects': ['compared to Grids']}\n",
      "  Instance 3: Grids - {'description': 'consist of heterogeneous systems that may have different hardware, OS, and configurations.', 'characteristics': ['operate in an asymmetric manner', 'Middleware in grids has to handle diverse configurations, leading to potential performance degradation', 'geographically distributed'], 'components': ['heterogeneous systems', 'Middleware'], 'use_cases': ['solving large-scale computational problems']}\n",
      "  ... and 28 more\n",
      "\n",
      "Resource (92 instances):\n",
      "  Instance 1: Dedicated machines - {'characteristics': ['needed for parallel computing'], 'id': '187ef863-1673-4966-a6f8-2e1eedb7b289', 'name': 'Dedicated machines'}\n",
      "  Instance 2: Cluster on Demand - {'type': 'cluster-based resources', 'allocation_aspects': ['Users can request cluster-based resources directly from cloud providers'], 'related_concepts': ['Clusters', 'Cloud Computing'], 'id': 'b0329775-e73b-4764-b003-496bec194fa0'}\n",
      "  Instance 3: internet connection - {'type': 'internet connection', 'id': '13ea646b-8c3f-44b1-8913-24c53f8e3d3c', 'name': 'internet connection'}\n",
      "  ... and 89 more\n",
      "\n",
      "Storage Type (16 instances):\n",
      "  Instance 1: Shared Folders - {'description': 'Simply sharing files', 'characteristics': ['does not make a system truly distributed'], 'management_aspects': ['Proper access control and system design are needed'], 'id': 'fc6142b6-4fd0-4deb-8132-c52ba02c22c7'}\n",
      "  Instance 2: RAID (Redundant Array of Independent Disks) - {'description': 'Combines multiple storage disks into one logical storage unit', 'characteristics': 'Provides redundancy for fault tolerance', 'id': '26973c28-013c-4290-a34c-ec0c43c2930b', 'name': 'RAID (Redundant Array of Independent Disks)'}\n",
      "  Instance 3: VM data storage - {'id': 'fdd23b00-0277-4adc-8960-25dbb37901ac', 'name': 'VM data storage'}\n",
      "  ... and 13 more\n",
      "\n",
      "Network Entity (60 instances):\n",
      "  Instance 1: Connected Devices - {'characteristics': ['communicate'], 'id': 'dca18e80-6d8a-43e9-ae5e-a37829404e61', 'name': 'Connected Devices'}\n",
      "  Instance 2: LAN (Local Area Networks) - {'type': 'Local Area Networks', 'characteristics': ['all machines are geographically close and in the same environment'], 'related_concepts': ['Clusters'], 'id': 'ed065304-92db-4c9f-919e-ae4729b0bfde'}\n",
      "  Instance 3: networked devices - {'characteristics': ['communicate'], 'related_components': ['applications'], 'id': '00496a9a-7fc3-4139-8c4d-c9ee66ac05f7', 'name': 'networked devices'}\n",
      "  ... and 57 more\n",
      "\n",
      "Role (28 instances):\n",
      "  Instance 1: Individual users - {'function': ['can participate in grid computing', 'can sell extra computational resources within a grid'], 'id': '42ea9f10-a7b9-4369-96e4-04358298d23a', 'name': 'Individual users'}\n",
      "  Instance 2: Users - {'description': 'Part of Stakeholders in the Cloud Ecosystem', 'id': 'c4e3badf-809e-42fe-98f4-fde54dc75e78', 'name': 'Users'}\n",
      "  Instance 3: Cloud providers - {'role': 'Cloud provider', 'service_offerings': ['offer HPC as a service', 'combine resources into a shared pool', 'implement pay-as-you-go billing models.'], 'id': '7ae1a6b7-ce5a-4add-9e91-812a34d705c0', 'name': 'Cloud providers'}\n",
      "  ... and 25 more\n",
      "\n",
      "Organization (23 instances):\n",
      "  Instance 1: Organizations - {'role': ['do not allow their internal systems to be part of a grid due to security concerns'], 'id': '77425516-7b76-43ee-81a1-5faf97f745f4', 'name': 'Organizations'}\n",
      "  Instance 2: AWS - {'service_offerings': ['bare metal as EC2 instances', 'other services similar to OpenStack'], 'role': ['large cloud service providers (CSPs)'], 'infrastructure_aspects': 'uses different hypervisors', 'id': '0bf2182f-aaba-497f-b185-dddf584b6109'}\n",
      "  Instance 3: AWS (Amazon Web Services) - {'role': 'Cloud provider', 'service_offerings': 'offer HPC as a service', 'market_position': 'one of a few major companies like: AWS', 'id': 'fd0bfc59-8944-4372-a611-b0e5970e39c0'}\n",
      "  ... and 20 more\n",
      "\n",
      "Software Component (66 instances):\n",
      "  Instance 1: Middleware - {'description': 'The software layer that facilitates communication between different system components.', 'function': ['Middleware is responsible for managing resources in the cloud.', 'Middleware is responsible for resource pooling.'], 'characteristics': ['abstracts hardware, OS, and software details from users', 'in clusters has fewer complexities', 'in grids has to handle diverse configurations, leading to potential performance degradation'], 'related_components': ['system components', 'applications', 'networked devices', 'connected components']}\n",
      "  Instance 2: Kubernetes - {'function': 'Cloud computing allows users to build their own Kubernetes clusters or use Kubernetes as a managed service.', 'id': 'd904dfbb-4f06-48d7-af19-20711463afd6', 'name': 'Kubernetes'}\n",
      "  Instance 3: Apache Spark - {'type': 'Example', 'id': '3972cdd2-4b85-477a-9aea-9cccf296ee98', 'name': 'Apache Spark'}\n",
      "  ... and 63 more\n",
      "\n",
      "Platform (24 instances):\n",
      "  Instance 1: AWS EC2 (Elastic Compute Cloud) - {'features': 'provides rapid elasticity', 'id': 'd5dc77f5-d020-49a0-a5ee-a89f813bfcd6', 'name': 'AWS EC2 (Elastic Compute Cloud)'}\n",
      "  Instance 2: AWS EC2 - {'id': 'ccea6174-71d9-4aab-91f6-f9e318d30df0', 'name': 'AWS EC2'}\n",
      "  Instance 3: Kubernetes - {'purpose': 'container orchestration platform', 'architecture': ['Control Plane', 'Worker Nodes (Computing Plane)'], 'components': ['Node', 'Pod', 'Scheduler', 'etcd', 'API Server', 'Controller Manager', 'Kube-proxy'], 'deployment_aspects': ['System Build (Manual Deployment)', 'Kubernetes as a Service (Managed)']}\n",
      "  ... and 21 more\n",
      "\n",
      "Service Model (17 instances):\n",
      "  Instance 1: Utility Computing Model - {'definition': 'Users pay only for what they consume.', 'id': 'd9308dbd-af49-4f56-936b-eb32ca3cd12f', 'name': 'Utility Computing Model'}\n",
      "  Instance 2: pay-as-you-go billing models - {'characteristics': 'Cloud providers implement pay-as-you-go billing models.', 'id': 'f874e9b5-765b-43e8-9f54-8819d36e02aa', 'name': 'pay-as-you-go billing models'}\n",
      "  Instance 3: Cloud Service Model - {'definition': 'define How users access services, How operators manage resources, Responsibility division between users and cloud service providers (CSPs)', 'id': 'c8528fd0-6de7-45ee-b470-650434f0e92f', 'name': 'Cloud Service Model'}\n",
      "  ... and 14 more\n",
      "\n",
      "Deployment Model (17 instances):\n",
      "  Instance 1: Private clouds - {'characteristics': 'not massive', 'access_scope': 'for organizations', 'id': 'ae9e4296-8986-4981-8b1d-80741cebc45a', 'name': 'Private clouds'}\n",
      "  Instance 2: Public cloud - {'market_position': '95-96% of cloud infrastructure belongs to a few major companies', 'id': '1ef0043c-5f83-4b97-a2fb-596d6e1e36e0', 'name': 'Public cloud'}\n",
      "  Instance 3: Cloud Deployment Models - {'definition': 'define how cloud infrastructure is set up, who can access it, and its intended use', 'id': '623b86cd-7168-487b-b9fa-04c5f114d4fd', 'name': 'Cloud Deployment Models'}\n",
      "  ... and 14 more\n",
      "\n",
      "Database Type (7 instances):\n",
      "  Instance 1: Relational Databases (RDS) - {'description': 'Structured storage with SQL support', 'characteristics': ['Structured storage', 'SQL support'], 'id': '579a6d22-cdb4-4c27-8fb3-61d96dc60aea', 'name': 'Relational Databases (RDS)'}\n",
      "  Instance 2: Non-Relational Databases (NoSQL) - {'description': 'Schema-less, scalable databases', 'characteristics': ['Schema-less', 'scalable'], 'id': '20de9269-2f13-4cae-97b5-28a09ee4d869', 'name': 'Non-Relational Databases (NoSQL)'}\n",
      "  Instance 3: Database Migration Services - {'use_cases': ['migrating data between databases'], 'id': 'e57f64f3-58e1-40cf-8438-02ae740617ca', 'name': 'Database Migration Services'}\n",
      "  ... and 4 more\n"
     ]
    }
   ],
   "source": [
    "# Execute Step 3\n",
    "deduplicated_entities = process_step3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c42bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
