{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4dad227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Dict, List, Any, Optional\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import uuid\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f36d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7aada5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4884710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-preview-04-17\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d43fffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_content(pdf_path, return_single_string=True, extract_metadata=False):\n",
    "    \"\"\"\n",
    "    Load and parse a PDF document, returning its text content.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        return_single_string (bool): If True, returns the entire PDF content as a single string.\n",
    "                                    If False, returns a list of strings (one per page).\n",
    "        extract_metadata (bool): If True, returns metadata along with content\n",
    "    \n",
    "    Returns:\n",
    "        If return_single_string is True and extract_metadata is False:\n",
    "            str: The entire text content of the PDF\n",
    "        If return_single_string is False and extract_metadata is False:\n",
    "            list: List of strings, one for each page\n",
    "        If extract_metadata is True:\n",
    "            tuple: (content, metadata) where content is either a string or list based on return_single_string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"PDF file not found at: {pdf_path}\")\n",
    "    \n",
    "    # Initialize the loader with the appropriate mode\n",
    "    mode = \"single\" if return_single_string else \"elements\"\n",
    "    loader = PyPDFLoader(pdf_path, mode=mode)\n",
    "    \n",
    "    # Load the documents\n",
    "    docs = loader.load()\n",
    "    \n",
    "    if return_single_string:\n",
    "        # With mode=\"single\", there should only be one document containing all pages\n",
    "        content = docs[0].page_content if docs else \"\"\n",
    "        metadata = docs[0].metadata if docs else {}\n",
    "    else:\n",
    "        # With default mode, each document is a page\n",
    "        content = [doc.page_content for doc in docs]\n",
    "        metadata = [doc.metadata for doc in docs]\n",
    "    \n",
    "    if extract_metadata:\n",
    "        return content, metadata\n",
    "    else:\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f714bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = load_pdf_content(\"Cloud Computing Copy Lecture Notes.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28f5c303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloud Computing Lecture Notes \n",
      "Distributed Computing/Systems \n",
      "Definition: \n",
      "Distributed computing refers to a system where computing resources are distributed \n",
      "across multiple locations rather than being centralized in a single system. This enables \n",
      "task distribution and efficient resource utilization. \n",
      "Why Use Distributed Systems? \n",
      "• Scalability Issues: Traditional computing faces bottlenecks due to hardware \n",
      "limitations, whereas distributed systems allow for hardware scaling. \n",
      "• Connected Devices: In a networked system, connected devices communicate, but \n",
      "this does not necessarily make them distributed. \n",
      "• IoT (Internet of Things): IoT is one of the largest examples of distributed computing. \n",
      "• Multi-layered System Design: Distributed computing enables systems to function \n",
      "in multiple layers, with each layer acting as a distributed entity. \n",
      "• User Perspective: Although the system consists of multiple machines, distributed \n",
      "computing presents a unified system to users. \n",
      " \n",
      "Parallel Comp\n"
     ]
    }
   ],
   "source": [
    "print(doc[:1000])  # Print the first 1000 characters of the loaded document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37f14e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic model for entity schema parser\n",
    "class EntitySchema(BaseModel):\n",
    "    \"\"\"Entity types and their properties.\"\"\"\n",
    "    entities: Dict[str, List[str]] = Field(\n",
    "        description=\"Dictionary mapping entity types to their possible properties\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8025570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entity extraction chain\n",
    "def create_entity_extraction_chain():\n",
    "    parser = JsonOutputParser(pydantic_object=EntitySchema)\n",
    "    \n",
    "    # Prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "    You are the first agent in a multi-step workflow to build a Knowledge Graph from raw text.\n",
    "\n",
    "    Workflow Steps Overview:\n",
    "    1. Extract high-level entity types and their properties from the text. [CURRENT STEP]\n",
    "    2. Extract specific instances of entities and their properties based on the identified types.\n",
    "    3. Deduplicate extracted instances and assign them unique identifiers.\n",
    "    4. Identify and define relationships between the instances of entities.\n",
    "    5. Create a structured knowledge graph using the extracted entities and relationships.\n",
    "\n",
    "    You are the FIRST agent in this workflow.\n",
    "\n",
    "\n",
    "    YOUR TASK:\n",
    "    - Identify high-level, general entity types (e.g., Person, Company, Location, Event).\n",
    "    - For each entity type, list all the possible (available) properties it might have.\n",
    "    - Focus on information that would be useful for structuring a knowledge graph.\n",
    "    - Stay general — do not extract specific names, examples, or relationships.\n",
    "    - Avoid unnecessary details or context-specific examples.\n",
    "\n",
    "    FORMAT:\n",
    "    - Return a valid JSON object.\n",
    "    - Keys = entity types (strings).\n",
    "    - Values = lists of property names (strings).\n",
    "    - Use double quotes for all keys and string values.\n",
    "    - No extra explanation, text, or markdown formatting.\n",
    "\n",
    "    EXAMPLES:\n",
    "    {{\n",
    "        \"Person\": [\"name\", \"age\", \"email\", \"address\"],\n",
    "        \"Company\": [\"name\", \"industry\", \"founded_date\"],\n",
    "        \"Location\": [\"name\", \"coordinates\", \"population\"]\n",
    "    }}\n",
    "\n",
    "    Text to process: {input}\n",
    "\n",
    "    {format_instructions}\n",
    "\n",
    "    Response:\n",
    "    \"\"\",\n",
    "        input_variables=[\"input\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Build the chain\n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    return chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88c9b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract entities from text with retry logic\n",
    "def extract_entity_schema(text, max_retries=3):\n",
    "    \"\"\"\n",
    "    Extract entity types and their properties from input text with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to analyze\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping entity types to lists of properties\n",
    "    \"\"\"\n",
    "    chain = create_entity_extraction_chain()\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = chain.invoke({\"input\": text})\n",
    "            # The result is the entities dictionary from the Pydantic model\n",
    "            return result.get(\"entities\", {})\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Attempt {attempt + 1} failed. Retrying... Error: {str(e)[:100]}...\")\n",
    "            else:\n",
    "                print(f\"All {max_retries} attempts failed. Last error: {str(e)[:100]}...\")\n",
    "                # Return empty dict as fallback\n",
    "                return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bfa82a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Person': ['name', 'age', 'occupation', 'employer', 'education', 'residence', 'email', 'tenure'], 'Company': ['name', 'location', 'founded_date', 'industry', 'specialization'], 'Location': ['name'], 'Educational Institution': ['name'], 'Field of Study': ['name']}\n",
      "Extracted Entity Schema:\n",
      "\n",
      "Person:\n",
      "- name\n",
      "- age\n",
      "- occupation\n",
      "- employer\n",
      "- education\n",
      "- residence\n",
      "- email\n",
      "- tenure\n",
      "\n",
      "Company:\n",
      "- name\n",
      "- location\n",
      "- founded_date\n",
      "- industry\n",
      "- specialization\n",
      "\n",
      "Location:\n",
      "- name\n",
      "\n",
      "Educational Institution:\n",
      "- name\n",
      "\n",
      "Field of Study:\n",
      "- name\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "John Doe, a 35-year-old software engineer, works at Google in Mountain View.\n",
    "He graduated from MIT with a degree in Computer Science and has been with the company for 5 years.\n",
    "Google, founded in 1998, is a technology company specializing in internet services and products.\n",
    "John lives in San Francisco and commutes to work daily. His email is john.doe@example.com.\n",
    "\"\"\"\n",
    "\n",
    "entities = extract_entity_schema(sample_text)\n",
    "print(entities)\n",
    "\n",
    "print(\"Extracted Entity Schema:\")\n",
    "for entity_type, properties in entities.items():\n",
    "    print(f\"\\n{entity_type}:\")\n",
    "    for prop in properties:\n",
    "        print(f\"- {prop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98d51aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Entity Schema:\n",
      "\n",
      "Computing Concept:\n",
      "- definition\n",
      "- characteristics\n",
      "- use_cases\n",
      "- limitations\n",
      "- aspects\n",
      "- related_concepts\n",
      "\n",
      "System Architecture:\n",
      "- description\n",
      "- characteristics\n",
      "- components\n",
      "- use_cases\n",
      "- comparison_aspects\n",
      "\n",
      "Platform:\n",
      "- overview\n",
      "- purpose\n",
      "- architecture\n",
      "- components\n",
      "- service_offerings\n",
      "- deployment_aspects\n",
      "- management_aspects\n",
      "- security_aspects\n",
      "- scalability_aspects\n",
      "- reliability_aspects\n",
      "- cost_aspects\n",
      "- features\n",
      "\n",
      "Resource:\n",
      "- description\n",
      "- characteristics\n",
      "- management_aspects\n",
      "- lifecycle_aspects\n",
      "- allocation_aspects\n",
      "- pricing_aspects\n",
      "- type\n",
      "\n",
      "Storage Type:\n",
      "- description\n",
      "- characteristics\n",
      "- use_cases\n",
      "- pricing_models\n",
      "- management_aspects\n",
      "\n",
      "Database Type:\n",
      "- description\n",
      "- characteristics\n",
      "- use_cases\n",
      "- management_aspects\n",
      "- migration_aspects\n",
      "\n",
      "Network Entity:\n",
      "- definition\n",
      "- purpose\n",
      "- characteristics\n",
      "- components\n",
      "- management_aspects\n",
      "- security_aspects\n",
      "- type\n",
      "\n",
      "Service Model:\n",
      "- definition\n",
      "- characteristics\n",
      "- responsibility_division\n",
      "\n",
      "Deployment Model:\n",
      "- definition\n",
      "- characteristics\n",
      "- access_scope\n",
      "- security_aspects\n",
      "- control_aspects\n",
      "- flexibility_aspects\n",
      "\n",
      "Role:\n",
      "- description\n",
      "- function\n",
      "- responsibility\n",
      "\n",
      "Organization:\n",
      "- description\n",
      "- role\n",
      "- service_offerings\n",
      "- market_position\n",
      "- infrastructure_aspects\n",
      "\n",
      "Software Component:\n",
      "- description\n",
      "- function\n",
      "- type\n",
      "- characteristics\n",
      "- related_components\n",
      "- management_aspects\n",
      "- security_aspects\n"
     ]
    }
   ],
   "source": [
    "# Extract entities from the loaded PDF document\n",
    "entities = extract_entity_schema(doc)\n",
    "\n",
    "print(\"\\nExtracted Entity Schema:\")\n",
    "for entity_type, properties in entities.items():\n",
    "    print(f\"\\n{entity_type}:\")\n",
    "    for prop in properties:\n",
    "        print(f\"- {prop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56dfdf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Save entity schema for next step\n",
    "with open(\"entity_schema.json\", \"w\") as f:\n",
    "    json.dump({\"entities\": entities}, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ddede4",
   "metadata": {},
   "source": [
    "# Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f41fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into chunks\n",
    "def split_text_into_chunks(text, chunk_size=5000, chunk_overlap=500):\n",
    "    \"\"\"\n",
    "    Split the input text into manageable chunks using RecursiveCharacterTextSplitter.\n",
    "    \n",
    "    Args:\n",
    "        text: The input text to be split\n",
    "        chunk_size: Maximum size of each chunk in characters\n",
    "        chunk_overlap: Overlap between consecutive chunks\n",
    "        \n",
    "    Returns:\n",
    "        list: List of Document objects\n",
    "    \"\"\"\n",
    "    logger.info(f\"Splitting text into chunks (size={chunk_size}, overlap={chunk_overlap})\")\n",
    "    \n",
    "    # Initialize the splitter with paragraph-focused splitting\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Try to split at paragraph boundaries first\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        keep_separator=True,\n",
    "        add_start_index=True  # Add start position metadata\n",
    "    )\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = splitter.create_documents([text])\n",
    "    \n",
    "    # Add chunk index as metadata\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.metadata[\"chunk_id\"] = i\n",
    "        chunk.metadata[\"total_chunks\"] = len(chunks)\n",
    "    \n",
    "    logger.info(f\"Text split into {len(chunks)} chunks\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30ab1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic models for entity instance extraction\n",
    "class EntityInstance(BaseModel):\n",
    "    \"\"\"A single instance of an entity with its properties.\"\"\"\n",
    "    property_values: Dict[str, Any] = Field(\n",
    "        description=\"Dictionary mapping property names to values for this instance\"\n",
    "    )\n",
    "    \n",
    "class EntityInstances(BaseModel):\n",
    "    \"\"\"Instances of a specific entity type.\"\"\"\n",
    "    Entity: str = Field(description=\"The entity type name\")\n",
    "    Instances: List[Dict[str, Any]] = Field(\n",
    "        description=\"List of instances found for this entity type\"\n",
    "    )\n",
    "\n",
    "class ChunkExtractionResult(BaseModel):\n",
    "    \"\"\"Result of entity extraction from a single chunk.\"\"\"\n",
    "    entities: List[EntityInstances] = Field(\n",
    "        description=\"List of entity types and their instances found in this chunk\"\n",
    "    )\n",
    "    chunk_id: int = Field(description=\"ID of the chunk this extraction is from\")\n",
    "    error: Optional[str] = Field(None, description=\"Error message if extraction failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b20783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create entity instance extraction chain\n",
    "def create_entity_instance_extraction_chain():\n",
    "    \"\"\"\n",
    "    Create a chain for extracting entity instances from text chunks.\n",
    "    \n",
    "    Returns:\n",
    "        Chain: A chain that extracts entity instances from text chunks\n",
    "    \"\"\"\n",
    "    # Entity instance extraction result parser\n",
    "    parser = JsonOutputParser(pydantic_object=ChunkExtractionResult)\n",
    "    \n",
    "    # Create prompt template for entity instance extraction\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are part of a multi-step workflow to build a Knowledge Graph from raw text.\n",
    "\n",
    "        Workflow Steps Overview:\n",
    "        1. Extract high-level entity types and their properties from the text. [COMPLETED]\n",
    "        2. Extract specific instances of entities and their properties from text chunks. [CURRENT STEP]\n",
    "        3. Deduplicate extracted instances and assign them unique identifiers.\n",
    "        4. Identify and define relationships between the instances of entities.\n",
    "        5. Create a structured knowledge graph using the extracted entities and relationships.\n",
    "\n",
    "        YOUR TASK:\n",
    "        You are processing a CHUNK of the full text. Focus ONLY on extracting CONCRETE INSTANCES of entities found in this chunk.\n",
    "\n",
    "        GIVEN:\n",
    "        1. A chunk of text\n",
    "        2. A schema of entity types and their possible properties\n",
    "\n",
    "        INSTRUCTIONS:\n",
    "        - Extract ALL instances of the predefined entity types found in this chunk\n",
    "        - For each instance, extract values for as many properties as are mentioned in the text\n",
    "        - Be precise - only extract information explicitly stated in this chunk\n",
    "        - Do NOT make up or infer missing properties\n",
    "        - If a property is not mentioned, omit it from the output (don't include it with null/empty values)\n",
    "\n",
    "        INPUT TEXT CHUNK:\n",
    "        {chunk}\n",
    "\n",
    "        ENTITY TYPES AND THEIR PROPERTIES:\n",
    "        {entity_schema}\n",
    "\n",
    "        FORMAT YOUR RESPONSE AS FOLLOWS:\n",
    "        - Return a valid JSON object\n",
    "        - Include the chunk_id provided with the input\n",
    "        - For each entity type found, include its name and a list of instance objects\n",
    "        - Each instance object should contain only the properties mentioned in this chunk\n",
    "        - Properties not mentioned should be omitted entirely (not included as null/empty)\n",
    "        - If no instances of a particular entity type are found, do not include that entity type\n",
    "\n",
    "        {format_instructions}\n",
    "\n",
    "        EXAMPLE RESPONSE FOR A CHUNK ABOUT PEOPLE AND COMPANIES:\n",
    "        {{\n",
    "        \"entities\": [\n",
    "            {{\n",
    "            \"Entity\": \"Person\",\n",
    "            \"Instances\": [\n",
    "                {{\"name\": \"John Doe\", \"age\": 35, \"email\": \"john@example.com\"}},\n",
    "                {{\"name\": \"Jane Smith\", \"email\": \"jane@example.com\"}}\n",
    "            ]\n",
    "            }},\n",
    "            {{\n",
    "            \"Entity\": \"Company\",\n",
    "            \"Instances\": [\n",
    "                {{\"name\": \"Google\", \"industry\": \"Technology\", \"founded\": 1998}}\n",
    "            ]\n",
    "            }}\n",
    "        ],\n",
    "        \"chunk_id\": 3\n",
    "        }}\n",
    "        Begin your extraction now: \"\"\", \n",
    "    input_variables=[\"chunk\", \"entity_schema\", \"chunk_id\"], \n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}, \n",
    "    )\n",
    "\n",
    "    # Build the chain\n",
    "    chain = prompt | llm | parser\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a08d8f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single chunk with retry logic\n",
    "def process_chunk(inputs, max_retries=3):\n",
    "    \"\"\"\n",
    "    Process a single text chunk to extract entity instances with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Dictionary containing chunk and entity_schema\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        \n",
    "    Returns:\n",
    "        dict: Extraction results\n",
    "    \"\"\"\n",
    "    chunk = inputs[\"chunk\"]\n",
    "    entity_schema = inputs[\"entity_schema\"]\n",
    "    chunk_id = chunk.metadata.get(\"chunk_id\", 0)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chain = create_entity_instance_extraction_chain()\n",
    "            result = chain.invoke({\n",
    "                \"chunk\": chunk.page_content,\n",
    "                \"entity_schema\": json.dumps(entity_schema, indent=2),\n",
    "                \"chunk_id\": chunk_id\n",
    "            })\n",
    "            logger.info(f\"Successfully processed chunk {chunk_id}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                logger.warning(f\"Attempt {attempt + 1} failed for chunk {chunk_id}. Retrying... Error: {str(e)[:100]}...\")\n",
    "            else:\n",
    "                logger.error(f\"All {max_retries} attempts failed for chunk {chunk_id}. Error: {str(e)[:100]}...\")\n",
    "                return {\n",
    "                    \"entities\": [],\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"error\": str(e)[:200]\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2aa65c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to extract entity instances in parallel\n",
    "async def extract_entity_instances(document, entity_schema, max_concurrency=5, chunk_size=5000, chunk_overlap=500):\n",
    "    \"\"\"\n",
    "    Extract entity instances from document chunks in parallel using asyncio.\n",
    "    \n",
    "    Args:\n",
    "        document: The full text document\n",
    "        entity_schema: Dictionary of entity types and their properties\n",
    "        max_concurrency: Maximum number of chunks to process in parallel\n",
    "        chunk_size: Size of each chunk in characters\n",
    "        chunk_overlap: Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        list: List of entity instances extracted from all chunks\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(\"Beginning entity instance extraction (async)\")\n",
    "    \n",
    "    # Split the document into chunks\n",
    "    chunks = split_text_into_chunks(document, chunk_size, chunk_overlap)\n",
    "    logger.info(f\"Document split into {len(chunks)} chunks\")\n",
    "    \n",
    "    # Create a semaphore to limit concurrency\n",
    "    semaphore = asyncio.Semaphore(max_concurrency)\n",
    "    \n",
    "    # Process chunks with limited concurrency\n",
    "    async def process_with_semaphore(chunk):\n",
    "        async with semaphore:\n",
    "            inputs = {\"chunk\": chunk, \"entity_schema\": entity_schema}\n",
    "            return process_chunk(inputs)\n",
    "    \n",
    "    # Create tasks for all chunks\n",
    "    tasks = [process_with_semaphore(chunk) for chunk in chunks]\n",
    "    \n",
    "    # Process chunks and collect results with progress tracking\n",
    "    all_results = []\n",
    "    total_chunks = len(chunks)\n",
    "    completed = 0\n",
    "    \n",
    "    # Use as_completed to process tasks as they finish\n",
    "    for future in asyncio.as_completed(tasks):\n",
    "        result = await future\n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Log progress\n",
    "        completed += 1\n",
    "        logger.info(f\"Completed {completed}/{total_chunks} chunks ({completed/total_chunks*100:.1f}%)\")\n",
    "        \n",
    "        # Save intermediate results\n",
    "        if completed % 10 == 0 or completed == total_chunks:\n",
    "            with open(f\"intermediate_results_{completed}.json\", \"w\") as f:\n",
    "                json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    logger.info(f\"Entity instance extraction completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90debccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to merge all results from chunks\n",
    "def merge_chunk_results(chunk_results):\n",
    "    \"\"\"\n",
    "    Merge the results from all chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunk_results: List of extraction results from chunks\n",
    "        \n",
    "    Returns:\n",
    "        list: Combined list of entity instances\n",
    "    \"\"\"\n",
    "    merged_results = []\n",
    "    \n",
    "    # Check for errors\n",
    "    errors = [r for r in chunk_results if r.get(\"error\")]\n",
    "    if errors:\n",
    "        logger.warning(f\"{len(errors)} chunks had errors during processing\")\n",
    "    \n",
    "    # Group by entity type\n",
    "    entity_instances = {}\n",
    "    for result in chunk_results:\n",
    "        if \"entities\" not in result:\n",
    "            continue\n",
    "            \n",
    "        for entity_data in result[\"entities\"]:\n",
    "            entity_type = entity_data.get(\"Entity\")\n",
    "            instances = entity_data.get(\"Instances\", [])\n",
    "            \n",
    "            if entity_type not in entity_instances:\n",
    "                entity_instances[entity_type] = []\n",
    "                \n",
    "            entity_instances[entity_type].extend(instances)\n",
    "    \n",
    "    # Convert to the expected format\n",
    "    for entity_type, instances in entity_instances.items():\n",
    "        merged_results.append({\n",
    "            \"Entity\": entity_type,\n",
    "            \"Instances\": instances\n",
    "        })\n",
    "    \n",
    "    logger.info(f\"Merged results for {len(entity_instances)} entity types\")\n",
    "    return merged_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae3b83c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Entity Schema:\n",
      "\n",
      "Computing Concept:\n",
      "  Properties: definition, characteristics, use_cases, limitations, aspects, related_concepts\n",
      "\n",
      "System Architecture:\n",
      "  Properties: description, characteristics, components, use_cases, comparison_aspects\n",
      "\n",
      "Platform:\n",
      "  Properties: overview, purpose, architecture, components, service_offerings, deployment_aspects, management_aspects, security_aspects, scalability_aspects, reliability_aspects, cost_aspects, features\n",
      "\n",
      "Resource:\n",
      "  Properties: description, characteristics, management_aspects, lifecycle_aspects, allocation_aspects, pricing_aspects, type\n",
      "\n",
      "Storage Type:\n",
      "  Properties: description, characteristics, use_cases, pricing_models, management_aspects\n",
      "\n",
      "Database Type:\n",
      "  Properties: description, characteristics, use_cases, management_aspects, migration_aspects\n",
      "\n",
      "Network Entity:\n",
      "  Properties: definition, purpose, characteristics, components, management_aspects, security_aspects, type\n",
      "\n",
      "Service Model:\n",
      "  Properties: definition, characteristics, responsibility_division\n",
      "\n",
      "Deployment Model:\n",
      "  Properties: definition, characteristics, access_scope, security_aspects, control_aspects, flexibility_aspects\n",
      "\n",
      "Role:\n",
      "  Properties: description, function, responsibility\n",
      "\n",
      "Organization:\n",
      "  Properties: description, role, service_offerings, market_position, infrastructure_aspects\n",
      "\n",
      "Software Component:\n",
      "  Properties: description, function, type, characteristics, related_components, management_aspects, security_aspects\n"
     ]
    }
   ],
   "source": [
    "# Load the entity schema from the previous step\n",
    "with open(\"entity_schema.json\", \"r\") as f:\n",
    "    entity_schema_data = json.load(f)\n",
    "    entity_schema = entity_schema_data.get(\"entities\", {})\n",
    "\n",
    "# Print the entity schema\n",
    "print(\"Using Entity Schema:\")\n",
    "for entity_type, properties in entity_schema.items():\n",
    "    print(f\"\\n{entity_type}:\")\n",
    "    print(f\"  Properties: {', '.join(properties)}\")\n",
    "\n",
    "# Execute entity instance extraction\n",
    "async def run_entity_extraction():\n",
    "    # Extract entity instances from chunks\n",
    "    chunk_results = await extract_entity_instances(\n",
    "        document=doc,\n",
    "        entity_schema=entity_schema,\n",
    "        max_concurrency=5,\n",
    "        chunk_size=5000,\n",
    "        chunk_overlap=500\n",
    "    )\n",
    "    \n",
    "    # Merge results from all chunks\n",
    "    merged_results = merge_chunk_results(chunk_results)\n",
    "    \n",
    "    # Save the merged results for the next step\n",
    "    with open(\"entity_instances_raw.json\", \"w\") as f:\n",
    "        json.dump(merged_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nExtracted {sum(len(entity['Instances']) for entity in merged_results)} instances across {len(merged_results)} entity types\")\n",
    "    \n",
    "    # Print sample results\n",
    "    print(\"\\nSample instances:\")\n",
    "    for entity in merged_results:\n",
    "        entity_type = entity[\"Entity\"]\n",
    "        instances = entity[\"Instances\"]\n",
    "        print(f\"\\n{entity_type} ({len(instances)} instances):\")\n",
    "        for i, instance in enumerate(instances[:3]):  # Show up to 3 instances per type\n",
    "            print(f\"  Instance {i+1}: {instance}\")\n",
    "        if len(instances) > 3:\n",
    "            print(f\"  ... and {len(instances) - 3} more\")\n",
    "    \n",
    "    return merged_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5fdc3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 12:22:37,679 - INFO - Beginning entity instance extraction (async)\n",
      "2025-04-27 12:22:37,680 - INFO - Splitting text into chunks (size=5000, overlap=500)\n",
      "2025-04-27 12:22:37,686 - INFO - Text split into 11 chunks\n",
      "2025-04-27 12:22:37,687 - INFO - Document split into 11 chunks\n",
      "2025-04-27 12:23:18,572 - INFO - Successfully processed chunk 2\n",
      "2025-04-27 12:24:08,679 - INFO - Successfully processed chunk 7\n",
      "2025-04-27 12:25:21,623 - INFO - Successfully processed chunk 8\n",
      "2025-04-27 12:26:25,472 - INFO - Successfully processed chunk 9\n",
      "2025-04-27 12:26:55,200 - INFO - Successfully processed chunk 1\n",
      "2025-04-27 12:27:30,953 - INFO - Successfully processed chunk 0\n",
      "2025-04-27 12:28:33,783 - INFO - Successfully processed chunk 4\n",
      "2025-04-27 12:29:44,732 - INFO - Successfully processed chunk 3\n",
      "2025-04-27 12:30:20,885 - INFO - Successfully processed chunk 5\n",
      "2025-04-27 12:31:02,915 - INFO - Successfully processed chunk 6\n",
      "2025-04-27 12:32:08,318 - INFO - Successfully processed chunk 10\n",
      "2025-04-27 12:32:08,319 - INFO - Completed 1/11 chunks (9.1%)\n",
      "2025-04-27 12:32:08,320 - INFO - Completed 2/11 chunks (18.2%)\n",
      "2025-04-27 12:32:08,321 - INFO - Completed 3/11 chunks (27.3%)\n",
      "2025-04-27 12:32:08,322 - INFO - Completed 4/11 chunks (36.4%)\n",
      "2025-04-27 12:32:08,323 - INFO - Completed 5/11 chunks (45.5%)\n",
      "2025-04-27 12:32:08,324 - INFO - Completed 6/11 chunks (54.5%)\n",
      "2025-04-27 12:32:08,325 - INFO - Completed 7/11 chunks (63.6%)\n",
      "2025-04-27 12:32:08,325 - INFO - Completed 8/11 chunks (72.7%)\n",
      "2025-04-27 12:32:08,326 - INFO - Completed 9/11 chunks (81.8%)\n",
      "2025-04-27 12:32:08,327 - INFO - Completed 10/11 chunks (90.9%)\n",
      "2025-04-27 12:32:08,351 - INFO - Completed 11/11 chunks (100.0%)\n",
      "2025-04-27 12:32:08,371 - INFO - Entity instance extraction completed in 570.69 seconds\n",
      "2025-04-27 12:32:08,372 - INFO - Merged results for 12 entity types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted 743 instances across 12 entity types\n",
      "\n",
      "Sample instances:\n",
      "\n",
      "Computing Concept (248 instances):\n",
      "  Instance 1: {'characteristics': 'prefers symmetric resources (same type of OS, servers, configurations, etc.)', 'aspects': 'simplifies management and performance', 'related_concepts': ['symmetric resources', 'Asymmetric resources']}\n",
      "  Instance 2: {'definition': 'allows creation of virtual objects from physical resources', 'aspects': 'not cloud computing, but it is a key enabler', 'characteristics': 'Supports resource pooling', 'related_concepts': ['resource pooling', 'elasticity', 'workload migration']}\n",
      "  Instance 3: {'related_concepts': ['Virtualization']}\n",
      "  ... and 245 more\n",
      "\n",
      "System Architecture (37 instances):\n",
      "  Instance 1: {'description': 'A design approach used for cloud services', 'related_concepts': ['Service Orientation']}\n",
      "  Instance 2: {'characteristics': 'Multiple users can use AWS, each assigned their own VPC'}\n",
      "  Instance 3: {'description': 'VPC → Virtual Machines (VMs) → Operating System (OS) → Platform → Applications'}\n",
      "  ... and 34 more\n",
      "\n",
      "Platform (37 instances):\n",
      "  Instance 1: {'related_concepts': ['Infrastructure as a Service (IaaS)']}\n",
      "  Instance 2: {'related_concepts': ['Platform as a Service (PaaS)']}\n",
      "  Instance 3: {'role': 'cloud provider', 'infrastructure_aspects': ['Global Infrastructure', 'data centers distributed worldwide', 'data centers located near sea ports for high-speed fiber-optic connectivity'], 'service_offerings': ['Compute Services (EC2)', 'Network Services', 'Identity & Access Management (IAM)', 'Storage Services']}\n",
      "  ... and 34 more\n",
      "\n",
      "Resource (114 instances):\n",
      "  Instance 1: {'characteristics': 'same type of OS, servers, configurations, etc.', 'management_aspects': 'simplifies management and performance', 'related_concepts': ['Homogeneity']}\n",
      "  Instance 2: {'characteristics': 'can be used', 'related_concepts': ['Homogeneity']}\n",
      "  Instance 3: {'characteristics': 'lead to performance degradation due to compatibility challenges', 'related_concepts': ['Homogeneity']}\n",
      "  ... and 111 more\n",
      "\n",
      "Storage Type (19 instances):\n",
      "  Instance 1: {'related_concepts': ['Service Orientation']}\n",
      "  Instance 2: {'description': 'Uses AWS Storage Servers for backups', 'use_cases': ['backups']}\n",
      "  Instance 3: {'name': 'Storage Service'}\n",
      "  ... and 16 more\n",
      "\n",
      "Network Entity (67 instances):\n",
      "  Instance 1: {'related_concepts': ['Service Orientation', 'IaaS']}\n",
      "  Instance 2: {'definition': 'A distributed network of cloud instances that store and distribute content to users efficiently', 'purpose': 'store and distribute content to users efficiently', 'components': 'A distributed network of cloud instances', 'related_concepts': ['Geographical Distribution']}\n",
      "  Instance 3: {'type': 'high-speed'}\n",
      "  ... and 64 more\n",
      "\n",
      "Service Model (26 instances):\n",
      "  Instance 1: {'definition': 'define: How users access services, How operators manage resources, Responsibility division between users and cloud service providers (CSPs)', 'characteristics': 'define how users access services'}\n",
      "  Instance 2: {'responsibility_division': 'Users manage: Applications, Operating Systems, Virtual Machines (VMs), Servers, Networking', 'characteristics': 'Users manage: Applications'}\n",
      "  Instance 3: {'responsibility_division': 'Users interact with applications but do not manage the underlying platform', 'characteristics': 'Users interact with applications but do not manage the underlying platform'}\n",
      "  ... and 23 more\n",
      "\n",
      "Deployment Model (29 instances):\n",
      "  Instance 1: {'definition': 'define how cloud infrastructure is set up, who can access it, and its intended use', 'characteristics': 'define how cloud infrastructure is set up'}\n",
      "  Instance 2: {'definition': 'Large-scale cloud infrastructure available to the general public', 'access_scope': 'available to the general public', 'security_aspects': 'security can be a concern'}\n",
      "  Instance 3: {'definition': 'Cloud infrastructure dedicated to a single organization', 'characteristics': 'offering better control and security', 'control_aspects': 'More control', 'security_aspects': 'More... security', 'access_scope': 'Restricted to a specific group (e.g., universities, research institutes)'}\n",
      "  ... and 26 more\n",
      "\n",
      "Role (27 instances):\n",
      "  Instance 1: {'responsibility': 'manages what in cloud-based systems', 'related_concepts': ['Responsibility Model', 'Cloud Service Model', 'IaaS', 'PaaS', 'SaaS']}\n",
      "  Instance 2: {'responsibility': 'manages what in cloud-based systems', 'related_concepts': ['Responsibility Model', 'Cloud Service Provider (CSP)', 'Cloud Service Model', 'IaaS', 'SaaS', 'Anything as a Service (XaaS)']}\n",
      "  Instance 3: {'responsibility': 'manages the platform', 'related_concepts': ['Cloud Service Model', 'PaaS', 'SaaS']}\n",
      "  ... and 24 more\n",
      "\n",
      "Organization (28 instances):\n",
      "  Instance 1: {'related_concepts': ['Community Cloud']}\n",
      "  Instance 2: {'related_concepts': ['Community Cloud']}\n",
      "  Instance 3: {'name': 'AWS'}\n",
      "  ... and 25 more\n",
      "\n",
      "Software Component (105 instances):\n",
      "  Instance 1: {'management_aspects': 'Users manage', 'related_components': ['Operating Systems', 'Virtual Machines (VMs)', 'Servers', 'Networking'], 'related_concepts': ['IaaS', 'PaaS', 'SaaS']}\n",
      "  Instance 2: {'management_aspects': 'Users manage', 'related_components': ['Applications', 'Virtual Machines (VMs)', 'Servers', 'Networking'], 'related_concepts': ['IaaS']}\n",
      "  Instance 3: {'related_concepts': ['Service Orientation']}\n",
      "  ... and 102 more\n",
      "\n",
      "Database Type (6 instances):\n",
      "  Instance 1: {'name': 'Relational Databases (RDS)', 'characteristics': ['Structured storage', 'with SQL support']}\n",
      "  Instance 2: {'name': 'Non-Relational Databases (NoSQL)', 'characteristics': ['Schema-less', 'scalable databases']}\n",
      "  Instance 3: {'name': 'Database Migration Services', 'use_cases': 'migrating data between databases'}\n",
      "  ... and 3 more\n"
     ]
    }
   ],
   "source": [
    "# Run the extraction\n",
    "entity_instances = await run_entity_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaad4a7",
   "metadata": {},
   "source": [
    "# Step 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16454cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_instances_parallel(document, entity_schema, max_concurrency=6):\n",
    "    \"\"\"\n",
    "    Extract entity instances from document chunks in parallel using RunnableParallel.\n",
    "    \n",
    "    Args:\n",
    "        document: The full text document\n",
    "        entity_schema: Dictionary of entity types and their properties\n",
    "        max_concurrency: Maximum number of chunks to process in parallel\n",
    "        \n",
    "    Returns:\n",
    "        list: List of entity instances extracted from all chunks\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(\"Beginning entity instance extraction (parallel)\")\n",
    "    \n",
    "    # Split the document into chunks\n",
    "    chunks = split_text_into_chunks(document, chunk_size=5000, chunk_overlap=500)\n",
    "    logger.info(f\"Document split into {len(chunks)} chunks\")\n",
    "    \n",
    "    # Prepare inputs for each chunk\n",
    "    inputs = [{\"chunk\": chunk, \"entity_schema\": entity_schema} for chunk in chunks]\n",
    "    \n",
    "    # Create a RunnableLambda for chunk processing\n",
    "    chunk_processor = RunnableLambda(process_chunk)\n",
    "    \n",
    "    # Process chunks in batches with progress tracking\n",
    "    all_results = []\n",
    "    batch_size = min(max_concurrency, len(chunks))\n",
    "    \n",
    "    # Use tqdm for progress tracking in batches\n",
    "    with tqdm(total=len(chunks), desc=\"Processing chunks\") as progress_bar:\n",
    "        for i in range(0, len(inputs), batch_size):\n",
    "            batch_inputs = inputs[i:i+batch_size]\n",
    "            \n",
    "            # Process the batch in parallel\n",
    "            batch_results = chunk_processor.batch(batch_inputs, config={\"max_concurrency\": max_concurrency})\n",
    "            all_results.extend(batch_results)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.update(len(batch_inputs))\n",
    "            \n",
    "            # Save intermediate results\n",
    "            if i + batch_size >= len(inputs) or (i > 0 and i % 20 == 0):\n",
    "                with open(f\"intermediate_results_{i + len(batch_inputs)}.json\", \"w\") as f:\n",
    "                    json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    logger.info(f\"Entity instance extraction completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73fbd50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_chunk_results(chunk_results):\n",
    "    \"\"\"\n",
    "    Merge the results from all chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunk_results: List of extraction results from chunks\n",
    "        \n",
    "    Returns:\n",
    "        list: Combined list of entity instances\n",
    "    \"\"\"\n",
    "    merged_results = []\n",
    "    \n",
    "    # Check for errors\n",
    "    errors = [r for r in chunk_results if r.get(\"error\")]\n",
    "    if errors:\n",
    "        logger.warning(f\"{len(errors)} chunks had errors during processing\")\n",
    "    \n",
    "    # Group by entity type\n",
    "    entity_instances = {}\n",
    "    for result in chunk_results:\n",
    "        if \"entities\" not in result:\n",
    "            continue\n",
    "            \n",
    "        for entity_data in result[\"entities\"]:\n",
    "            entity_type = entity_data.get(\"Entity\")\n",
    "            instances = entity_data.get(\"Instances\", [])\n",
    "            \n",
    "            if entity_type not in entity_instances:\n",
    "                entity_instances[entity_type] = []\n",
    "                \n",
    "            entity_instances[entity_type].extend(instances)\n",
    "    \n",
    "    # Convert to the expected format\n",
    "    for entity_type, instances in entity_instances.items():\n",
    "        merged_results.append({\n",
    "            \"Entity\": entity_type,\n",
    "            \"Instances\": instances\n",
    "        })\n",
    "    \n",
    "    logger.info(f\"Merged results for {len(entity_instances)} entity types\")\n",
    "    return merged_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99bbe428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 13:41:33,107 - INFO - Beginning entity instance extraction (parallel)\n",
      "2025-04-27 13:41:33,108 - INFO - Splitting text into chunks (size=5000, overlap=500)\n",
      "2025-04-27 13:41:33,114 - INFO - Text split into 11 chunks\n",
      "2025-04-27 13:41:33,115 - INFO - Document split into 11 chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23eb62dc8ae47b190b75f4a692b3e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chunks:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 13:42:04,279 - INFO - Successfully processed chunk 1\n",
      "2025-04-27 13:42:11,523 - INFO - Successfully processed chunk 0\n",
      "2025-04-27 13:42:12,641 - INFO - Successfully processed chunk 5\n",
      "2025-04-27 13:42:18,263 - INFO - Successfully processed chunk 2\n",
      "2025-04-27 13:42:37,020 - INFO - Successfully processed chunk 4\n",
      "2025-04-27 13:42:44,575 - INFO - Successfully processed chunk 3\n",
      "2025-04-27 13:43:24,298 - INFO - Successfully processed chunk 6\n",
      "2025-04-27 13:43:35,297 - INFO - Successfully processed chunk 7\n",
      "2025-04-27 13:43:43,510 - INFO - Successfully processed chunk 9\n",
      "2025-04-27 13:43:53,806 - INFO - Successfully processed chunk 10\n",
      "2025-04-27 13:44:00,099 - INFO - Successfully processed chunk 8\n",
      "2025-04-27 13:44:00,163 - INFO - Entity instance extraction completed in 147.06 seconds\n",
      "2025-04-27 13:44:00,164 - INFO - Merged results for 12 entity types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted 743 instances across 12 entity types\n",
      "\n",
      "Sample instances:\n",
      "\n",
      "Computing Concept (248 instances):\n",
      "  Instance 1: {'definition': 'involves executing multiple processes simultaneously to enhance speed and efficiency', 'use_cases': ['Vector processing', 'Image processing', 'Matrix multiplication'], 'limitations': ['Not all applications can be parallelized', 'Some components of code can be executed in parallel, while others may not', 'Specific programming languages are required for parallel computing']}\n",
      "  Instance 2: {}\n",
      "  Instance 3: {}\n",
      "  ... and 245 more\n",
      "\n",
      "System Architecture (37 instances):\n",
      "  Instance 1: {'definition': 'a system where computing resources are distributed across multiple locations rather than being centralized in a single system', 'characteristics': ['allow for hardware scaling']}\n",
      "  Instance 2: {}\n",
      "  Instance 3: {'use_cases': ['example of distributed computing']}\n",
      "  ... and 34 more\n",
      "\n",
      "Resource (114 instances):\n",
      "  Instance 1: {}\n",
      "  Instance 2: {}\n",
      "  Instance 3: {}\n",
      "  ... and 111 more\n",
      "\n",
      "Storage Type (19 instances):\n",
      "  Instance 1: {}\n",
      "  Instance 2: {'related_concepts': ['Service Orientation']}\n",
      "  Instance 3: {'description': 'Combines multiple storage disks into one logical storage unit', 'characteristics': ['Provides redundancy for fault tolerance']}\n",
      "  ... and 16 more\n",
      "\n",
      "Network Entity (67 instances):\n",
      "  Instance 1: {'type': 'Local Area Networks'}\n",
      "  Instance 2: {}\n",
      "  Instance 3: {}\n",
      "  ... and 64 more\n",
      "\n",
      "Service Model (26 instances):\n",
      "  Instance 1: {}\n",
      "  Instance 2: {'definition': 'Users pay only for what they consume'}\n",
      "  Instance 3: {}\n",
      "  ... and 23 more\n",
      "\n",
      "Role (27 instances):\n",
      "  Instance 1: {}\n",
      "  Instance 2: {}\n",
      "  Instance 3: {}\n",
      "  ... and 24 more\n",
      "\n",
      "Organization (28 instances):\n",
      "  Instance 1: {}\n",
      "  Instance 2: {}\n",
      "  Instance 3: {'role': 'Cloud provider', 'service_offerings': ['offer HPC as a service']}\n",
      "  ... and 25 more\n",
      "\n",
      "Software Component (105 instances):\n",
      "  Instance 1: {'description': 'The software layer that facilitates communication between different system components', 'function': ['Acts as a bridge between applications and networked devices', 'Provides a unified interface for interacting with connected components']}\n",
      "  Instance 2: {}\n",
      "  Instance 3: {}\n",
      "  ... and 102 more\n",
      "\n",
      "Platform (37 instances):\n",
      "  Instance 1: {'features': 'provides rapid elasticity'}\n",
      "  Instance 2: {'related_concepts': ['Infrastructure as a Service (IaaS)']}\n",
      "  Instance 3: {'related_concepts': ['Platform as a Service (PaaS)']}\n",
      "  ... and 34 more\n",
      "\n",
      "Deployment Model (29 instances):\n",
      "  Instance 1: {'definition': 'define how cloud infrastructure is set up, who can access it, and its intended use', 'characteristics': 'define how cloud infrastructure is set up'}\n",
      "  Instance 2: {'definition': 'Large-scale cloud infrastructure available to the general public', 'access_scope': 'available to the general public', 'security_aspects': 'security can be a concern'}\n",
      "  Instance 3: {'definition': 'Cloud infrastructure dedicated to a single organization', 'characteristics': 'offering better control and security', 'control_aspects': 'More control', 'security_aspects': 'More... security', 'access_scope': 'Restricted to a specific group (e.g., universities, research institutes)'}\n",
      "  ... and 26 more\n",
      "\n",
      "Database Type (6 instances):\n",
      "  Instance 1: {'name': 'Relational Databases (RDS)', 'characteristics': ['Structured storage', 'with SQL support']}\n",
      "  Instance 2: {'name': 'Non-Relational Databases (NoSQL)', 'characteristics': ['Schema-less', 'scalable databases']}\n",
      "  Instance 3: {'name': 'Database Migration Services', 'use_cases': 'migrating data between databases'}\n",
      "  ... and 3 more\n"
     ]
    }
   ],
   "source": [
    "# Load the entity schema\n",
    "with open(\"entity_schema.json\", \"r\") as f:\n",
    "    entity_schema_data = json.load(f)\n",
    "    entity_schema = entity_schema_data.get(\"entities\", {})\n",
    "\n",
    "\n",
    "# Extract entity instances using parallel processing\n",
    "chunk_results = extract_entity_instances_parallel(\n",
    "    document=doc,\n",
    "    entity_schema=entity_schema,\n",
    "    max_concurrency=6  # Process 6 chunks in parallel\n",
    ")\n",
    "\n",
    "# Merge results from all chunks\n",
    "merged_results = merge_chunk_results(chunk_results)\n",
    "\n",
    "# Save the merged results for the next step\n",
    "with open(\"entity_instances_raw.json\", \"w\") as f:\n",
    "    json.dump(merged_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nExtracted {sum(len(entity['Instances']) for entity in merged_results)} instances across {len(merged_results)} entity types\")\n",
    "\n",
    "# Print sample results\n",
    "print(\"\\nSample instances:\")\n",
    "for entity in merged_results:\n",
    "    entity_type = entity[\"Entity\"]\n",
    "    instances = entity[\"Instances\"]\n",
    "    print(f\"\\n{entity_type} ({len(instances)} instances):\")\n",
    "    for i, instance in enumerate(instances[:3]):  # Show up to 3 instances per type\n",
    "        print(f\"  Instance {i+1}: {instance}\")\n",
    "    if len(instances) > 3:\n",
    "        print(f\"  ... and {len(instances) - 3} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f65c16f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
