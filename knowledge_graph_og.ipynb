{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4dad227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Dict, List, Any, Optional\n",
    "from collections import Counter\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import uuid\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from aperturedb import Connector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f36d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7aada5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True) # Add override=True\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")  \n",
    "db_host = os.getenv(\"APERTUREDB_HOST\")\n",
    "db_password = os.getenv(\"APERTUREDB_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4884710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-preview-04-17\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d43fffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_content(pdf_path, return_single_string=True, extract_metadata=False):\n",
    "    \"\"\"\n",
    "    Load and parse a PDF document, returning its text content.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        return_single_string (bool): If True, returns the entire PDF content as a single string.\n",
    "                                    If False, returns a list of strings (one per page).\n",
    "        extract_metadata (bool): If True, returns metadata along with content\n",
    "    \n",
    "    Returns:\n",
    "        If return_single_string is True and extract_metadata is False:\n",
    "            str: The entire text content of the PDF\n",
    "        If return_single_string is False and extract_metadata is False:\n",
    "            list: List of strings, one for each page\n",
    "        If extract_metadata is True:\n",
    "            tuple: (content, metadata) where content is either a string or list based on return_single_string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"PDF file not found at: {pdf_path}\")\n",
    "    \n",
    "    # Initialize the loader with the appropriate mode\n",
    "    mode = \"single\" if return_single_string else \"elements\"\n",
    "    loader = PyPDFLoader(pdf_path, mode=mode)\n",
    "    \n",
    "    # Load the documents\n",
    "    docs = loader.load()\n",
    "    \n",
    "    if return_single_string:\n",
    "        # With mode=\"single\", there should only be one document containing all pages\n",
    "        content = docs[0].page_content if docs else \"\"\n",
    "        metadata = docs[0].metadata if docs else {}\n",
    "    else:\n",
    "        # With default mode, each document is a page\n",
    "        content = [doc.page_content for doc in docs]\n",
    "        metadata = [doc.metadata for doc in docs]\n",
    "    \n",
    "    if extract_metadata:\n",
    "        return content, metadata\n",
    "    else:\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f714bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = load_pdf_content(\"Cloud Computing Copy Lecture Notes.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28f5c303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloud Computing Lecture Notes \n",
      "Distributed Computing/Systems \n",
      "Definition: \n",
      "Distributed computing refers to a system where computing resources are distributed \n",
      "across multiple locations rather than being centralized in a single system. This enables \n",
      "task distribution and efficient resource utilization. \n",
      "Why Use Distributed Systems? \n",
      "• Scalability Issues: Traditional computing faces bottlenecks due to hardware \n",
      "limitations, whereas distributed systems allow for hardware scaling. \n",
      "• Connected Devices: In a networked system, connected devices communicate, but \n",
      "this does not necessarily make them distributed. \n",
      "• IoT (Internet of Things): IoT is one of the largest examples of distributed computing. \n",
      "• Multi-layered System Design: Distributed computing enables systems to function \n",
      "in multiple layers, with each layer acting as a distributed entity. \n",
      "• User Perspective: Although the system consists of multiple machines, distributed \n",
      "computing presents a unified system to users. \n",
      " \n",
      "Parallel Comp\n"
     ]
    }
   ],
   "source": [
    "print(doc[:1000])  # Print the first 1000 characters of the loaded document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63716dec",
   "metadata": {},
   "source": [
    "# Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37f14e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic model for entity schema parser\n",
    "class EntitySchema(BaseModel):\n",
    "    \"\"\"Entity types and their properties.\"\"\"\n",
    "    entities: Dict[str, List[str]] = Field(\n",
    "        description=\"Dictionary mapping entity types to their possible properties\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8025570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entity extraction chain\n",
    "def create_entity_extraction_chain():\n",
    "    parser = JsonOutputParser(pydantic_object=EntitySchema)\n",
    "    \n",
    "    # Prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "    You are the first agent in a multi-step workflow to build a Knowledge Graph from raw text.\n",
    "\n",
    "    Workflow Steps Overview:\n",
    "    1. Extract high-level entity types and their properties from the text. [CURRENT STEP]\n",
    "    2. Extract specific instances of entities and their properties based on the identified types.\n",
    "    3. Deduplicate extracted instances and assign them unique identifiers.\n",
    "    4. Identify and define relationships between the instances of entities.\n",
    "    5. Create a structured knowledge graph using the extracted entities and relationships.\n",
    "\n",
    "    You are the FIRST agent in this workflow.\n",
    "\n",
    "\n",
    "    YOUR TASK:\n",
    "    - Identify high-level, general entity types (e.g., Person, Company, Location, Event).\n",
    "    - For each entity type, list all the possible (available) properties it might have.\n",
    "    - Focus on information that would be useful for structuring a knowledge graph.\n",
    "    - Stay general — do not extract specific names, examples, or relationships.\n",
    "    - Avoid unnecessary details or context-specific examples.\n",
    "\n",
    "    FORMAT:\n",
    "    - Return a valid JSON object.\n",
    "    - Keys = entity types (strings).\n",
    "    - Values = lists of property names (strings).\n",
    "    - Use double quotes for all keys and string values.\n",
    "    - No extra explanation, text, or markdown formatting.\n",
    "\n",
    "    EXAMPLES:\n",
    "    {{\n",
    "        \"Person\": [\"name\", \"age\", \"email\", \"address\"],\n",
    "        \"Company\": [\"name\", \"industry\", \"founded_date\"],\n",
    "        \"Location\": [\"name\", \"coordinates\", \"population\"]\n",
    "    }}\n",
    "\n",
    "    Text to process: {input}\n",
    "\n",
    "    {format_instructions}\n",
    "\n",
    "    Response:\n",
    "    \"\"\",\n",
    "        input_variables=[\"input\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Build the chain\n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    return chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88c9b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract entities from text with retry logic\n",
    "def extract_entity_schema(text, max_retries=3):\n",
    "    \"\"\"\n",
    "    Extract entity types and their properties from input text with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to analyze\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping entity types to lists of properties\n",
    "    \"\"\"\n",
    "    chain = create_entity_extraction_chain()\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = chain.invoke({\"input\": text})\n",
    "            # The result is the entities dictionary from the Pydantic model\n",
    "            return result.get(\"entities\", {})\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Attempt {attempt + 1} failed. Retrying... Error: {str(e)[:100]}...\")\n",
    "            else:\n",
    "                print(f\"All {max_retries} attempts failed. Last error: {str(e)[:100]}...\")\n",
    "                # Return empty dict as fallback\n",
    "                return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bfa82a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Person': ['name', 'age', 'occupation', 'employer', 'education', 'residence', 'email', 'tenure'], 'Company': ['name', 'location', 'founded_date', 'industry', 'specialization'], 'Location': ['name'], 'Educational Institution': ['name'], 'Field of Study': ['name']}\n",
      "Extracted Entity Schema:\n",
      "\n",
      "Person:\n",
      "- name\n",
      "- age\n",
      "- occupation\n",
      "- employer\n",
      "- education\n",
      "- residence\n",
      "- email\n",
      "- tenure\n",
      "\n",
      "Company:\n",
      "- name\n",
      "- location\n",
      "- founded_date\n",
      "- industry\n",
      "- specialization\n",
      "\n",
      "Location:\n",
      "- name\n",
      "\n",
      "Educational Institution:\n",
      "- name\n",
      "\n",
      "Field of Study:\n",
      "- name\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "John Doe, a 35-year-old software engineer, works at Google in Mountain View.\n",
    "He graduated from MIT with a degree in Computer Science and has been with the company for 5 years.\n",
    "Google, founded in 1998, is a technology company specializing in internet services and products.\n",
    "John lives in San Francisco and commutes to work daily. His email is john.doe@example.com.\n",
    "\"\"\"\n",
    "\n",
    "entities = extract_entity_schema(sample_text)\n",
    "print(entities)\n",
    "\n",
    "print(\"Extracted Entity Schema:\")\n",
    "for entity_type, properties in entities.items():\n",
    "    print(f\"\\n{entity_type}:\")\n",
    "    for prop in properties:\n",
    "        print(f\"- {prop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98d51aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Entity Schema:\n",
      "\n",
      "Computing Concept:\n",
      "- definition\n",
      "- characteristics\n",
      "- use_cases\n",
      "- limitations\n",
      "- aspects\n",
      "- related_concepts\n",
      "\n",
      "System Architecture:\n",
      "- description\n",
      "- characteristics\n",
      "- components\n",
      "- use_cases\n",
      "- comparison_aspects\n",
      "\n",
      "Platform:\n",
      "- overview\n",
      "- purpose\n",
      "- architecture\n",
      "- components\n",
      "- service_offerings\n",
      "- deployment_aspects\n",
      "- management_aspects\n",
      "- security_aspects\n",
      "- scalability_aspects\n",
      "- reliability_aspects\n",
      "- cost_aspects\n",
      "- features\n",
      "\n",
      "Resource:\n",
      "- description\n",
      "- characteristics\n",
      "- management_aspects\n",
      "- lifecycle_aspects\n",
      "- allocation_aspects\n",
      "- pricing_aspects\n",
      "- type\n",
      "\n",
      "Storage Type:\n",
      "- description\n",
      "- characteristics\n",
      "- use_cases\n",
      "- pricing_models\n",
      "- management_aspects\n",
      "\n",
      "Database Type:\n",
      "- description\n",
      "- characteristics\n",
      "- use_cases\n",
      "- management_aspects\n",
      "- migration_aspects\n",
      "\n",
      "Network Entity:\n",
      "- definition\n",
      "- purpose\n",
      "- characteristics\n",
      "- components\n",
      "- management_aspects\n",
      "- security_aspects\n",
      "- type\n",
      "\n",
      "Service Model:\n",
      "- definition\n",
      "- characteristics\n",
      "- responsibility_division\n",
      "\n",
      "Deployment Model:\n",
      "- definition\n",
      "- characteristics\n",
      "- access_scope\n",
      "- security_aspects\n",
      "- control_aspects\n",
      "- flexibility_aspects\n",
      "\n",
      "Role:\n",
      "- description\n",
      "- function\n",
      "- responsibility\n",
      "\n",
      "Organization:\n",
      "- description\n",
      "- role\n",
      "- service_offerings\n",
      "- market_position\n",
      "- infrastructure_aspects\n",
      "\n",
      "Software Component:\n",
      "- description\n",
      "- function\n",
      "- type\n",
      "- characteristics\n",
      "- related_components\n",
      "- management_aspects\n",
      "- security_aspects\n"
     ]
    }
   ],
   "source": [
    "# Extract entities from the loaded PDF document\n",
    "entities = extract_entity_schema(doc)\n",
    "\n",
    "print(\"\\nExtracted Entity Schema:\")\n",
    "for entity_type, properties in entities.items():\n",
    "    print(f\"\\n{entity_type}:\")\n",
    "    for prop in properties:\n",
    "        print(f\"- {prop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56dfdf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Save entity schema for next step\n",
    "with open(\"entity_schema.json\", \"w\") as f:\n",
    "    json.dump({\"entities\": entities}, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ddede4",
   "metadata": {},
   "source": [
    "# Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f41fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into chunks\n",
    "def split_text_into_chunks(text, chunk_size=5000, chunk_overlap=500):\n",
    "    \"\"\"\n",
    "    Split the input text into manageable chunks using RecursiveCharacterTextSplitter.\n",
    "    \n",
    "    Args:\n",
    "        text: The input text to be split\n",
    "        chunk_size: Maximum size of each chunk in characters\n",
    "        chunk_overlap: Overlap between consecutive chunks\n",
    "        \n",
    "    Returns:\n",
    "        list: List of Document objects\n",
    "    \"\"\"\n",
    "    logger.info(f\"Splitting text into chunks (size={chunk_size}, overlap={chunk_overlap})\")\n",
    "    \n",
    "    # Initialize the splitter with paragraph-focused splitting\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Try to split at paragraph boundaries first\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        keep_separator=True,\n",
    "        add_start_index=True  # Add start position metadata\n",
    "    )\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = splitter.create_documents([text])\n",
    "    \n",
    "    # Add chunk index as metadata\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.metadata[\"chunk_id\"] = i\n",
    "        chunk.metadata[\"total_chunks\"] = len(chunks)\n",
    "    \n",
    "    logger.info(f\"Text split into {len(chunks)} chunks\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30ab1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic models for entity instance extraction\n",
    "class EntityInstances(BaseModel):\n",
    "    \"\"\"Instances of a specific entity type.\"\"\"\n",
    "    Entity: str = Field(description=\"The entity type name\")\n",
    "    Instances: Dict[str, Dict[str, Any]] = Field(\n",
    "        description=\"Dictionary mapping instance names to their properties\"\n",
    "    )\n",
    "\n",
    "class ChunkExtractionResult(BaseModel):\n",
    "    \"\"\"Result of entity extraction from a single chunk.\"\"\"\n",
    "    entities: List[EntityInstances] = Field(\n",
    "        description=\"List of entity types and their instances found in this chunk\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b20783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create entity instance extraction chain\n",
    "def create_entity_instance_extraction_chain():\n",
    "    \"\"\"\n",
    "    Create a chain for extracting entity instances from text chunks.\n",
    "    \n",
    "    Returns:\n",
    "        Chain: A chain that extracts entity instances from text chunks\n",
    "    \"\"\"\n",
    "    # Entity instance extraction result parser\n",
    "    parser = JsonOutputParser(pydantic_object=ChunkExtractionResult)\n",
    "    \n",
    "    # Create prompt template for entity instance extraction\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are part of a multi-step workflow to build a Knowledge Graph from raw text.\n",
    "\n",
    "        Workflow Steps Overview:\n",
    "        1. Extract high-level entity types and their properties from the text. [COMPLETED]\n",
    "        2. Extract specific instances of entities and their properties from text chunks. [CURRENT STEP]\n",
    "        3. Deduplicate extracted instances and assign them unique identifiers.\n",
    "        4. Identify and define relationships between the instances of entities.\n",
    "        5. Create a structured knowledge graph using the extracted entities and relationships.\n",
    "\n",
    "        YOUR TASK:\n",
    "        You are processing a CHUNK of the full text. Focus ONLY on extracting CONCRETE INSTANCES of entities found in this chunk.\n",
    "\n",
    "        GIVEN:\n",
    "        1. A chunk of text\n",
    "        2. A schema of entity types and their possible properties\n",
    "\n",
    "        INSTRUCTIONS:\n",
    "        - Extract ALL instances of the predefined entity types found in this chunk\n",
    "        - For each instance, extract values for as many properties as are mentioned in the text\n",
    "        - Be precise - only extract information explicitly stated in this chunk\n",
    "        - Do NOT make up or infer missing properties\n",
    "        - If a property is not mentioned, omit it from the output (don't include it with null/empty values)\n",
    "\n",
    "        INPUT TEXT CHUNK:\n",
    "        {chunk}\n",
    "\n",
    "        ENTITY TYPES AND THEIR PROPERTIES:\n",
    "        {entity_schema}\n",
    "\n",
    "        FORMAT YOUR RESPONSE AS FOLLOWS:\n",
    "        - Return a valid JSON object\n",
    "        - For each entity type found, include its name and an \"Instances\" object\n",
    "        - \"Instances\" should be a dictionary where:\n",
    "          - Keys are the instance names \n",
    "          - Values are objects containing the (available) instance properties \n",
    "        - Properties not mentioned should be omitted entirely\n",
    "        - If no instances of a particular entity type are found, do not include that entity type\n",
    "\n",
    "        {format_instructions}\n",
    "\n",
    "        EXAMPLE RESPONSE FOR A CHUNK ABOUT PEOPLE AND COMPANIES:\n",
    "        {{\n",
    "        \"entities\": [\n",
    "            {{\n",
    "            \"Entity\": \"Person\",\n",
    "            \"Instances\": {{\n",
    "                \"John Doe\": {{\n",
    "                    \"name\": \"John Doe\",\n",
    "                    \"age\": 35,\n",
    "                    \"email\": \"john@example.com\"\n",
    "                }},\n",
    "                \"Jane Smith\": {{\n",
    "                    \"name\": \"Jane Smith\",\n",
    "                    \"email\": \"jane@example.com\"\n",
    "                }}\n",
    "            }}\n",
    "            }},\n",
    "            {{\n",
    "            \"Entity\": \"Company\",\n",
    "            \"Instances\": {{\n",
    "                \"Google\": {{\n",
    "                    \"industry\": \"Technology\",\n",
    "                    \"founded\": 1998\n",
    "                }}\n",
    "            }}\n",
    "            }}\n",
    "        ]\n",
    "        }}\n",
    "        Begin your extraction now: \"\"\", \n",
    "    input_variables=[\"chunk\", \"entity_schema\", \"chunk_id\"], \n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}, \n",
    "    )\n",
    "\n",
    "    # Build the chain\n",
    "    chain = prompt | llm | parser\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a08d8f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single chunk with retry logic\n",
    "def process_chunk(inputs, max_retries=3):\n",
    "    \"\"\"\n",
    "    Process a single text chunk to extract entity instances with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Dictionary containing chunk and entity_schema\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        \n",
    "    Returns:\n",
    "        dict: Extraction results\n",
    "    \"\"\"\n",
    "    chunk = inputs[\"chunk\"]\n",
    "    entity_schema = inputs[\"entity_schema\"]\n",
    "    chunk_id = chunk.metadata.get(\"chunk_id\", 0)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chain = create_entity_instance_extraction_chain()\n",
    "            result = chain.invoke({\n",
    "                \"chunk\": chunk.page_content,\n",
    "                \"entity_schema\": json.dumps(entity_schema, indent=2),\n",
    "                \"chunk_id\": chunk_id\n",
    "            })\n",
    "            logger.info(f\"Successfully processed chunk {chunk_id}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                logger.warning(f\"Attempt {attempt + 1} failed for chunk {chunk_id}. Retrying... Error: {str(e)[:100]}...\")\n",
    "            else:\n",
    "                logger.error(f\"All {max_retries} attempts failed for chunk {chunk_id}. Error: {str(e)[:100]}...\")\n",
    "                return {\n",
    "                    \"entities\": [],\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"error\": str(e)[:200]\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16454cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the extract_entity_instances_parallel function to stop saving intermediate files\n",
    "def extract_entity_instances_parallel(document, entity_schema, max_concurrency=6):\n",
    "    \"\"\"\n",
    "    Extract entity instances from document chunks in parallel using RunnableParallel.\n",
    "    \n",
    "    Args:\n",
    "        document: The full text document\n",
    "        entity_schema: Dictionary of entity types and their properties\n",
    "        max_concurrency: Maximum number of chunks to process in parallel\n",
    "        \n",
    "    Returns:\n",
    "        list: List of entity instances extracted from all chunks\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(\"Beginning entity instance extraction (parallel)\")\n",
    "    \n",
    "    # Split the document into chunks\n",
    "    chunks = split_text_into_chunks(document, chunk_size=5000, chunk_overlap=500)\n",
    "    logger.info(f\"Document split into {len(chunks)} chunks\")\n",
    "    \n",
    "    # Prepare inputs for each chunk\n",
    "    inputs = [{\"chunk\": chunk, \"entity_schema\": entity_schema} for chunk in chunks]\n",
    "    \n",
    "    # Create a RunnableLambda for chunk processing\n",
    "    chunk_processor = RunnableLambda(process_chunk)\n",
    "    \n",
    "    # Process chunks in batches with progress tracking\n",
    "    all_results = []\n",
    "    batch_size = min(max_concurrency, len(chunks))\n",
    "    \n",
    "    # Use tqdm for progress tracking in batches\n",
    "    with tqdm(total=len(chunks), desc=\"Processing chunks\") as progress_bar:\n",
    "        for i in range(0, len(inputs), batch_size):\n",
    "            batch_inputs = inputs[i:i+batch_size]\n",
    "            \n",
    "            # Process the batch in parallel\n",
    "            batch_results = chunk_processor.batch(batch_inputs, config={\"max_concurrency\": max_concurrency})\n",
    "            all_results.extend(batch_results)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.update(len(batch_inputs))\n",
    "            \n",
    "            # Log progress without saving intermediate files\n",
    "            logger.info(f\"Processed {min(i+batch_size, len(inputs))}/{len(inputs)} chunks ({min((i+batch_size)/len(inputs), 1.0)*100:.1f}%)\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    logger.info(f\"Entity instance extraction completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73fbd50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_chunk_results(chunk_results):\n",
    "    \"\"\"\n",
    "    Merge the results from all chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunk_results: List of extraction results from chunks\n",
    "        \n",
    "    Returns:\n",
    "        list: Combined list of entity instances\n",
    "    \"\"\"\n",
    "    merged_results = []\n",
    "    \n",
    "    # Check for errors\n",
    "    errors = [r for r in chunk_results if r.get(\"error\")]\n",
    "    if errors:\n",
    "        logger.warning(f\"{len(errors)} chunks had errors during processing\")\n",
    "    \n",
    "    # Group by entity type\n",
    "    entity_instances = {}\n",
    "    for result in chunk_results:\n",
    "        if \"entities\" not in result:\n",
    "            continue\n",
    "            \n",
    "        for entity_data in result[\"entities\"]:\n",
    "            entity_type = entity_data.get(\"Entity\")\n",
    "            instances = entity_data.get(\"Instances\", {})\n",
    "            \n",
    "            if entity_type not in entity_instances:\n",
    "                entity_instances[entity_type] = {}\n",
    "                \n",
    "            # Merge instances from this chunk into the collected instances\n",
    "            # If instance already exists, update with any new properties\n",
    "            for instance_name, instance_props in instances.items():\n",
    "                if instance_name in entity_instances[entity_type]:\n",
    "                    # Add any new properties from this instance\n",
    "                    entity_instances[entity_type][instance_name].update(instance_props)\n",
    "                else:\n",
    "                    # Add the new instance\n",
    "                    entity_instances[entity_type][instance_name] = instance_props\n",
    "    \n",
    "    # Convert to the expected format\n",
    "    for entity_type, instances in entity_instances.items():\n",
    "        merged_results.append({\n",
    "            \"Entity\": entity_type,\n",
    "            \"Instances\": instances\n",
    "        })\n",
    "    \n",
    "    logger.info(f\"Merged results for {len(entity_instances)} entity types\")\n",
    "    return merged_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99bbe428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 13:11:39,222 - INFO - Beginning entity instance extraction (parallel)\n",
      "2025-05-21 13:11:39,224 - INFO - Splitting text into chunks (size=5000, overlap=500)\n",
      "2025-05-21 13:11:39,229 - INFO - Text split into 11 chunks\n",
      "2025-05-21 13:11:39,231 - INFO - Document split into 11 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   0%|          | 0/11 [00:00<?, ?it/s]2025-05-21 13:12:07,798 - INFO - Successfully processed chunk 0\n",
      "2025-05-21 13:12:10,587 - INFO - Successfully processed chunk 3\n",
      "2025-05-21 13:12:18,397 - INFO - Successfully processed chunk 2\n",
      "2025-05-21 13:12:18,490 - INFO - Successfully processed chunk 1\n",
      "2025-05-21 13:12:25,341 - INFO - Successfully processed chunk 4\n",
      "2025-05-21 13:12:27,146 - INFO - Successfully processed chunk 5\n",
      "Processing chunks:  55%|█████▍    | 6/11 [00:47<00:39,  7.98s/it]2025-05-21 13:12:27,175 - INFO - Processed 6/11 chunks (54.5%)\n",
      "2025-05-21 13:12:52,975 - INFO - Successfully processed chunk 6\n",
      "2025-05-21 13:13:10,845 - INFO - Successfully processed chunk 10\n",
      "2025-05-21 13:13:16,781 - INFO - Successfully processed chunk 8\n",
      "2025-05-21 13:13:27,128 - INFO - Successfully processed chunk 9\n",
      "2025-05-21 13:13:47,176 - INFO - Successfully processed chunk 7\n",
      "Processing chunks: 100%|██████████| 11/11 [02:07<00:00, 12.34s/it]2025-05-21 13:13:47,186 - INFO - Processed 11/11 chunks (100.0%)\n",
      "Processing chunks: 100%|██████████| 11/11 [02:07<00:00, 11.62s/it]\n",
      "2025-05-21 13:13:47,188 - INFO - Entity instance extraction completed in 127.97 seconds\n",
      "2025-05-21 13:13:47,190 - INFO - Merged results for 12 entity types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted 538 instances across 12 entity types\n",
      "Results saved to entity_instances.json\n",
      "\n",
      "Sample instances:\n",
      "\n",
      "Computing Concept (157 instances):\n",
      "  Instance 1: Distributed Computing - {'definition': 'a system where computing resources are distributed across multiple locations rather ...\n",
      "  Instance 2: Traditional computing - {'limitations': ['faces bottlenecks due to hardware limitations']}...\n",
      "  Instance 3: Parallel Computing - {'definition': 'involves executing multiple processes simultaneously to enhance speed and efficiency...\n",
      "  ... and 154 more\n",
      "\n",
      "System Architecture (31 instances):\n",
      "  Instance 1: Distributed Systems - {'description': 'a system where computing resources are distributed across multiple locations rather...\n",
      "  Instance 2: Clusters - {'description': 'consist of multiple machines with similar hardware and operating systems, working t...\n",
      "  Instance 3: Grids - {'description': 'consist of heterogeneous systems that may have different hardware, OS, and configur...\n",
      "  ... and 28 more\n",
      "\n",
      "Resource (92 instances):\n",
      "  Instance 1: Dedicated machines - {'characteristics': ['needed for parallel computing']}...\n",
      "  Instance 2: Cluster on Demand - {'type': 'cluster-based resources', 'allocation_aspects': ['Users can request cluster-based resource...\n",
      "  Instance 3: internet connection - {'type': 'internet connection'}...\n",
      "  ... and 89 more\n",
      "\n",
      "Storage Type (16 instances):\n",
      "  Instance 1: Shared Folders - {'description': 'Simply sharing files', 'characteristics': ['does not make a system truly distribute...\n",
      "  Instance 2: RAID (Redundant Array of Independent Disks) - {'description': 'Combines multiple storage disks into one logical storage unit', 'characteristics': ...\n",
      "  Instance 3: VM data storage - {}...\n",
      "  ... and 13 more\n",
      "\n",
      "Network Entity (60 instances):\n",
      "  Instance 1: Connected Devices - {'characteristics': ['communicate']}...\n",
      "  Instance 2: LAN (Local Area Networks) - {'type': 'Local Area Networks', 'characteristics': ['all machines are geographically close and in th...\n",
      "  Instance 3: networked devices - {'characteristics': ['communicate'], 'related_components': ['applications']}...\n",
      "  ... and 57 more\n",
      "\n",
      "Role (28 instances):\n",
      "  Instance 1: Individual users - {'function': ['can participate in grid computing', 'can sell extra computational resources within a ...\n",
      "  Instance 2: Users - {'description': 'Part of Stakeholders in the Cloud Ecosystem'}...\n",
      "  Instance 3: Cloud providers - {'role': 'Cloud provider', 'service_offerings': ['offer HPC as a service', 'combine resources into a...\n",
      "  ... and 25 more\n",
      "\n",
      "Organization (23 instances):\n",
      "  Instance 1: Organizations - {'role': ['do not allow their internal systems to be part of a grid due to security concerns']}...\n",
      "  Instance 2: AWS - {'service_offerings': ['bare metal as EC2 instances', 'other services similar to OpenStack'], 'role'...\n",
      "  Instance 3: AWS (Amazon Web Services) - {'role': 'Cloud provider', 'service_offerings': 'offer HPC as a service', 'market_position': 'one of...\n",
      "  ... and 20 more\n",
      "\n",
      "Software Component (66 instances):\n",
      "  Instance 1: Middleware - {'description': 'The software layer that facilitates communication between different system componen...\n",
      "  Instance 2: Kubernetes - {'function': 'Cloud computing allows users to build their own Kubernetes clusters or use Kubernetes ...\n",
      "  Instance 3: Apache Spark - {'type': 'Example'}...\n",
      "  ... and 63 more\n",
      "\n",
      "Platform (24 instances):\n",
      "  Instance 1: AWS EC2 (Elastic Compute Cloud) - {'features': 'provides rapid elasticity'}...\n",
      "  Instance 2: AWS EC2 - {}...\n",
      "  Instance 3: Kubernetes - {'purpose': 'container orchestration platform', 'architecture': ['Control Plane', 'Worker Nodes (Com...\n",
      "  ... and 21 more\n",
      "\n",
      "Service Model (17 instances):\n",
      "  Instance 1: Utility Computing Model - {'definition': 'Users pay only for what they consume.'}...\n",
      "  Instance 2: pay-as-you-go billing models - {'characteristics': 'Cloud providers implement pay-as-you-go billing models.'}...\n",
      "  Instance 3: Cloud Service Model - {'definition': 'define How users access services, How operators manage resources, Responsibility div...\n",
      "  ... and 14 more\n",
      "\n",
      "Deployment Model (17 instances):\n",
      "  Instance 1: Private clouds - {'characteristics': 'not massive', 'access_scope': 'for organizations'}...\n",
      "  Instance 2: Public cloud - {'market_position': '95-96% of cloud infrastructure belongs to a few major companies'}...\n",
      "  Instance 3: Cloud Deployment Models - {'definition': 'define how cloud infrastructure is set up, who can access it, and its intended use'}...\n",
      "  ... and 14 more\n",
      "\n",
      "Database Type (7 instances):\n",
      "  Instance 1: Relational Databases (RDS) - {'description': 'Structured storage with SQL support', 'characteristics': ['Structured storage', 'SQ...\n",
      "  Instance 2: Non-Relational Databases (NoSQL) - {'description': 'Schema-less, scalable databases', 'characteristics': ['Schema-less', 'scalable']}...\n",
      "  Instance 3: Database Migration Services - {'use_cases': ['migrating data between databases']}...\n",
      "  ... and 4 more\n"
     ]
    }
   ],
   "source": [
    "# Load the entity schema\n",
    "with open(\"entity_schema.json\", \"r\") as f:\n",
    "    entity_schema_data = json.load(f)\n",
    "    entity_schema = entity_schema_data.get(\"entities\", {})\n",
    "\n",
    "# Extract entity instances using parallel processing\n",
    "chunk_results = extract_entity_instances_parallel(\n",
    "    document=doc,\n",
    "    entity_schema=entity_schema,\n",
    "    max_concurrency=6  # Process 6 chunks in parallel\n",
    ")\n",
    "\n",
    "# Merge results from all chunks\n",
    "merged_results = merge_chunk_results(chunk_results)\n",
    "\n",
    "# Save only one final result file\n",
    "output_filepath = \"entity_instances.json\"\n",
    "with open(output_filepath, \"w\") as f:\n",
    "    json.dump(merged_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nExtracted {sum(len(entity['Instances']) for entity in merged_results)} instances across {len(merged_results)} entity types\")\n",
    "print(f\"Results saved to {output_filepath}\")\n",
    "\n",
    "# Print sample results\n",
    "print(\"\\nSample instances:\")\n",
    "for entity in merged_results:\n",
    "    entity_type = entity[\"Entity\"]\n",
    "    instances = entity[\"Instances\"]\n",
    "    print(f\"\\n{entity_type} ({len(instances)} instances):\")\n",
    "    \n",
    "    # Get list of instance keys (names) and take first 3\n",
    "    instance_keys = list(instances.keys())[:3]\n",
    "    \n",
    "    # Display up to 3 instances\n",
    "    for i, instance_name in enumerate(instance_keys):\n",
    "        instance_data = instances[instance_name]\n",
    "        print(f\"  Instance {i+1}: {instance_name} - {str(instance_data)[:100]}...\")\n",
    "    \n",
    "    if len(instances) > 3:\n",
    "        print(f\"  ... and {len(instances) - 3} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd1f739",
   "metadata": {},
   "source": [
    "# Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07a8743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_and_assign_ids(entity_data):\n",
    "    \"\"\"\n",
    "    Remove duplicate entity instances and assign incremental integer IDs to each remaining instance.\n",
    "    \n",
    "    Args:\n",
    "        entity_data: List of dictionaries containing entity types and their instances\n",
    "        \n",
    "    Returns:\n",
    "        list: Deduplicated entity instances with unique integer IDs assigned\n",
    "    \"\"\"\n",
    "    logger.info(\"Beginning deduplication and ID assignment\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create a deep copy of the input data to avoid modifying it\n",
    "    deduplicated_data = []\n",
    "    \n",
    "    # Track statistics\n",
    "    total_instances_before = 0\n",
    "    total_instances_after = 0\n",
    "    duplicates_found = 0\n",
    "    \n",
    "    # Use a counter for assigning incremental IDs\n",
    "    id_counter = 1\n",
    "    \n",
    "    # Process each entity type\n",
    "    for entity in entity_data:\n",
    "        entity_type = entity[\"Entity\"]\n",
    "        instances = entity[\"Instances\"]\n",
    "        total_instances_before += len(instances)\n",
    "        \n",
    "        # Create a new dictionary for the deduplicated instances\n",
    "        deduplicated_instances = {}\n",
    "        \n",
    "        # For each instance, check if it already exists and merge if needed\n",
    "        for instance_name, instance_props in instances.items():\n",
    "            # If this instance name already exists, merge properties\n",
    "            if instance_name in deduplicated_instances:\n",
    "                duplicates_found += 1\n",
    "                existing_props = deduplicated_instances[instance_name]\n",
    "                \n",
    "                # Merge properties, keeping all unique properties\n",
    "                for key, value in instance_props.items():\n",
    "                    if key not in existing_props:\n",
    "                        existing_props[key] = value\n",
    "                        \n",
    "                # Log information about the merge\n",
    "                logger.debug(f\"Merged duplicate instance '{instance_name}' in entity type '{entity_type}'\")\n",
    "            else:\n",
    "                # Add this instance to the deduplicated set\n",
    "                deduplicated_instances[instance_name] = instance_props.copy()\n",
    "        \n",
    "        # Now assign unique integer IDs to each instance\n",
    "        for instance_name, props in deduplicated_instances.items():\n",
    "            # Assign an incremental integer ID\n",
    "            props[\"id\"] = id_counter\n",
    "            id_counter += 1\n",
    "        \n",
    "        # Add the deduplicated entity to the result\n",
    "        deduplicated_data.append({\n",
    "            \"Entity\": entity_type,\n",
    "            \"Instances\": deduplicated_instances\n",
    "        })\n",
    "        \n",
    "        total_instances_after += len(deduplicated_instances)\n",
    "        \n",
    "        logger.info(f\"Processed entity type '{entity_type}': {len(instances)} instances → {len(deduplicated_instances)} unique instances\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    logger.info(f\"Deduplication completed in {end_time - start_time:.2f} seconds\")\n",
    "    logger.info(f\"Total instances before: {total_instances_before}, after: {total_instances_after}\")\n",
    "    logger.info(f\"Removed {duplicates_found} duplicate instances\")\n",
    "    logger.info(f\"Assigned IDs from 1 to {id_counter-1}\")\n",
    "    \n",
    "    return deduplicated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14014df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_step3():\n",
    "    \"\"\"\n",
    "    Execute Step 3: Deduplicate entity instances and assign unique IDs.\n",
    "    \"\"\"\n",
    "    # Load data from Step 2\n",
    "    logger.info(\"Loading entity instances from Step 2\")\n",
    "    try:\n",
    "        with open(\"entity_instances.json\", \"r\") as f:\n",
    "            entity_instances = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(\"entity_instances.json not found. Please complete Step 2 first.\")\n",
    "        return None\n",
    "    \n",
    "    # Deduplicate and assign IDs\n",
    "    deduplicated_data = deduplicate_and_assign_ids(entity_instances)\n",
    "    \n",
    "    # Save the deduplicated data\n",
    "    output_filepath = \"deduplicated_entities_2.json\"\n",
    "    with open(output_filepath, \"w\") as f:\n",
    "        json.dump(deduplicated_data, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"Deduplicated entities saved to {output_filepath}\")\n",
    "    \n",
    "    # Print summary\n",
    "    total_instances = sum(len(entity[\"Instances\"]) for entity in deduplicated_data)\n",
    "    print(f\"\\nDeduplication complete: {total_instances} unique instances across {len(deduplicated_data)} entity types\")\n",
    "    print(f\"Results saved to {output_filepath}\")\n",
    "    \n",
    "    # Print sample results\n",
    "    print(\"\\nSample deduplicated instances:\")\n",
    "    for entity in deduplicated_data:\n",
    "        entity_type = entity[\"Entity\"]\n",
    "        instances = entity[\"Instances\"]\n",
    "        print(f\"\\n{entity_type} ({len(instances)} instances):\")\n",
    "        \n",
    "        # Get list of instance keys (names) and take first 3\n",
    "        instance_keys = list(instances.keys())[:3]\n",
    "        \n",
    "        # Display up to 3 instances\n",
    "        for i, instance_name in enumerate(instance_keys):\n",
    "            instance_data = instances[instance_name]\n",
    "            # Show ID and a few other properties \n",
    "            props_preview = {k: v for k, v in list(instance_data.items())[:4]}\n",
    "            print(f\"  Instance {i+1}: {instance_name} - {props_preview}\")\n",
    "        \n",
    "        if len(instances) > 3:\n",
    "            print(f\"  ... and {len(instances) - 3} more\")\n",
    "    \n",
    "    return deduplicated_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80795399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 13:14:19,603 - INFO - Loading entity instances from Step 2\n",
      "2025-05-21 13:14:19,607 - INFO - Beginning deduplication and ID assignment\n",
      "2025-05-21 13:14:19,608 - INFO - Processed entity type 'Computing Concept': 157 instances → 157 unique instances\n",
      "2025-05-21 13:14:19,609 - INFO - Processed entity type 'System Architecture': 31 instances → 31 unique instances\n",
      "2025-05-21 13:14:19,611 - INFO - Processed entity type 'Resource': 92 instances → 92 unique instances\n",
      "2025-05-21 13:14:19,612 - INFO - Processed entity type 'Storage Type': 16 instances → 16 unique instances\n",
      "2025-05-21 13:14:19,613 - INFO - Processed entity type 'Network Entity': 60 instances → 60 unique instances\n",
      "2025-05-21 13:14:19,614 - INFO - Processed entity type 'Role': 28 instances → 28 unique instances\n",
      "2025-05-21 13:14:19,615 - INFO - Processed entity type 'Organization': 23 instances → 23 unique instances\n",
      "2025-05-21 13:14:19,616 - INFO - Processed entity type 'Software Component': 66 instances → 66 unique instances\n",
      "2025-05-21 13:14:19,617 - INFO - Processed entity type 'Platform': 24 instances → 24 unique instances\n",
      "2025-05-21 13:14:19,618 - INFO - Processed entity type 'Service Model': 17 instances → 17 unique instances\n",
      "2025-05-21 13:14:19,619 - INFO - Processed entity type 'Deployment Model': 17 instances → 17 unique instances\n",
      "2025-05-21 13:14:19,620 - INFO - Processed entity type 'Database Type': 7 instances → 7 unique instances\n",
      "2025-05-21 13:14:19,621 - INFO - Deduplication completed in 0.01 seconds\n",
      "2025-05-21 13:14:19,622 - INFO - Total instances before: 538, after: 538\n",
      "2025-05-21 13:14:19,623 - INFO - Removed 0 duplicate instances\n",
      "2025-05-21 13:14:19,624 - INFO - Assigned IDs from 1 to 538\n",
      "2025-05-21 13:14:19,655 - INFO - Deduplicated entities saved to deduplicated_entities_2.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deduplication complete: 538 unique instances across 12 entity types\n",
      "Results saved to deduplicated_entities_2.json\n",
      "\n",
      "Sample deduplicated instances:\n",
      "\n",
      "Computing Concept (157 instances):\n",
      "  Instance 1: Distributed Computing - {'definition': 'a system where computing resources are distributed across multiple locations rather than being centralized in a single system.', 'aspects': ['enables task distribution and efficient resource utilization', 'enables systems to function in multiple layers, with each layer acting as a distributed entity', 'presents a unified system to users'], 'use_cases': ['IoT (Internet of Things)'], 'related_concepts': ['Parallel Computing', 'Cloud Computing']}\n",
      "  Instance 2: Traditional computing - {'limitations': ['faces bottlenecks due to hardware limitations'], 'id': 2}\n",
      "  Instance 3: Parallel Computing - {'definition': 'involves executing multiple processes simultaneously to enhance speed and efficiency.', 'aspects': ['Distribution & Speed', 'Core Objective'], 'use_cases': ['Vector processing', 'Image processing', 'Matrix multiplication'], 'limitations': ['Not all applications can be parallelized', 'Some components of code can be executed in parallel, while others may not', 'Specific programming languages are required for parallel computing']}\n",
      "  ... and 154 more\n",
      "\n",
      "System Architecture (31 instances):\n",
      "  Instance 1: Distributed Systems - {'description': 'a system where computing resources are distributed across multiple locations rather than being centralized in a single system.', 'characteristics': ['allow for hardware scaling', 'function in multiple layers, with each layer acting as a distributed entity', 'presents a unified system to users'], 'components': ['multiple machines', 'Shared Folders', 'Middleware'], 'use_cases': ['IoT (Internet of Things)']}\n",
      "  Instance 2: Clusters - {'description': 'consist of multiple machines with similar hardware and operating systems, working together in a symmetric manner.', 'characteristics': ['Middleware abstracts hardware, OS, and software details from users', 'All machines have the same type of hardware and OS', 'Parallel computing is implemented efficiently in clusters', 'Middleware in clusters has fewer complexities', 'typically exist in LAN (Local Area Networks)', 'all machines are geographically close and in the same environment'], 'components': ['multiple machines', 'Middleware'], 'comparison_aspects': ['compared to Grids']}\n",
      "  Instance 3: Grids - {'description': 'consist of heterogeneous systems that may have different hardware, OS, and configurations.', 'characteristics': ['operate in an asymmetric manner', 'Middleware in grids has to handle diverse configurations, leading to potential performance degradation', 'geographically distributed'], 'components': ['heterogeneous systems', 'Middleware'], 'use_cases': ['solving large-scale computational problems']}\n",
      "  ... and 28 more\n",
      "\n",
      "Resource (92 instances):\n",
      "  Instance 1: Dedicated machines - {'characteristics': ['needed for parallel computing'], 'id': 189}\n",
      "  Instance 2: Cluster on Demand - {'type': 'cluster-based resources', 'allocation_aspects': ['Users can request cluster-based resources directly from cloud providers'], 'related_concepts': ['Clusters', 'Cloud Computing'], 'id': 190}\n",
      "  Instance 3: internet connection - {'type': 'internet connection', 'id': 191}\n",
      "  ... and 89 more\n",
      "\n",
      "Storage Type (16 instances):\n",
      "  Instance 1: Shared Folders - {'description': 'Simply sharing files', 'characteristics': ['does not make a system truly distributed'], 'management_aspects': ['Proper access control and system design are needed'], 'id': 281}\n",
      "  Instance 2: RAID (Redundant Array of Independent Disks) - {'description': 'Combines multiple storage disks into one logical storage unit', 'characteristics': 'Provides redundancy for fault tolerance', 'id': 282}\n",
      "  Instance 3: VM data storage - {'id': 283}\n",
      "  ... and 13 more\n",
      "\n",
      "Network Entity (60 instances):\n",
      "  Instance 1: Connected Devices - {'characteristics': ['communicate'], 'id': 297}\n",
      "  Instance 2: LAN (Local Area Networks) - {'type': 'Local Area Networks', 'characteristics': ['all machines are geographically close and in the same environment'], 'related_concepts': ['Clusters'], 'id': 298}\n",
      "  Instance 3: networked devices - {'characteristics': ['communicate'], 'related_components': ['applications'], 'id': 299}\n",
      "  ... and 57 more\n",
      "\n",
      "Role (28 instances):\n",
      "  Instance 1: Individual users - {'function': ['can participate in grid computing', 'can sell extra computational resources within a grid'], 'id': 357}\n",
      "  Instance 2: Users - {'description': 'Part of Stakeholders in the Cloud Ecosystem', 'id': 358}\n",
      "  Instance 3: Cloud providers - {'role': 'Cloud provider', 'service_offerings': ['offer HPC as a service', 'combine resources into a shared pool', 'implement pay-as-you-go billing models.'], 'id': 359}\n",
      "  ... and 25 more\n",
      "\n",
      "Organization (23 instances):\n",
      "  Instance 1: Organizations - {'role': ['do not allow their internal systems to be part of a grid due to security concerns'], 'id': 385}\n",
      "  Instance 2: AWS - {'service_offerings': ['bare metal as EC2 instances', 'other services similar to OpenStack'], 'role': ['large cloud service providers (CSPs)'], 'infrastructure_aspects': 'uses different hypervisors', 'id': 386}\n",
      "  Instance 3: AWS (Amazon Web Services) - {'role': 'Cloud provider', 'service_offerings': 'offer HPC as a service', 'market_position': 'one of a few major companies like: AWS', 'id': 387}\n",
      "  ... and 20 more\n",
      "\n",
      "Software Component (66 instances):\n",
      "  Instance 1: Middleware - {'description': 'The software layer that facilitates communication between different system components.', 'function': ['Middleware is responsible for managing resources in the cloud.', 'Middleware is responsible for resource pooling.'], 'characteristics': ['abstracts hardware, OS, and software details from users', 'in clusters has fewer complexities', 'in grids has to handle diverse configurations, leading to potential performance degradation'], 'related_components': ['system components', 'applications', 'networked devices', 'connected components']}\n",
      "  Instance 2: Kubernetes - {'function': 'Cloud computing allows users to build their own Kubernetes clusters or use Kubernetes as a managed service.', 'id': 409}\n",
      "  Instance 3: Apache Spark - {'type': 'Example', 'id': 410}\n",
      "  ... and 63 more\n",
      "\n",
      "Platform (24 instances):\n",
      "  Instance 1: AWS EC2 (Elastic Compute Cloud) - {'features': 'provides rapid elasticity', 'id': 474}\n",
      "  Instance 2: AWS EC2 - {'id': 475}\n",
      "  Instance 3: Kubernetes - {'purpose': 'container orchestration platform', 'architecture': ['Control Plane', 'Worker Nodes (Computing Plane)'], 'components': ['Node', 'Pod', 'Scheduler', 'etcd', 'API Server', 'Controller Manager', 'Kube-proxy'], 'deployment_aspects': ['System Build (Manual Deployment)', 'Kubernetes as a Service (Managed)']}\n",
      "  ... and 21 more\n",
      "\n",
      "Service Model (17 instances):\n",
      "  Instance 1: Utility Computing Model - {'definition': 'Users pay only for what they consume.', 'id': 498}\n",
      "  Instance 2: pay-as-you-go billing models - {'characteristics': 'Cloud providers implement pay-as-you-go billing models.', 'id': 499}\n",
      "  Instance 3: Cloud Service Model - {'definition': 'define How users access services, How operators manage resources, Responsibility division between users and cloud service providers (CSPs)', 'id': 500}\n",
      "  ... and 14 more\n",
      "\n",
      "Deployment Model (17 instances):\n",
      "  Instance 1: Private clouds - {'characteristics': 'not massive', 'access_scope': 'for organizations', 'id': 515}\n",
      "  Instance 2: Public cloud - {'market_position': '95-96% of cloud infrastructure belongs to a few major companies', 'id': 516}\n",
      "  Instance 3: Cloud Deployment Models - {'definition': 'define how cloud infrastructure is set up, who can access it, and its intended use', 'id': 517}\n",
      "  ... and 14 more\n",
      "\n",
      "Database Type (7 instances):\n",
      "  Instance 1: Relational Databases (RDS) - {'description': 'Structured storage with SQL support', 'characteristics': ['Structured storage', 'SQL support'], 'id': 532}\n",
      "  Instance 2: Non-Relational Databases (NoSQL) - {'description': 'Schema-less, scalable databases', 'characteristics': ['Schema-less', 'scalable'], 'id': 533}\n",
      "  Instance 3: Database Migration Services - {'use_cases': ['migrating data between databases'], 'id': 534}\n",
      "  ... and 4 more\n"
     ]
    }
   ],
   "source": [
    "# Execute Step 3\n",
    "deduplicated_entities = process_step3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bb46bf",
   "metadata": {},
   "source": [
    "# Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30c42bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic models for relationship extraction\n",
    "class EntityReference(BaseModel):\n",
    "    \"\"\"Reference to an entity instance\"\"\"\n",
    "    entity: str = Field(description=\"Entity type name\")\n",
    "    id: int = Field(description=\"Unique ID of the entity instance\")  # Changed from str to int\n",
    "\n",
    "class Relationship(BaseModel):\n",
    "    \"\"\"Relationship between two entity instances\"\"\"\n",
    "    relationship: str = Field(description=\"Type or name of the relationship\")\n",
    "    source: EntityReference = Field(description=\"Source entity of the relationship\")\n",
    "    destination: EntityReference = Field(description=\"Destination entity of the relationship\")\n",
    "\n",
    "class RelationshipExtractionResult(BaseModel):\n",
    "    \"\"\"Result of relationship extraction\"\"\"\n",
    "    relationships: List[Relationship] = Field(description=\"List of extracted relationships between entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5911857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relationship_extraction_chain():\n",
    "    # Relationship extraction result parser\n",
    "    parser = JsonOutputParser(pydantic_object=RelationshipExtractionResult)\n",
    "    \n",
    "    # Create prompt template for relationship extraction\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are the fourth agent in a multi-step workflow to build a Knowledge Graph from raw text.\n",
    "\n",
    "        Workflow Steps Overview:\n",
    "        1. Extract high-level entity types and their properties from the text. [COMPLETED]\n",
    "        2. Extract specific instances of entities and their properties from text chunks. [COMPLETED]\n",
    "        3. Deduplicate extracted instances and assign them unique identifiers. [COMPLETED]\n",
    "        4. Identify and define relationships between the instances of entities. [CURRENT STEP]\n",
    "        5. Create a structured knowledge graph using the extracted entities and relationships.\n",
    "\n",
    "        YOUR TASK:\n",
    "        Extract relationships between the entity instances that have already been identified in the text.\n",
    "\n",
    "        GIVEN:\n",
    "        1. Original text document\n",
    "        2. List of entity types and their instances with unique IDs\n",
    "\n",
    "        INSTRUCTIONS:\n",
    "        - Identify relationships between entity instances\n",
    "        - Focus on explicit relationships mentioned in the text\n",
    "        - Be precise - only extract relationships clearly stated in the text\n",
    "        - Use common relationship types like \"works_at\", \"part_of\", \"authored\", \"connected_to\", etc.\n",
    "        - Ensure all relationships reference valid entity instances with their correct IDs\n",
    "        - IMPORTANT: All entity IDs are integers, not strings\n",
    "\n",
    "        ORIGINAL TEXT:\n",
    "        {input_text}\n",
    "\n",
    "        ENTITY INSTANCES:\n",
    "        {entity_instances}\n",
    "\n",
    "        FORMAT YOUR RESPONSE:\n",
    "        Return a list of relationship objects, where each relationship contains:\n",
    "        1. The relationship type/name (e.g., \"works_at\", \"located_in\")\n",
    "        2. Source entity reference (entity type and instance ID as an integer)\n",
    "        3. Destination entity reference (entity type and instance ID as an integer)\n",
    "\n",
    "        {format_instructions}\n",
    "\n",
    "        EXAMPLE RESPONSE:\n",
    "        {{\n",
    "          \"relationships\": [\n",
    "            {{\n",
    "              \"relationship\": \"works_at\",\n",
    "              \"source\": {{\"entity\": \"Person\", \"id\": 1}},\n",
    "              \"destination\": {{\"entity\": \"Company\", \"id\": 3}}\n",
    "            }},\n",
    "            {{\n",
    "              \"relationship\": \"located_in\",\n",
    "              \"source\": {{\"entity\": \"Company\", \"id\": 3}},\n",
    "              \"destination\": {{\"entity\": \"Location\", \"id\": 76}}\n",
    "            }}\n",
    "          ]\n",
    "        }}\n",
    "\n",
    "        Begin your extraction now:\n",
    "        \"\"\", \n",
    "        input_variables=[\"input_text\", \"entity_instances\"], \n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()}, \n",
    "    )\n",
    "\n",
    "    # Build the chain\n",
    "    chain = prompt | llm | parser\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44ab53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk_for_relationships(inputs, max_retries=3):\n",
    "    \"\"\"\n",
    "    Process a single text chunk to extract relationships with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Dictionary containing chunk, entity_instances, and other parameters\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        \n",
    "    Returns:\n",
    "        dict: Extracted relationships from this chunk\n",
    "    \"\"\"\n",
    "    chunk = inputs[\"chunk\"]\n",
    "    entity_instances = inputs[\"entity_instances\"]\n",
    "    chunk_id = chunk.metadata.get(\"chunk_id\", 0)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            chain = create_relationship_extraction_chain()\n",
    "            \n",
    "            # Create a simplified version of entities for the prompt\n",
    "            simplified_entities = []\n",
    "            entity_id_map = {}\n",
    "            \n",
    "            for entity_data in entity_instances:\n",
    "                entity_type = entity_data[\"Entity\"]\n",
    "                instances = []\n",
    "                \n",
    "                for instance_name, props in entity_data[\"Instances\"].items():\n",
    "                    instance_id = props.get(\"id\")\n",
    "                    if instance_id is not None:\n",
    "                        # Ensure ID is an integer\n",
    "                        instance_id = int(instance_id)\n",
    "                        instances.append({\n",
    "                            \"name\": instance_name,\n",
    "                            \"id\": instance_id\n",
    "                        })\n",
    "                        entity_id_map[instance_id] = {\"entity_type\": entity_type}\n",
    "                \n",
    "                simplified_entities.append({\n",
    "                    \"Entity\": entity_type,\n",
    "                    \"Instances\": instances\n",
    "                })\n",
    "            \n",
    "            result = chain.invoke({\n",
    "                \"input_text\": chunk.page_content,\n",
    "                \"entity_instances\": json.dumps(simplified_entities, indent=2)\n",
    "            })\n",
    "            \n",
    "            # Validate the relationships against our entity ID map\n",
    "            valid_relationships = []\n",
    "            for rel in result.get(\"relationships\", []):\n",
    "                # Ensure IDs are integers\n",
    "                source_id = int(rel[\"source\"][\"id\"])\n",
    "                dest_id = int(rel[\"destination\"][\"id\"])\n",
    "                \n",
    "                if source_id in entity_id_map and dest_id in entity_id_map:\n",
    "                    # Add chunk ID metadata for debugging/analysis\n",
    "                    rel[\"chunk_id\"] = chunk_id\n",
    "                    valid_relationships.append(rel)\n",
    "            \n",
    "            logger.info(f\"Successfully processed chunk {chunk_id}, found {len(valid_relationships)} relationships\")\n",
    "            return {\"relationships\": valid_relationships, \"chunk_id\": chunk_id}\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                logger.warning(f\"Attempt {attempt + 1} failed for chunk {chunk_id}. Retrying... Error: {str(e)[:100]}...\")\n",
    "            else:\n",
    "                logger.error(f\"All {max_retries} attempts failed for chunk {chunk_id}. Error: {str(e)[:100]}...\")\n",
    "                return {\n",
    "                    \"relationships\": [],\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"error\": str(e)[:200]\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81b7bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_relationship_results(chunk_results):\n",
    "    \"\"\"\n",
    "    Merge relationship extraction results from all chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunk_results: List of relationship extraction results from chunks\n",
    "        \n",
    "    Returns:\n",
    "        dict: Merged relationship extraction results\n",
    "    \"\"\"\n",
    "    # Create a set to track unique relationships and avoid duplicates\n",
    "    unique_relationships = set()\n",
    "    merged_relationships = []\n",
    "    \n",
    "    # Check for errors\n",
    "    errors = [r for r in chunk_results if r.get(\"error\")]\n",
    "    if errors:\n",
    "        logger.warning(f\"{len(errors)} chunks had errors during relationship extraction\")\n",
    "    \n",
    "    # Merge relationships from all chunks\n",
    "    for result in chunk_results:\n",
    "        relationships = result.get(\"relationships\", [])\n",
    "        \n",
    "        for rel in relationships:\n",
    "            # Create a tuple that uniquely identifies this relationship\n",
    "            rel_key = (\n",
    "                rel[\"relationship\"], \n",
    "                rel[\"source\"][\"entity\"], \n",
    "                int(rel[\"source\"][\"id\"]),  # Ensure integer\n",
    "                rel[\"destination\"][\"entity\"],\n",
    "                int(rel[\"destination\"][\"id\"])  # Ensure integer\n",
    "            )\n",
    "            \n",
    "            # Only add if we haven't seen this exact relationship before\n",
    "            if rel_key not in unique_relationships:\n",
    "                unique_relationships.add(rel_key)\n",
    "                merged_relationships.append(rel)\n",
    "    \n",
    "    logger.info(f\"Merged {len(merged_relationships)} unique relationships from all chunks\")\n",
    "    return {\"relationships\": merged_relationships}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41a73946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relationships_parallel(input_text, entity_instances, max_concurrency=6):\n",
    "    \"\"\"\n",
    "    Extract relationships from document chunks in parallel using RunnableParallel.\n",
    "    \n",
    "    Args:\n",
    "        input_text: The full text document\n",
    "        entity_instances: Complete list of deduplicated entity instances\n",
    "        max_concurrency: Maximum number of chunks to process in parallel\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing extracted relationships from all chunks\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(\"Beginning relationship extraction (parallel)\")\n",
    "    \n",
    "    # Split the document into chunks (reusing the function from Step 2)\n",
    "    chunks = split_text_into_chunks(input_text, chunk_size=5000, chunk_overlap=500)\n",
    "    logger.info(f\"Document split into {len(chunks)} chunks for relationship extraction\")\n",
    "    \n",
    "    # Prepare inputs for each chunk\n",
    "    inputs = [{\"chunk\": chunk, \"entity_instances\": entity_instances} for chunk in chunks]\n",
    "    \n",
    "    # Create a RunnableLambda for chunk processing\n",
    "    chunk_processor = RunnableLambda(process_chunk_for_relationships)\n",
    "    \n",
    "    # Process chunks in batches with progress tracking\n",
    "    all_results = []\n",
    "    batch_size = min(max_concurrency, len(chunks))\n",
    "    \n",
    "    # Use tqdm for progress tracking in batches\n",
    "    with tqdm(total=len(chunks), desc=\"Processing chunks\") as progress_bar:\n",
    "        for i in range(0, len(inputs), batch_size):\n",
    "            batch_inputs = inputs[i:i+batch_size]\n",
    "            \n",
    "            # Process the batch in parallel\n",
    "            batch_results = chunk_processor.batch(batch_inputs, config={\"max_concurrency\": max_concurrency})\n",
    "            all_results.extend(batch_results)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.update(len(batch_inputs))\n",
    "            \n",
    "            # Log progress\n",
    "            logger.info(f\"Processed {min(i+batch_size, len(inputs))}/{len(inputs)} chunks ({min((i+batch_size)/len(inputs), 1.0)*100:.1f}%)\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    logger.info(f\"Relationship extraction completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Merge results from all chunks\n",
    "    merged_results = merge_relationship_results(all_results)\n",
    "    \n",
    "    return merged_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a1c62fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_step4():\n",
    "    \"\"\"\n",
    "    Execute Step 4: Extract relationships between entity instances using parallel processing.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing extracted relationships\n",
    "    \"\"\"\n",
    "    # Load data from Step 3\n",
    "    logger.info(\"Loading deduplicated entities from Step 3\")\n",
    "    try:\n",
    "        with open(\"deduplicated_entities_2.json\", \"r\") as f:\n",
    "            entity_instances = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(\"Deduplicated entities file not found. Please complete Step 3 first.\")\n",
    "        return None\n",
    "    \n",
    "    # Load original text from PDF\n",
    "    try:\n",
    "        input_text = load_pdf_content(\"Cloud Computing Copy Lecture Notes.pdf\")\n",
    "        logger.info(f\"Loaded original text: {len(input_text)} characters\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load PDF content: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Extract relationships using parallel processing\n",
    "    relationships = extract_relationships_parallel(input_text, entity_instances, max_concurrency=6)\n",
    "    \n",
    "    # Save the relationships\n",
    "    output_filepath = \"entity_relationships.json\"\n",
    "    with open(output_filepath, \"w\") as f:\n",
    "        json.dump(relationships, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"Extracted relationships saved to {output_filepath}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nRelationship extraction complete: {len(relationships['relationships'])} relationships found\")\n",
    "    print(f\"Results saved to {output_filepath}\")\n",
    "    \n",
    "    # Print sample results\n",
    "    print(\"\\nSample relationships:\")\n",
    "    for i, rel in enumerate(relationships[\"relationships\"][:5]):  # Show up to 5 relationships\n",
    "        print(f\"\\nRelationship {i+1}:\")\n",
    "        print(f\"  Type: {rel['relationship']}\")\n",
    "        print(f\"  Source: {rel['source']['entity']} - (ID: {rel['source']['id']})\")\n",
    "        print(f\"  Destination: {rel['destination']['entity']} - (ID: {rel['destination']['id']})\")\n",
    "    \n",
    "    if len(relationships[\"relationships\"]) > 5:\n",
    "        print(f\"\\n... and {len(relationships['relationships']) - 5} more relationships\")\n",
    "    \n",
    "    return relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d01bda4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 13:21:02,784 - INFO - Loading deduplicated entities from Step 3\n",
      "2025-05-21 13:21:05,123 - INFO - Loaded original text: 48269 characters\n",
      "2025-05-21 13:21:05,124 - INFO - Beginning relationship extraction (parallel)\n",
      "2025-05-21 13:21:05,125 - INFO - Splitting text into chunks (size=5000, overlap=500)\n",
      "2025-05-21 13:21:05,130 - INFO - Text split into 11 chunks\n",
      "2025-05-21 13:21:05,132 - INFO - Document split into 11 chunks for relationship extraction\n",
      "Processing chunks:   0%|          | 0/11 [00:00<?, ?it/s]2025-05-21 13:22:04,673 - INFO - Successfully processed chunk 1, found 70 relationships\n",
      "2025-05-21 13:22:09,979 - INFO - Successfully processed chunk 3, found 86 relationships\n",
      "2025-05-21 13:22:21,726 - INFO - Successfully processed chunk 4, found 109 relationships\n",
      "2025-05-21 13:22:39,737 - INFO - Successfully processed chunk 5, found 116 relationships\n",
      "2025-05-21 13:23:07,745 - INFO - Successfully processed chunk 2, found 89 relationships\n",
      "2025-05-21 13:24:55,496 - INFO - Successfully processed chunk 0, found 291 relationships\n",
      "Processing chunks:  55%|█████▍    | 6/11 [03:50<03:11, 38.39s/it]2025-05-21 13:24:55,508 - INFO - Processed 6/11 chunks (54.5%)\n",
      "2025-05-21 13:25:47,431 - INFO - Successfully processed chunk 10, found 74 relationships\n",
      "2025-05-21 13:25:56,082 - INFO - Successfully processed chunk 6, found 60 relationships\n",
      "2025-05-21 13:26:07,393 - INFO - Successfully processed chunk 8, found 69 relationships\n",
      "2025-05-21 13:26:16,660 - INFO - Successfully processed chunk 7, found 130 relationships\n",
      "2025-05-21 13:26:30,378 - INFO - Successfully processed chunk 9, found 128 relationships\n",
      "Processing chunks: 100%|██████████| 11/11 [05:25<00:00, 27.84s/it]2025-05-21 13:26:30,391 - INFO - Processed 11/11 chunks (100.0%)\n",
      "Processing chunks: 100%|██████████| 11/11 [05:25<00:00, 29.57s/it]\n",
      "2025-05-21 13:26:30,396 - INFO - Relationship extraction completed in 325.27 seconds\n",
      "2025-05-21 13:26:30,402 - INFO - Merged 1170 unique relationships from all chunks\n",
      "2025-05-21 13:26:30,501 - INFO - Extracted relationships saved to entity_relationships.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Relationship extraction complete: 1170 relationships found\n",
      "Results saved to entity_relationships.json\n",
      "\n",
      "Sample relationships:\n",
      "\n",
      "Relationship 1:\n",
      "  Type: implemented_by\n",
      "  Source: Computing Concept - (ID: 1)\n",
      "  Destination: System Architecture - (ID: 181)\n",
      "\n",
      "Relationship 2:\n",
      "  Type: enables\n",
      "  Source: Computing Concept - (ID: 1)\n",
      "  Destination: Computing Concept - (ID: 37)\n",
      "\n",
      "Relationship 3:\n",
      "  Type: presents_to\n",
      "  Source: Computing Concept - (ID: 1)\n",
      "  Destination: Role - (ID: 358)\n",
      "\n",
      "Relationship 4:\n",
      "  Type: required_for\n",
      "  Source: Resource - (ID: 189)\n",
      "  Destination: Computing Concept - (ID: 3)\n",
      "\n",
      "Relationship 5:\n",
      "  Type: bridges\n",
      "  Source: Software Component - (ID: 408)\n",
      "  Destination: Software Component - (ID: 413)\n",
      "\n",
      "... and 1165 more relationships\n"
     ]
    }
   ],
   "source": [
    "# Execute Step 4 with parallel processing\n",
    "relationships = process_step4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "566163ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created graph with 200 nodes and 189 edges\n",
      "Interactive visualization saved to knowledge_graph.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"800\"\n",
       "            src=\"knowledge_graph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x26b17c739d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import json\n",
    "import os\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "def load_graph_data():\n",
    "    \"\"\"Load entity and relationship data from JSON files\"\"\"\n",
    "    # Load entities (nodes)\n",
    "    with open(\"deduplicated_entities_2.json\", \"r\") as f:\n",
    "        entities = json.load(f)\n",
    "    \n",
    "    # Load relationships (edges)\n",
    "    with open(\"entity_relationships.json\", \"r\") as f:\n",
    "        relationships = json.load(f)\n",
    "    \n",
    "    return entities, relationships[\"relationships\"]\n",
    "\n",
    "def create_knowledge_graph(entities, relationships, max_nodes=None):\n",
    "    \"\"\"Create a NetworkX graph from entities and relationships\"\"\"\n",
    "    # Create a new graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Track entity types for coloring\n",
    "    entity_types = set()\n",
    "    entity_type_map = {}\n",
    "    \n",
    "    # Add nodes with properties\n",
    "    node_count = 0\n",
    "    for entity_group in entities:\n",
    "        entity_type = entity_group[\"Entity\"]\n",
    "        entity_types.add(entity_type)\n",
    "        \n",
    "        for instance_name, props in entity_group[\"Instances\"].items():\n",
    "            if max_nodes and node_count >= max_nodes:\n",
    "                break\n",
    "                \n",
    "            # Add node with properties\n",
    "            node_id = props[\"id\"]\n",
    "            G.add_node(node_id, \n",
    "                       label=instance_name, \n",
    "                       title=f\"<b>{entity_type}: {instance_name}</b><br>\" + \n",
    "                             \"<br>\".join([f\"{k}: {v}\" for k, v in props.items() if k != \"id\"]),\n",
    "                       group=entity_type,\n",
    "                       entity_type=entity_type)\n",
    "            \n",
    "            entity_type_map[node_id] = entity_type\n",
    "            node_count += 1\n",
    "    \n",
    "    # Add edges with relationship types\n",
    "    edge_count = 0\n",
    "    for rel in relationships:\n",
    "        source_id = rel[\"source\"][\"id\"]\n",
    "        target_id = rel[\"destination\"][\"id\"]\n",
    "        rel_type = rel[\"relationship\"]\n",
    "        \n",
    "        # Only add edges between nodes that exist in the graph\n",
    "        if source_id in G.nodes and target_id in G.nodes:\n",
    "            G.add_edge(source_id, target_id, \n",
    "                       title=rel_type,\n",
    "                       label=rel_type[:10] + \"...\" if len(rel_type) > 10 else rel_type)\n",
    "            edge_count += 1\n",
    "    \n",
    "    print(f\"Created graph with {len(G.nodes)} nodes and {len(G.edges)} edges\")\n",
    "    return G, entity_types, entity_type_map\n",
    "\n",
    "def visualize_with_pyvis(G, entity_types, output_path=\"knowledge_graph.html\", height=800, width=1000):\n",
    "    \"\"\"Create an interactive visualization using PyVis\"\"\"\n",
    "    # Set up colors for entity types\n",
    "    color_palette = [\"#3da4ab\", \"#f15025\", \"#094074\", \"#ffcc00\", \"#7b2cbf\", \"#219ebc\", \n",
    "                     \"#fb8b24\", \"#8338ec\", \"#06d6a0\", \"#ef476f\", \"#073b4c\", \"#118ab2\"]\n",
    "    \n",
    "    color_map = {}\n",
    "    for i, entity_type in enumerate(entity_types):\n",
    "        color_map[entity_type] = color_palette[i % len(color_palette)]\n",
    "    \n",
    "    # Create PyVis network\n",
    "    net = Network(height=f\"{height}px\", width=f\"{width}px\", \n",
    "                 bgcolor=\"#222222\", font_color=\"white\")\n",
    "    \n",
    "    # Configure physics for better layout\n",
    "    net.barnes_hut(gravity=-80000, central_gravity=0.3, spring_length=250, spring_strength=0.001)\n",
    "    \n",
    "    # Add nodes and edges from NetworkX graph\n",
    "    net.from_nx(G)\n",
    "    \n",
    "    # Set node colors based on entity type and configure nodes\n",
    "    for node in net.nodes:\n",
    "        entity_type = node[\"group\"]\n",
    "        node[\"color\"] = color_map.get(entity_type, \"#666666\")\n",
    "        node[\"size\"] = 15  # Base size\n",
    "        \n",
    "        # Adjust size based on node connectivity (larger for more connected nodes)\n",
    "        if \"physics\" not in node:\n",
    "            node[\"physics\"] = True\n",
    "    \n",
    "    # Configure edges\n",
    "    for edge in net.edges:\n",
    "        edge[\"color\"] = {\"color\": \"#ffffff\", \"opacity\": 0.5}\n",
    "        edge[\"width\"] = 1.5\n",
    "    \n",
    "    # Add legend as HTML\n",
    "    legend_html = \"\"\"\n",
    "    <div style=\"position: absolute; top: 10px; left: 10px; z-index: 1; background-color: rgba(0,0,0,0.7); padding: 10px; border-radius: 5px;\">\n",
    "        <h3 style=\"color: white; margin-bottom: 5px;\">Entity Types</h3>\n",
    "    \"\"\"\n",
    "    \n",
    "    for entity_type, color in color_map.items():\n",
    "        legend_html += f\"\"\"\n",
    "        <div style=\"margin: 5px 0;\">\n",
    "            <span style=\"display: inline-block; width: 15px; height: 15px; border-radius: 50%; background-color: {color};\"></span>\n",
    "            <span style=\"color: white; margin-left: 5px;\">{entity_type}</span>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    legend_html += \"\"\"\n",
    "    <div style=\"margin-top: 10px; color: white;\">\n",
    "        <p><b>Controls:</b><br>\n",
    "        - Scroll to zoom<br>\n",
    "        - Drag to pan<br>\n",
    "        - Click node to focus</p>\n",
    "    </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add navigation buttons\n",
    "    buttons_html = \"\"\"\n",
    "    <div style=\"position: absolute; bottom: 10px; left: 10px; z-index: 1;\">\n",
    "        <button id=\"zoom-in\" style=\"background: #444; color: white; border: none; padding: 5px 10px; margin-right: 5px; cursor: pointer;\">Zoom In</button>\n",
    "        <button id=\"zoom-out\" style=\"background: #444; color: white; border: none; padding: 5px 10px; margin-right: 5px; cursor: pointer;\">Zoom Out</button>\n",
    "        <button id=\"reset\" style=\"background: #444; color: white; border: none; padding: 5px 10px; cursor: pointer;\">Reset View</button>\n",
    "    </div>\n",
    "    \n",
    "    <script>\n",
    "        document.getElementById('zoom-in').addEventListener('click', function() {\n",
    "            network.zoomIn();\n",
    "        });\n",
    "        document.getElementById('zoom-out').addEventListener('click', function() {\n",
    "            network.zoomOut();\n",
    "        });\n",
    "        document.getElementById('reset').addEventListener('click', function() {\n",
    "            network.fit();\n",
    "        });\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Customize the visualization\n",
    "    net.set_options(\"\"\"\n",
    "    {\n",
    "      \"nodes\": {\n",
    "        \"borderWidth\": 2,\n",
    "        \"borderWidthSelected\": 4,\n",
    "        \"font\": {\n",
    "          \"size\": 12,\n",
    "          \"face\": \"Tahoma\",\n",
    "          \"strokeWidth\": 3,\n",
    "          \"strokeColor\": \"#222222\"\n",
    "        },\n",
    "        \"shadow\": true\n",
    "      },\n",
    "      \"edges\": {\n",
    "        \"arrows\": {\n",
    "          \"to\": {\n",
    "            \"enabled\": true,\n",
    "            \"scaleFactor\": 0.5\n",
    "          }\n",
    "        },\n",
    "        \"smooth\": {\n",
    "          \"enabled\": true,\n",
    "          \"type\": \"dynamic\",\n",
    "          \"roundness\": 0.5\n",
    "        },\n",
    "        \"font\": {\n",
    "          \"size\": 10,\n",
    "          \"color\": \"#ffffff\",\n",
    "          \"strokeWidth\": 3,\n",
    "          \"strokeColor\": \"#222222\"\n",
    "        }\n",
    "      },\n",
    "      \"physics\": {\n",
    "        \"stabilization\": {\n",
    "          \"iterations\": 100\n",
    "        }\n",
    "      },\n",
    "      \"interaction\": {\n",
    "        \"hover\": true,\n",
    "        \"navigationButtons\": true,\n",
    "        \"keyboard\": {\n",
    "          \"enabled\": true\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\")\n",
    "    \n",
    "    # Save the visualization to an HTML file\n",
    "    net.save_graph(output_path)\n",
    "    \n",
    "    # Add legend and buttons to the HTML file\n",
    "    with open(output_path, \"r\") as f:\n",
    "        html_content = f.read()\n",
    "    \n",
    "    # Inject legend and buttons HTML\n",
    "    modified_html = html_content.replace(\"</body>\", f\"{legend_html}{buttons_html}</body>\")\n",
    "    \n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(modified_html)\n",
    "    \n",
    "    print(f\"Interactive visualization saved to {output_path}\")\n",
    "    \n",
    "    # Display in notebook\n",
    "    return IFrame(output_path, width=width, height=height)\n",
    "\n",
    "def visualize_knowledge_graph():\n",
    "    \"\"\"Main function to visualize the knowledge graph\"\"\"\n",
    "    # Load data\n",
    "    entities, relationships = load_graph_data()\n",
    "    \n",
    "    # Create main visualization (limiting to a manageable number of nodes)\n",
    "    G, entity_types, entity_type_map = create_knowledge_graph(entities, relationships, max_nodes=200)\n",
    "    \n",
    "    # Create interactive visualization - only generating the HTML file\n",
    "    vis = visualize_with_pyvis(G, entity_types)\n",
    "    \n",
    "    # Display the interactive visualization\n",
    "    return vis\n",
    "\n",
    "# Run the visualization\n",
    "interactive_graph = visualize_knowledge_graph()\n",
    "display(interactive_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f051d659",
   "metadata": {},
   "source": [
    "# Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c0489c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ApertureDB connection parameters\n",
    "db_host = os.getenv(\"APERTUREDB_HOST\")\n",
    "db_password = os.getenv(\"APERTUREDB_PASSWORD\")\n",
    "db_user = \"admin\"\n",
    "\n",
    "# Create ApertureDB connector\n",
    "def create_db_connection():\n",
    "    return Connector.Connector(\n",
    "        host=db_host,\n",
    "        user=db_user,\n",
    "        password=db_password\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "32fc7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entities():\n",
    "    \"\"\"Load deduplicated entities from JSON file\"\"\"\n",
    "    with open(\"deduplicated_entities_2.json\", \"r\") as f:\n",
    "        entities = json.load(f)\n",
    "    return entities\n",
    "\n",
    "def ensure_indexes(client, entity_types):\n",
    "    \"\"\"Create indexes for each entity type's id property\"\"\"\n",
    "    for entity_type in entity_types:\n",
    "        # Create index for the \"id\" property for this entity type\n",
    "        query = [{\n",
    "            \"CreateIndex\": {\n",
    "                \"class_name\": entity_type,\n",
    "                \"property_name\": \"id\",\n",
    "                \"index_type\": \"integer\"  # Using integer index for ID values\n",
    "            }\n",
    "        }]\n",
    "        \n",
    "        try:\n",
    "            response, _ = client.query(query)\n",
    "            if \"CreateIndex\" in response[0] and response[0][\"CreateIndex\"][\"status\"] == 0:\n",
    "                logger.info(f\"Created index for {entity_type}.id\")\n",
    "            else:\n",
    "                logger.warning(f\"Index creation response: {response[0]}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Index for {entity_type}.id may already exist: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7fc836b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_entity(args):\n",
    "    \"\"\"\n",
    "    Process and insert a single entity instance into ApertureDB\n",
    "    \n",
    "    Args:\n",
    "        args: Tuple containing (entity_type, instance_name, properties)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Result of the operation\n",
    "    \"\"\"\n",
    "    entity_type, instance_name, properties = args\n",
    "    \n",
    "    try:\n",
    "        # Create a new connection for this thread\n",
    "        client = create_db_connection()\n",
    "        \n",
    "        # Extract the integer ID\n",
    "        entity_id = properties.get(\"id\")\n",
    "        if not entity_id:\n",
    "            return {\n",
    "                \"status\": \"error\", \n",
    "                \"entity_type\": entity_type, \n",
    "                \"name\": instance_name, \n",
    "                \"error\": \"Missing ID\"\n",
    "            }\n",
    "        \n",
    "        # Convert all property values to strings - keep the ID as an integer\n",
    "        string_properties = {}\n",
    "        for key, value in properties.items():\n",
    "            if value is not None:\n",
    "                if key == \"id\":\n",
    "                    # Keep ID as an integer\n",
    "                    string_properties[key] = int(value)\n",
    "                else:\n",
    "                    # Convert other properties to strings\n",
    "                    string_properties[key] = str(value)\n",
    "        \n",
    "        # Ensure name property is set\n",
    "        if \"name\" not in string_properties and instance_name:\n",
    "            string_properties[\"name\"] = str(instance_name)\n",
    "        \n",
    "        # Construct query for adding the entity - ID is now in properties\n",
    "        query = [{\n",
    "            \"AddEntity\": {\n",
    "                \"class\": entity_type,\n",
    "                \"properties\": string_properties,\n",
    "                \"if_not_found\": {\n",
    "                    \"id\": [\"==\", entity_id]  # Use id for deduplication\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "        \n",
    "        # Execute query\n",
    "        response, _ = client.query(query)\n",
    "        \n",
    "        # Check response\n",
    "        if \"AddEntity\" in response[0] and response[0][\"AddEntity\"][\"status\"] == 0:\n",
    "            return {\n",
    "                \"status\": \"success\", \n",
    "                \"entity_type\": entity_type, \n",
    "                \"id\": entity_id,\n",
    "                \"name\": instance_name\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"status\": \"error\", \n",
    "                \"entity_type\": entity_type, \n",
    "                \"id\": entity_id,\n",
    "                \"name\": instance_name, \n",
    "                \"error\": str(response)\n",
    "            }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"exception\", \n",
    "            \"entity_type\": entity_type, \n",
    "            \"id\": properties.get(\"id\", \"\"),\n",
    "            \"name\": instance_name, \n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "80465d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_entities_parallel(entities, max_workers=10, batch_size=100):\n",
    "    \"\"\"\n",
    "    Insert entities into ApertureDB using parallel processing\n",
    "    \n",
    "    Args:\n",
    "        entities: List of entity data from deduplicated_entities_2.json\n",
    "        max_workers: Maximum number of parallel workers\n",
    "        batch_size: Entities to process in each batch\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (success_count, error_count, total_time)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(f\"Beginning entity insertion with {max_workers} workers\")\n",
    "    \n",
    "    # Extract unique entity types\n",
    "    entity_types = [entity[\"Entity\"] for entity in entities]\n",
    "    \n",
    "    # Create main client and ensure indexes\n",
    "    main_client = create_db_connection()\n",
    "    ensure_indexes(main_client, entity_types)\n",
    "    \n",
    "    # Prepare entity instances for processing\n",
    "    entity_instances = []\n",
    "    for entity_data in entities:\n",
    "        entity_type = entity_data[\"Entity\"]\n",
    "        instances = entity_data[\"Instances\"]\n",
    "        \n",
    "        for instance_name, properties in instances.items():\n",
    "            entity_instances.append((entity_type, instance_name, properties))\n",
    "    \n",
    "    total_entities = len(entity_instances)\n",
    "    logger.info(f\"Found {total_entities} entities to insert\")\n",
    "    \n",
    "    # Process entities in batches to avoid overwhelming the system\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Create a map to track UUID to ApertureDB _uniqueid mapping\n",
    "    uuid_to_uniqueid_map = {}\n",
    "    \n",
    "    with tqdm(total=total_entities, desc=\"Inserting entities\") as progress:\n",
    "        for i in range(0, total_entities, batch_size):\n",
    "            batch = entity_instances[i:i+batch_size]\n",
    "            \n",
    "            # Process batch with parallel workers\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                results = list(executor.map(process_entity, batch))\n",
    "            \n",
    "            # Update counts\n",
    "            batch_success = sum(1 for r in results if r[\"status\"] == \"success\")\n",
    "            batch_errors = len(batch) - batch_success\n",
    "            \n",
    "            success_count += batch_success\n",
    "            error_count += batch_errors\n",
    "            \n",
    "            # Update progress\n",
    "            progress.update(len(batch))\n",
    "            \n",
    "            # Log errors if any\n",
    "            if batch_errors > 0:\n",
    "                errors = [r for r in results if r[\"status\"] != \"success\"]\n",
    "                for error in errors[:5]:  # Show first 5 errors\n",
    "                    logger.warning(f\"Error: {error['entity_type']} - {error['name']}: {error.get('error', 'Unknown error')}\")\n",
    "                \n",
    "                if len(errors) > 5:\n",
    "                    logger.warning(f\"... and {len(errors) - 5} more errors\")\n",
    "            \n",
    "            logger.info(f\"Processed {min(i+batch_size, total_entities)}/{total_entities} entities - Success: {batch_success}, Errors: {batch_errors}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    logger.info(f\"Entity insertion completed in {total_time:.2f} seconds\")\n",
    "    logger.info(f\"Total: {total_entities}, Success: {success_count}, Errors: {error_count}\")\n",
    "    \n",
    "    # Save the UUID mapping file for later use in creating connections\n",
    "    with open(\"uuid_mapping.json\", \"w\") as f:\n",
    "        json.dump(uuid_to_uniqueid_map, f, indent=2)\n",
    "    \n",
    "    return success_count, error_count, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fb709f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_entity_insertion():\n",
    "    \"\"\"Verify that entities were properly inserted by checking counts\"\"\"\n",
    "    client = create_db_connection()\n",
    "    \n",
    "    # Load original entities for comparison\n",
    "    entities = load_entities()\n",
    "    \n",
    "    # Check a few entity types\n",
    "    check_types = [entities[i][\"Entity\"] for i in range(min(3, len(entities)))]\n",
    "    \n",
    "    print(\"\\n----- Verification -----\")\n",
    "    for entity_type in check_types:\n",
    "        # Count entities of this type in ApertureDB\n",
    "        query = [{\n",
    "            \"FindEntity\": {\n",
    "                \"with_class\": entity_type,\n",
    "                \"results\": {\n",
    "                    \"count\": True,\n",
    "                    \"list\": [\"id\"]  # Retrieve id values to verify they exist\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "        \n",
    "        try:\n",
    "            response, _ = client.query(query)\n",
    "            if \"FindEntity\" in response[0] and response[0][\"FindEntity\"][\"status\"] == 0:\n",
    "                count = response[0][\"FindEntity\"].get(\"count\", 0)\n",
    "                print(f\"{entity_type}: {count} entities found in database\")\n",
    "                \n",
    "                # If we got entities back, check a few id values\n",
    "                if \"entities\" in response[0][\"FindEntity\"] and len(response[0][\"FindEntity\"][\"entities\"]) > 0:\n",
    "                    sample = response[0][\"FindEntity\"][\"entities\"][:2]  # Show first 2 entities\n",
    "                    print(f\"  Sample id values: {[e.get('properties', {}).get('id', 'missing') for e in sample]}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"{entity_type}: Failed to get count - {response[0]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{entity_type}: Error during verification - {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d6ac6ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_step5():\n",
    "    \"\"\"Execute Step 5: Insert all entities into ApertureDB\"\"\"\n",
    "    logger.info(\"Starting Step 5: Entity insertion into ApertureDB\")\n",
    "    \n",
    "    # Load entities from JSON\n",
    "    entities = load_entities()\n",
    "    if not entities:\n",
    "        logger.error(\"Failed to load entities from deduplicated_entities_2.json\")\n",
    "        return\n",
    "    \n",
    "    # Count total entities for reporting\n",
    "    total_instances = sum(len(entity[\"Instances\"]) for entity in entities)\n",
    "    logger.info(f\"Loaded {total_instances} entities across {len(entities)} entity types\")\n",
    "    \n",
    "    # Insert entities in parallel\n",
    "    success, errors, duration = insert_entities_parallel(\n",
    "        entities,\n",
    "        max_workers=10,  # Adjust based on your system capabilities and API limits\n",
    "        batch_size=50    # Adjust based on entity complexity and API limits\n",
    "    )\n",
    "    \n",
    "    # Report results\n",
    "    print(\"\\n----- Entity Insertion Results -----\")\n",
    "    print(f\"Total entities: {total_instances}\")\n",
    "    print(f\"Successfully inserted: {success}\")\n",
    "    print(f\"Errors: {errors}\")\n",
    "    print(f\"Total time: {duration:.2f} seconds\")\n",
    "    \n",
    "    # Verify some data was inserted by checking a few entity types\n",
    "    if success > 0:\n",
    "        verify_entity_insertion()\n",
    "    \n",
    "    return success, errors, duration\n",
    "\n",
    "def verify_entity_insertion():\n",
    "    \"\"\"Verify that entities were properly inserted by checking counts\"\"\"\n",
    "    client = create_db_connection()\n",
    "    \n",
    "    # Load original entities for comparison\n",
    "    entities = load_entities()\n",
    "    \n",
    "    # Check a few entity types\n",
    "    check_types = [entities[i][\"Entity\"] for i in range(min(3, len(entities)))]\n",
    "    \n",
    "    print(\"\\n----- Verification -----\")\n",
    "    for entity_type in check_types:\n",
    "        # Count entities of this type in ApertureDB\n",
    "        query = [{\n",
    "            \"FindEntity\": {\n",
    "                \"with_class\": entity_type,\n",
    "                \"results\": {\n",
    "                    \"count\": True,\n",
    "                    \"list\": [\"_ref\"]  # Also retrieve _ref values to verify they exist\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "        \n",
    "        try:\n",
    "            response, _ = client.query(query)\n",
    "            if \"FindEntity\" in response[0] and response[0][\"FindEntity\"][\"status\"] == 0:\n",
    "                count = response[0][\"FindEntity\"].get(\"count\", 0)\n",
    "                print(f\"{entity_type}: {count} entities found in database\")\n",
    "                \n",
    "                # If we got entities back, check a few _ref values\n",
    "                if \"entities\" in response[0][\"FindEntity\"] and len(response[0][\"FindEntity\"][\"entities\"]) > 0:\n",
    "                    sample = response[0][\"FindEntity\"][\"entities\"][:2]  # Show first 2 entities\n",
    "                    print(f\"  Sample _ref values: {[e.get('_ref', 'missing') for e in sample]}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"{entity_type}: Failed to get count - {response[0]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{entity_type}: Error during verification - {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0e281440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 14:02:41,776 - INFO - Starting Step 5: Entity insertion into ApertureDB\n",
      "2025-05-21 14:02:41,780 - INFO - Loaded 538 entities across 12 entity types\n",
      "2025-05-21 14:02:41,782 - INFO - Beginning entity insertion with 10 workers\n",
      "2025-05-21 14:02:44,019 - WARNING - Index for Computing Concept.id may already exist: 0\n",
      "2025-05-21 14:02:44,242 - WARNING - Index for System Architecture.id may already exist: 0\n",
      "2025-05-21 14:02:44,465 - WARNING - Index for Resource.id may already exist: 0\n",
      "2025-05-21 14:02:44,694 - WARNING - Index for Storage Type.id may already exist: 0\n",
      "2025-05-21 14:02:44,928 - WARNING - Index for Network Entity.id may already exist: 0\n",
      "2025-05-21 14:02:45,164 - WARNING - Index for Role.id may already exist: 0\n",
      "2025-05-21 14:02:45,390 - WARNING - Index for Organization.id may already exist: 0\n",
      "2025-05-21 14:02:45,616 - WARNING - Index for Software Component.id may already exist: 0\n",
      "2025-05-21 14:02:45,839 - WARNING - Index for Platform.id may already exist: 0\n",
      "2025-05-21 14:02:46,065 - WARNING - Index for Service Model.id may already exist: 0\n",
      "2025-05-21 14:02:46,289 - WARNING - Index for Deployment Model.id may already exist: 0\n",
      "2025-05-21 14:02:46,516 - WARNING - Index for Database Type.id may already exist: 0\n",
      "2025-05-21 14:02:46,519 - INFO - Found 538 entities to insert\n",
      "Inserting entities:   9%|▉         | 50/538 [00:12<02:01,  4.03it/s]2025-05-21 14:02:58,948 - INFO - Processed 50/538 entities - Success: 50, Errors: 0\n",
      "Inserting entities:  19%|█▊        | 100/538 [00:28<02:07,  3.45it/s]2025-05-21 14:03:14,934 - INFO - Processed 100/538 entities - Success: 50, Errors: 0\n",
      "Inserting entities:  28%|██▊       | 150/538 [00:41<01:48,  3.58it/s]2025-05-21 14:03:28,208 - INFO - Processed 150/538 entities - Success: 50, Errors: 0\n",
      "Inserting entities:  37%|███▋      | 200/538 [00:54<01:30,  3.72it/s]2025-05-21 14:03:40,837 - INFO - Processed 200/538 entities - Success: 50, Errors: 0\n",
      "Inserting entities:  46%|████▋     | 250/538 [01:06<01:15,  3.83it/s]2025-05-21 14:03:53,238 - INFO - Processed 250/538 entities - Success: 50, Errors: 0\n",
      "Inserting entities:  56%|█████▌    | 300/538 [01:18<01:00,  3.96it/s]2025-05-21 14:04:05,001 - INFO - Processed 300/538 entities - Success: 50, Errors: 0\n",
      "Inserting entities:  65%|██████▌   | 350/538 [01:30<00:46,  4.03it/s]2025-05-21 14:04:16,951 - INFO - Processed 350/538 entities - Success: 50, Errors: 0\n",
      "Inserting entities:  74%|███████▍  | 400/538 [01:41<00:33,  4.13it/s]2025-05-21 14:04:28,422 - INFO - Processed 400/538 entities - Success: 50, Errors: 0\n",
      "Inserting entities:  84%|████████▎ | 450/538 [01:53<00:21,  4.17it/s]2025-05-21 14:04:40,174 - INFO - Processed 450/538 entities - Success: 50, Errors: 0\n",
      "Inserting entities:  93%|█████████▎| 500/538 [02:05<00:09,  4.15it/s]2025-05-21 14:04:52,335 - INFO - Processed 500/538 entities - Success: 50, Errors: 0\n",
      "Inserting entities: 100%|██████████| 538/538 [02:16<00:00,  4.01it/s]2025-05-21 14:05:02,759 - INFO - Processed 538/538 entities - Success: 38, Errors: 0\n",
      "Inserting entities: 100%|██████████| 538/538 [02:16<00:00,  3.95it/s]\n",
      "2025-05-21 14:05:02,763 - INFO - Entity insertion completed in 140.98 seconds\n",
      "2025-05-21 14:05:02,764 - INFO - Total: 538, Success: 538, Errors: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Entity Insertion Results -----\n",
      "Total entities: 538\n",
      "Successfully inserted: 538\n",
      "Errors: 0\n",
      "Total time: 140.98 seconds\n",
      "\n",
      "----- Verification -----\n",
      "Computing Concept: 157 entities found in database\n",
      "  Sample _ref values: [None, None]\n",
      "System Architecture: 31 entities found in database\n",
      "  Sample _ref values: [None, None]\n",
      "Resource: 92 entities found in database\n",
      "  Sample _ref values: [None, None]\n"
     ]
    }
   ],
   "source": [
    "# Execute Step 5\n",
    "success, errors, duration = process_step5()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d693f11",
   "metadata": {},
   "source": [
    "# Step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa140f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 14:50:39,113 - INFO - Starting Step 5.2: Relationship creation in ApertureDB\n",
      "2025-05-21 14:50:39,128 - INFO - Beginning relationship creation with 10 workers\n",
      "2025-05-21 14:50:39,131 - INFO - Found 1170 relationships to create\n",
      "Creating relationships:   0%|          | 0/1170 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 14:50:42,087 - INFO - Processed relationship 1/1170 (0.1%)\n",
      "2025-05-21 14:50:43,983 - INFO - Processed relationship 11/1170 (0.9%)\n",
      "Creating relationships:   2%|▏         | 20/1170 [00:05<05:05,  3.77it/s]2025-05-21 14:50:46,848 - INFO - Processed relationship 21/1170 (1.8%)\n",
      "2025-05-21 14:50:48,965 - INFO - Processed relationship 31/1170 (2.6%)\n",
      "Creating relationships:   3%|▎         | 40/1170 [00:10<04:50,  3.89it/s]2025-05-21 14:50:52,165 - INFO - Processed relationship 41/1170 (3.5%)\n",
      "2025-05-21 14:50:54,059 - INFO - Processed relationship 51/1170 (4.4%)\n",
      "Creating relationships:   5%|▌         | 60/1170 [00:15<04:41,  3.94it/s]2025-05-21 14:50:56,867 - INFO - Processed relationship 61/1170 (5.2%)\n",
      "2025-05-21 14:50:59,063 - INFO - Processed relationship 71/1170 (6.1%)\n",
      "Creating relationships:   7%|▋         | 80/1170 [00:20<04:36,  3.95it/s]2025-05-21 14:51:01,967 - INFO - Processed relationship 81/1170 (6.9%)\n",
      "2025-05-21 14:51:04,129 - INFO - Processed relationship 91/1170 (7.8%)\n",
      "Creating relationships:   9%|▊         | 100/1170 [00:25<04:32,  3.93it/s]2025-05-21 14:51:04,641 - WARNING - Error: is_a_type_of - Software Component:198 → Software Component:411: Failed to create connection: {'source_found': False, 'dest_found': True, 'connection_created': True}\n",
      "2025-05-21 14:51:06,963 - INFO - Processed relationship 101/1170 (8.6%)\n",
      "2025-05-21 14:51:09,173 - INFO - Processed relationship 111/1170 (9.5%)\n",
      "Creating relationships:  10%|█         | 120/1170 [00:30<04:25,  3.95it/s]2025-05-21 14:51:09,656 - WARNING - Error: is_a_type_of - Software Component:209 → Software Component:411: Failed to create connection: {'source_found': False, 'dest_found': True, 'connection_created': True}\n",
      "2025-05-21 14:51:11,966 - INFO - Processed relationship 121/1170 (10.3%)\n",
      "2025-05-21 14:51:14,140 - INFO - Processed relationship 131/1170 (11.2%)\n",
      "Creating relationships:  12%|█▏        | 140/1170 [00:35<04:19,  3.96it/s]2025-05-21 14:51:14,669 - WARNING - Error: is_a_type_of - Role:244 → Resource:246: Failed to create connection: {'source_found': False, 'dest_found': True, 'connection_created': True}\n",
      "2025-05-21 14:51:14,672 - WARNING - Error: used_for - Resource:273 → Computing Concept:274: Failed to create connection: {'source_found': True, 'dest_found': False, 'connection_created': True}\n",
      "2025-05-21 14:51:14,675 - WARNING - Error: part_of - Computing Concept:274 → Computing Concept:135: Failed to create connection: {'source_found': False, 'dest_found': True, 'connection_created': True}\n",
      "2025-05-21 14:51:17,128 - INFO - Processed relationship 141/1170 (12.1%)\n",
      "2025-05-21 14:51:19,171 - INFO - Processed relationship 151/1170 (12.9%)\n",
      "Creating relationships:  14%|█▎        | 160/1170 [00:40<04:13,  3.99it/s]2025-05-21 14:51:22,083 - INFO - Processed relationship 161/1170 (13.8%)\n",
      "2025-05-21 14:51:24,130 - INFO - Processed relationship 171/1170 (14.6%)\n",
      "Creating relationships:  15%|█▌        | 180/1170 [00:45<04:07,  4.00it/s]2025-05-21 14:51:24,580 - WARNING - Error: manages - Computing Concept:342 → Network Entity:311: Failed to create connection: {'source_found': False, 'dest_found': True, 'connection_created': True}\n",
      "2025-05-21 14:51:24,585 - WARNING - Error: represents - Network Entity:349 → Computing Concept:350: Failed to create connection: {'source_found': True, 'dest_found': False, 'connection_created': True}\n",
      "2025-05-21 14:51:26,947 - INFO - Processed relationship 181/1170 (15.5%)\n",
      "2025-05-21 14:51:29,209 - INFO - Processed relationship 191/1170 (16.3%)\n",
      "Creating relationships:  17%|█▋        | 200/1170 [00:50<04:03,  3.99it/s]2025-05-21 14:51:32,367 - INFO - Processed relationship 201/1170 (17.2%)\n",
      "2025-05-21 14:51:34,419 - INFO - Processed relationship 211/1170 (18.0%)\n",
      "Creating relationships:  19%|█▉        | 220/1170 [00:55<04:00,  3.95it/s]2025-05-21 14:51:37,119 - INFO - Processed relationship 221/1170 (18.9%)\n",
      "2025-05-21 14:51:39,358 - INFO - Processed relationship 231/1170 (19.7%)\n",
      "Creating relationships:  21%|██        | 240/1170 [01:00<03:54,  3.97it/s]2025-05-21 14:51:42,171 - INFO - Processed relationship 241/1170 (20.6%)\n",
      "2025-05-21 14:51:44,298 - INFO - Processed relationship 251/1170 (21.5%)\n",
      "Creating relationships:  22%|██▏       | 260/1170 [01:05<03:48,  3.98it/s]2025-05-21 14:51:47,568 - INFO - Processed relationship 261/1170 (22.3%)\n",
      "2025-05-21 14:51:49,335 - INFO - Processed relationship 271/1170 (23.2%)\n",
      "Creating relationships:  24%|██▍       | 280/1170 [01:11<03:48,  3.90it/s]2025-05-21 14:51:52,703 - INFO - Processed relationship 281/1170 (24.0%)\n",
      "2025-05-21 14:51:54,821 - INFO - Processed relationship 291/1170 (24.9%)\n",
      "Creating relationships:  26%|██▌       | 300/1170 [01:18<04:11,  3.46it/s]2025-05-21 14:51:59,671 - INFO - Processed relationship 301/1170 (25.7%)\n",
      "2025-05-21 14:52:01,935 - INFO - Processed relationship 311/1170 (26.6%)\n",
      "Creating relationships:  27%|██▋       | 320/1170 [01:24<04:12,  3.37it/s]2025-05-21 14:52:06,014 - INFO - Processed relationship 321/1170 (27.4%)\n",
      "2025-05-21 14:52:09,714 - INFO - Processed relationship 331/1170 (28.3%)\n",
      "Creating relationships:  29%|██▉       | 340/1170 [01:31<04:21,  3.17it/s]2025-05-21 14:52:15,488 - INFO - Processed relationship 351/1170 (30.0%)\n",
      "2025-05-21 14:52:17,146 - INFO - Processed relationship 341/1170 (29.1%)\n",
      "Creating relationships:  31%|███       | 360/1170 [01:39<04:38,  2.91it/s]2025-05-21 14:52:21,911 - INFO - Processed relationship 361/1170 (30.9%)\n",
      "2025-05-21 14:52:23,981 - INFO - Processed relationship 371/1170 (31.7%)\n",
      "Creating relationships:  32%|███▏      | 380/1170 [01:47<04:39,  2.82it/s]2025-05-21 14:52:29,061 - INFO - Processed relationship 381/1170 (32.6%)\n",
      "2025-05-21 14:52:34,442 - INFO - Processed relationship 391/1170 (33.4%)\n",
      "Creating relationships:  34%|███▍      | 400/1170 [01:56<04:53,  2.62it/s]2025-05-21 14:52:39,374 - INFO - Processed relationship 401/1170 (34.3%)\n",
      "2025-05-21 14:52:40,062 - INFO - Processed relationship 411/1170 (35.1%)\n",
      "Creating relationships:  36%|███▌      | 420/1170 [02:04<04:45,  2.63it/s]2025-05-21 14:52:43,179 - WARNING - Error: involves_user_handling_in_iaas - Service Model:506 → Network Entity:149: Failed to create connection: {'source_found': True, 'dest_found': False, 'connection_created': True}\n",
      "2025-05-21 14:52:46,764 - INFO - Processed relationship 421/1170 (36.0%)\n",
      "2025-05-21 14:52:47,920 - INFO - Processed relationship 431/1170 (36.8%)\n",
      "Creating relationships:  38%|███▊      | 440/1170 [02:12<04:52,  2.49it/s]2025-05-21 14:52:54,354 - INFO - Processed relationship 441/1170 (37.7%)\n",
      "2025-05-21 14:52:56,622 - INFO - Processed relationship 451/1170 (38.5%)\n",
      "Creating relationships:  39%|███▉      | 460/1170 [02:21<04:45,  2.49it/s]2025-05-21 14:53:00,241 - WARNING - Error: involves_providers - Computing Concept:522 → Role:359: Failed to create connection: {'source_found': False, 'dest_found': True, 'connection_created': True}\n",
      "2025-05-21 14:53:00,242 - WARNING - Error: helps_avoid - Computing Concept:522 → Computing Concept:116: Failed to create connection: {'source_found': False, 'dest_found': True, 'connection_created': True}\n",
      "2025-05-21 14:53:00,243 - WARNING - Error: uses_services_from - Computing Concept:522 → Role:359: Failed to create connection: {'source_found': False, 'dest_found': True, 'connection_created': True}\n",
      "2025-05-21 14:53:03,459 - INFO - Processed relationship 461/1170 (39.4%)\n",
      "2025-05-21 14:53:04,721 - INFO - Processed relationship 471/1170 (40.3%)\n",
      "Creating relationships:  41%|████      | 480/1170 [02:29<04:42,  2.45it/s]2025-05-21 14:53:12,011 - INFO - Processed relationship 481/1170 (41.1%)\n",
      "2025-05-21 14:53:13,218 - INFO - Processed relationship 491/1170 (42.0%)\n",
      "Creating relationships:  43%|████▎     | 500/1170 [02:37<04:31,  2.47it/s]2025-05-21 14:53:19,179 - INFO - Processed relationship 501/1170 (42.8%)\n",
      "2025-05-21 14:53:21,324 - INFO - Processed relationship 511/1170 (43.7%)\n",
      "Creating relationships:  44%|████▍     | 520/1170 [02:48<04:50,  2.23it/s]2025-05-21 14:53:31,312 - INFO - Processed relationship 521/1170 (44.5%)\n",
      "2025-05-21 14:53:35,000 - INFO - Processed relationship 531/1170 (45.4%)\n",
      "Creating relationships:  46%|████▌     | 540/1170 [02:55<04:27,  2.35it/s]2025-05-21 14:53:38,512 - INFO - Processed relationship 541/1170 (46.2%)\n",
      "2025-05-21 14:53:40,518 - INFO - Processed relationship 551/1170 (47.1%)\n",
      "Creating relationships:  48%|████▊     | 560/1170 [03:03<04:12,  2.42it/s]2025-05-21 14:53:45,154 - INFO - Processed relationship 561/1170 (47.9%)\n",
      "2025-05-21 14:53:47,258 - INFO - Processed relationship 571/1170 (48.8%)\n",
      "Creating relationships:  50%|████▉     | 580/1170 [03:09<03:44,  2.63it/s]2025-05-21 14:53:51,471 - INFO - Processed relationship 581/1170 (49.7%)\n",
      "2025-05-21 14:53:53,353 - INFO - Processed relationship 591/1170 (50.5%)\n",
      "Creating relationships:  51%|█████▏    | 600/1170 [03:19<03:59,  2.38it/s]2025-05-21 14:54:04,105 - INFO - Processed relationship 601/1170 (51.4%)\n",
      "2025-05-21 14:54:04,207 - INFO - Processed relationship 611/1170 (52.2%)\n",
      "Creating relationships:  53%|█████▎    | 620/1170 [03:27<03:41,  2.48it/s]2025-05-21 14:54:08,857 - INFO - Processed relationship 621/1170 (53.1%)\n",
      "2025-05-21 14:54:10,872 - INFO - Processed relationship 631/1170 (53.9%)\n",
      "Creating relationships:  55%|█████▍    | 640/1170 [03:32<03:09,  2.79it/s]2025-05-21 14:54:14,638 - INFO - Processed relationship 641/1170 (54.8%)\n",
      "2025-05-21 14:54:15,884 - INFO - Processed relationship 651/1170 (55.6%)\n",
      "Creating relationships:  56%|█████▋    | 660/1170 [03:39<02:59,  2.84it/s]2025-05-21 14:54:20,843 - INFO - Processed relationship 661/1170 (56.5%)\n",
      "2025-05-21 14:54:22,797 - INFO - Processed relationship 671/1170 (57.4%)\n",
      "Creating relationships:  58%|█████▊    | 680/1170 [03:46<02:59,  2.73it/s]2025-05-21 14:54:28,968 - INFO - Processed relationship 681/1170 (58.2%)\n",
      "2025-05-21 14:54:31,165 - INFO - Processed relationship 691/1170 (59.1%)\n",
      "Creating relationships:  60%|█████▉    | 700/1170 [03:53<02:48,  2.79it/s]2025-05-21 14:54:35,548 - INFO - Processed relationship 701/1170 (59.9%)\n",
      "2025-05-21 14:54:37,990 - INFO - Processed relationship 711/1170 (60.8%)\n",
      "Creating relationships:  62%|██████▏   | 720/1170 [04:00<02:36,  2.87it/s]2025-05-21 14:54:41,979 - INFO - Processed relationship 721/1170 (61.6%)\n",
      "2025-05-21 14:54:43,982 - INFO - Processed relationship 731/1170 (62.5%)\n",
      "Creating relationships:  63%|██████▎   | 740/1170 [04:05<02:18,  3.11it/s]2025-05-21 14:54:47,234 - INFO - Processed relationship 741/1170 (63.3%)\n",
      "2025-05-21 14:54:49,167 - INFO - Processed relationship 751/1170 (64.2%)\n",
      "Creating relationships:  65%|██████▍   | 760/1170 [04:10<02:04,  3.30it/s]2025-05-21 14:54:52,233 - INFO - Processed relationship 761/1170 (65.0%)\n",
      "2025-05-21 14:54:54,353 - INFO - Processed relationship 771/1170 (65.9%)\n",
      "Creating relationships:  67%|██████▋   | 780/1170 [04:16<01:59,  3.27it/s]2025-05-21 14:54:58,645 - INFO - Processed relationship 781/1170 (66.8%)\n",
      "2025-05-21 14:55:00,625 - INFO - Processed relationship 791/1170 (67.6%)\n",
      "Creating relationships:  68%|██████▊   | 800/1170 [04:21<01:46,  3.46it/s]2025-05-21 14:55:04,263 - INFO - Processed relationship 801/1170 (68.5%)\n",
      "2025-05-21 14:55:05,646 - INFO - Processed relationship 811/1170 (69.3%)\n",
      "Creating relationships:  70%|███████   | 820/1170 [04:27<01:39,  3.52it/s]2025-05-21 14:55:08,761 - INFO - Processed relationship 821/1170 (70.2%)\n",
      "2025-05-21 14:55:10,996 - INFO - Processed relationship 831/1170 (71.0%)\n",
      "Creating relationships:  72%|███████▏  | 840/1170 [04:32<01:30,  3.66it/s]2025-05-21 14:55:14,006 - INFO - Processed relationship 841/1170 (71.9%)\n",
      "2025-05-21 14:55:16,073 - INFO - Processed relationship 851/1170 (72.7%)\n",
      "Creating relationships:  74%|███████▎  | 860/1170 [04:37<01:22,  3.75it/s]2025-05-21 14:55:19,083 - INFO - Processed relationship 861/1170 (73.6%)\n",
      "2025-05-21 14:55:22,341 - INFO - Processed relationship 871/1170 (74.4%)\n",
      "Creating relationships:  75%|███████▌  | 880/1170 [04:43<01:20,  3.58it/s]2025-05-21 14:55:25,110 - INFO - Processed relationship 881/1170 (75.3%)\n",
      "2025-05-21 14:55:27,208 - INFO - Processed relationship 891/1170 (76.2%)\n",
      "Creating relationships:  77%|███████▋  | 900/1170 [04:48<01:14,  3.64it/s]2025-05-21 14:55:30,712 - INFO - Processed relationship 901/1170 (77.0%)\n",
      "2025-05-21 14:55:32,476 - INFO - Processed relationship 911/1170 (77.9%)\n",
      "Creating relationships:  79%|███████▊  | 920/1170 [04:54<01:09,  3.59it/s]2025-05-21 14:55:36,269 - INFO - Processed relationship 921/1170 (78.7%)\n",
      "2025-05-21 14:55:38,240 - INFO - Processed relationship 931/1170 (79.6%)\n",
      "Creating relationships:  80%|████████  | 940/1170 [05:00<01:06,  3.48it/s]2025-05-21 14:55:39,886 - WARNING - Error: has_assigned - Network Entity:326 → Resource:324: Failed to create connection: {'source_found': True, 'dest_found': False, 'connection_created': True}\n",
      "2025-05-21 14:55:42,415 - INFO - Processed relationship 941/1170 (80.4%)\n",
      "2025-05-21 14:55:44,609 - INFO - Processed relationship 951/1170 (81.3%)\n",
      "Creating relationships:  82%|████████▏ | 960/1170 [05:05<00:58,  3.59it/s]2025-05-21 14:55:48,067 - INFO - Processed relationship 961/1170 (82.1%)\n",
      "2025-05-21 14:55:50,236 - INFO - Processed relationship 971/1170 (83.0%)\n",
      "Creating relationships:  84%|████████▍ | 980/1170 [05:13<00:58,  3.23it/s]2025-05-21 14:55:54,948 - INFO - Processed relationship 981/1170 (83.8%)\n",
      "2025-05-21 14:55:57,156 - INFO - Processed relationship 991/1170 (84.7%)\n",
      "Creating relationships:  85%|████████▌ | 1000/1170 [05:18<00:49,  3.44it/s]2025-05-21 14:56:02,031 - INFO - Processed relationship 1011/1170 (86.4%)\n",
      "2025-05-21 14:56:02,665 - INFO - Processed relationship 1001/1170 (85.6%)\n",
      "Creating relationships:  87%|████████▋ | 1020/1170 [05:25<00:47,  3.18it/s]2025-05-21 14:56:07,573 - INFO - Processed relationship 1021/1170 (87.3%)\n",
      "2025-05-21 14:56:09,512 - INFO - Processed relationship 1031/1170 (88.1%)\n",
      "Creating relationships:  89%|████████▉ | 1040/1170 [05:31<00:38,  3.36it/s]2025-05-21 14:56:12,661 - INFO - Processed relationship 1041/1170 (89.0%)\n",
      "2025-05-21 14:56:14,722 - INFO - Processed relationship 1051/1170 (89.8%)\n",
      "Creating relationships:  91%|█████████ | 1060/1170 [05:35<00:31,  3.53it/s]2025-05-21 14:56:18,100 - INFO - Processed relationship 1061/1170 (90.7%)\n",
      "2025-05-21 14:56:20,183 - INFO - Processed relationship 1071/1170 (91.5%)\n",
      "Creating relationships:  92%|█████████▏| 1080/1170 [05:41<00:25,  3.49it/s]2025-05-21 14:56:23,259 - INFO - Processed relationship 1081/1170 (92.4%)\n",
      "2025-05-21 14:56:25,581 - INFO - Processed relationship 1091/1170 (93.2%)\n",
      "Creating relationships:  94%|█████████▍| 1100/1170 [05:46<00:19,  3.64it/s]2025-05-21 14:56:29,209 - INFO - Processed relationship 1101/1170 (94.1%)\n",
      "2025-05-21 14:56:30,660 - INFO - Processed relationship 1111/1170 (95.0%)\n",
      "Creating relationships:  96%|█████████▌| 1120/1170 [05:52<00:13,  3.63it/s]2025-05-21 14:56:33,793 - INFO - Processed relationship 1121/1170 (95.8%)\n",
      "2025-05-21 14:56:36,134 - INFO - Processed relationship 1131/1170 (96.7%)\n",
      "Creating relationships:  97%|█████████▋| 1140/1170 [05:57<00:08,  3.66it/s]2025-05-21 14:56:36,896 - WARNING - Error: component_of - Database Type:465 → Platform:492: Failed to create connection: {'source_found': False, 'dest_found': True, 'connection_created': True}\n",
      "2025-05-21 14:56:36,899 - WARNING - Error: stores_metadata_of - Database Type:465 → Deployment Model:531: Failed to create connection: {'source_found': False, 'dest_found': True, 'connection_created': True}\n",
      "2025-05-21 14:56:36,909 - WARNING - Error: is_not_a - Database Type:465 → Service Model:513: Failed to create connection: {'source_found': False, 'dest_found': True, 'connection_created': True}\n",
      "2025-05-21 14:56:39,224 - INFO - Processed relationship 1141/1170 (97.5%)\n",
      "2025-05-21 14:56:41,400 - INFO - Processed relationship 1151/1170 (98.4%)\n",
      "Creating relationships:  99%|█████████▉| 1160/1170 [06:02<00:02,  3.75it/s]2025-05-21 14:56:41,923 - WARNING - Error: mandatory_for - Service Model:458 → Platform:492: Failed to create connection: {'source_found': False, 'dest_found': True, 'connection_created': True}\n",
      "2025-05-21 14:56:44,197 - INFO - Processed relationship 1170/1170 (100.0%)\n",
      "2025-05-21 14:56:44,365 - INFO - Processed relationship 1161/1170 (99.2%)\n",
      "Creating relationships: 100%|██████████| 1170/1170 [06:05<00:00,  3.20it/s]\n",
      "2025-05-21 14:56:44,608 - INFO - Relationship creation completed in 365.48 seconds\n",
      "2025-05-21 14:56:44,609 - INFO - Total: 1170, Success: 1154, Errors: 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Relationship Creation Results -----\n",
      "Total relationships: 1170\n",
      "Successfully created: 1154\n",
      "Errors: 16\n",
      "Total time: 365.48 seconds\n",
      "\n",
      "----- Relationship Verification -----\n",
      "Relationship 'depicts': 3 connections found in database\n",
      "Relationship 'helps_with': 1 connections found in database\n",
      "Relationship 'allows_to_operate': 1 connections found in database\n",
      "Relationship 'enforces_for': 2 connections found in database\n",
      "Relationship 'utilized_for': 1 connections found in database\n"
     ]
    }
   ],
   "source": [
    "def process_relationship(args):\n",
    "    \"\"\"\n",
    "    Process and create a single relationship between entities in ApertureDB\n",
    "    \n",
    "    Args:\n",
    "        args: Tuple containing (relationship_data, total_relationships, index)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Result of the operation\n",
    "    \"\"\"\n",
    "    relationship, total_relationships, index = args\n",
    "    \n",
    "    try:\n",
    "        # Create a connection for this thread\n",
    "        client = create_db_connection()\n",
    "        \n",
    "        # Extract relationship information\n",
    "        rel_type = relationship[\"relationship\"]\n",
    "        source_entity_type = relationship[\"source\"][\"entity\"]\n",
    "        source_entity_id = relationship[\"source\"][\"id\"]\n",
    "        dest_entity_type = relationship[\"destination\"][\"entity\"]\n",
    "        dest_entity_id = relationship[\"destination\"][\"id\"]\n",
    "        \n",
    "        # Create a query that:\n",
    "        # 1. Finds the source entity by class and ID\n",
    "        # 2. Finds the destination entity by class and ID\n",
    "        # 3. Creates a connection between them with relationship type as class\n",
    "        query = [\n",
    "            # Find source entity and assign temporary reference 1\n",
    "            {\n",
    "                \"FindEntity\": {\n",
    "                    \"with_class\": source_entity_type,\n",
    "                    \"constraints\": {\n",
    "                        \"id\": [\"==\", source_entity_id]\n",
    "                    },\n",
    "                    \"_ref\": 1,  # Assign temporary reference 1\n",
    "                    \"results\": {\n",
    "                        \"count\": True,\n",
    "                        \"list\": [\"id\"]  # Just need minimal data\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            # Find destination entity and assign temporary reference 2\n",
    "            {\n",
    "                \"FindEntity\": {\n",
    "                    \"with_class\": dest_entity_type,\n",
    "                    \"constraints\": {\n",
    "                        \"id\": [\"==\", dest_entity_id]\n",
    "                    },\n",
    "                    \"_ref\": 2,  # Assign temporary reference 2\n",
    "                    \"results\": {\n",
    "                        \"count\": True,\n",
    "                        \"list\": [\"id\"]  # Just need minimal data\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            # Create connection from source to destination\n",
    "            {\n",
    "                \"AddConnection\": {\n",
    "                    \"class\": rel_type,  # Use relationship type as connection class\n",
    "                    \"src\": 1,  # Reference to source entity\n",
    "                    \"dst\": 2,  # Reference to destination entity\n",
    "                    # Optional properties for the connection could be added here\n",
    "                    \"properties\": {\n",
    "                        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Execute the query\n",
    "        response, _ = client.query(query)\n",
    "        \n",
    "        # Check for success\n",
    "        source_found = False\n",
    "        dest_found = False\n",
    "        connection_created = False\n",
    "        \n",
    "        if len(response) >= 3:\n",
    "            # Check if source entity was found\n",
    "            if \"FindEntity\" in response[0] and response[0][\"FindEntity\"][\"status\"] == 0:\n",
    "                source_count = response[0][\"FindEntity\"].get(\"count\", 0)\n",
    "                source_found = source_count > 0\n",
    "            \n",
    "            # Check if destination entity was found\n",
    "            if \"FindEntity\" in response[1] and response[1][\"FindEntity\"][\"status\"] == 0:\n",
    "                dest_count = response[1][\"FindEntity\"].get(\"count\", 0)\n",
    "                dest_found = dest_count > 0\n",
    "            \n",
    "            # Check if connection was created\n",
    "            if \"AddConnection\" in response[2] and response[2][\"AddConnection\"][\"status\"] == 0:\n",
    "                connection_created = True\n",
    "        \n",
    "        # Log progress periodically\n",
    "        if index % 10 == 0 or index == total_relationships - 1:\n",
    "            logger.info(f\"Processed relationship {index + 1}/{total_relationships} ({(index + 1) / total_relationships * 100:.1f}%)\")\n",
    "            \n",
    "        if source_found and dest_found and connection_created:\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"relationship\": rel_type,\n",
    "                \"source\": f\"{source_entity_type}:{source_entity_id}\",\n",
    "                \"destination\": f\"{dest_entity_type}:{dest_entity_id}\"\n",
    "            }\n",
    "        else:\n",
    "            error_details = {\n",
    "                \"source_found\": source_found,\n",
    "                \"dest_found\": dest_found,\n",
    "                \"connection_created\": connection_created\n",
    "            }\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"relationship\": rel_type,\n",
    "                \"source\": f\"{source_entity_type}:{source_entity_id}\",\n",
    "                \"destination\": f\"{dest_entity_type}:{dest_entity_id}\",\n",
    "                \"error\": f\"Failed to create connection: {error_details}\"\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"exception\",\n",
    "            \"relationship\": rel_type if 'rel_type' in locals() else \"unknown\",\n",
    "            \"source\": f\"{source_entity_type}:{source_entity_id}\" if 'source_entity_type' in locals() else \"unknown\",\n",
    "            \"destination\": f\"{dest_entity_type}:{dest_entity_id}\" if 'dest_entity_type' in locals() else \"unknown\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "def create_relationships_parallel(relationships, max_workers=10, batch_size=20):\n",
    "    \"\"\"\n",
    "    Create relationships between entities in ApertureDB using parallel processing\n",
    "    \n",
    "    Args:\n",
    "        relationships: List of relationships from entity_relationships.json\n",
    "        max_workers: Maximum number of parallel workers\n",
    "        batch_size: Relationships to process in each batch\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (success_count, error_count, total_time)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(f\"Beginning relationship creation with {max_workers} workers\")\n",
    "    \n",
    "    total_relationships = len(relationships)\n",
    "    logger.info(f\"Found {total_relationships} relationships to create\")\n",
    "    \n",
    "    # Process relationships in batches\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Prepare relationship data with indices\n",
    "    relationship_data = [(rel, total_relationships, idx) for idx, rel in enumerate(relationships)]\n",
    "    \n",
    "    with tqdm(total=total_relationships, desc=\"Creating relationships\") as progress:\n",
    "        for i in range(0, total_relationships, batch_size):\n",
    "            batch = relationship_data[i:i+batch_size]\n",
    "            \n",
    "            # Process batch with parallel workers\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                results = list(executor.map(process_relationship, batch))\n",
    "            \n",
    "            # Update counts\n",
    "            batch_success = sum(1 for r in results if r[\"status\"] == \"success\")\n",
    "            batch_errors = len(batch) - batch_success\n",
    "            \n",
    "            success_count += batch_success\n",
    "            error_count += batch_errors\n",
    "            \n",
    "            # Update progress\n",
    "            progress.update(len(batch))\n",
    "            \n",
    "            # Log errors if any\n",
    "            if batch_errors > 0:\n",
    "                errors = [r for r in results if r[\"status\"] != \"success\"]\n",
    "                for error in errors[:3]:  # Show first 3 errors\n",
    "                    logger.warning(f\"Error: {error.get('relationship', 'unknown')} - {error.get('source', 'unknown')} → {error.get('destination', 'unknown')}: {error.get('error', 'Unknown error')}\")\n",
    "                \n",
    "                if len(errors) > 3:\n",
    "                    logger.warning(f\"... and {len(errors) - 3} more errors\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    logger.info(f\"Relationship creation completed in {total_time:.2f} seconds\")\n",
    "    logger.info(f\"Total: {total_relationships}, Success: {success_count}, Errors: {error_count}\")\n",
    "    \n",
    "    return success_count, error_count, total_time\n",
    "\n",
    "def verify_relationships():\n",
    "    \"\"\"Verify that relationships were created by checking counts\"\"\"\n",
    "    client = create_db_connection()\n",
    "    \n",
    "    # Load relationships data\n",
    "    with open(\"entity_relationships.json\", \"r\") as f:\n",
    "        relationships_data = json.load(f)\n",
    "        relationships = relationships_data.get(\"relationships\", [])\n",
    "    \n",
    "    # Get list of unique relationship types\n",
    "    relationship_types = set(rel[\"relationship\"] for rel in relationships)\n",
    "    sample_types = list(relationship_types)[:5]  # Check up to 5 types\n",
    "    \n",
    "    print(\"\\n----- Relationship Verification -----\")\n",
    "    for rel_type in sample_types:\n",
    "        # Count connections of this type in ApertureDB\n",
    "        query = [{\n",
    "            \"FindConnection\": {\n",
    "                \"with_class\": rel_type,\n",
    "                \"results\": {\n",
    "                    \"count\": True\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "        \n",
    "        try:\n",
    "            response, _ = client.query(query)\n",
    "            if \"FindConnection\" in response[0] and response[0][\"FindConnection\"][\"status\"] == 0:\n",
    "                count = response[0][\"FindConnection\"].get(\"count\", 0)\n",
    "                print(f\"Relationship '{rel_type}': {count} connections found in database\")\n",
    "            else:\n",
    "                print(f\"Relationship '{rel_type}': Failed to get count - {response[0]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Relationship '{rel_type}': Error during verification - {str(e)}\")\n",
    "\n",
    "def process_step5_2():\n",
    "    \"\"\"Execute Step 5.2: Create relationships between entities in ApertureDB\"\"\"\n",
    "    logger.info(\"Starting Step 5.2: Relationship creation in ApertureDB\")\n",
    "    \n",
    "    # Load relationships from JSON\n",
    "    try:\n",
    "        with open(\"entity_relationships.json\", \"r\") as f:\n",
    "            relationships_data = json.load(f)\n",
    "            relationships = relationships_data.get(\"relationships\", [])\n",
    "    except FileNotFoundError:\n",
    "        logger.error(\"entity_relationships.json not found. Please complete Step 4 first.\")\n",
    "        return None\n",
    "    \n",
    "    if not relationships:\n",
    "        logger.error(\"No relationships found in entity_relationships.json\")\n",
    "        return None\n",
    "    \n",
    "    # Create relationships in parallel\n",
    "    success, errors, duration = create_relationships_parallel(\n",
    "        relationships,\n",
    "        max_workers=10,  # Adjust based on system capabilities and API limits\n",
    "        batch_size=20    # Smaller batch size for more complex operations\n",
    "    )\n",
    "    \n",
    "    # Report results\n",
    "    print(\"\\n----- Relationship Creation Results -----\")\n",
    "    print(f\"Total relationships: {len(relationships)}\")\n",
    "    print(f\"Successfully created: {success}\")\n",
    "    print(f\"Errors: {errors}\")\n",
    "    print(f\"Total time: {duration:.2f} seconds\")\n",
    "    \n",
    "    # Verify some relationships were created\n",
    "    if success > 0:\n",
    "        verify_relationships()\n",
    "    \n",
    "    return success, errors, duration\n",
    "\n",
    "# Execute Step 5.2\n",
    "success_rel, errors_rel, duration_rel = process_step5_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfaf20d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
